the agreement in question involves number in #nouns# and $reflexive pronouns$ and is syntactic rather than semantic in nature because grammatical number in english , like grammatical gender in languages such as french , is partly arbitrary .	1	conjunction	conjunction	1
the agreement in question involves number in nouns and reflexive pronouns and is syntactic rather than semantic in nature because grammatical number in english , like #grammatical gender# in $languages$ such as french , is partly arbitrary .	5	feature-of	feature-of	1
the agreement in question involves number in nouns and reflexive pronouns and is syntactic rather than semantic in nature because grammatical number in english , like grammatical gender in $languages$ such as #french# , is partly arbitrary .	3	hyponym-of	hyponym-of	1
in this paper , a novel #method# to learn the $intrinsic object structure$ for robust visual tracking is proposed .	0	used-for	used-for	1
in this paper , a novel method to learn the #intrinsic object structure# for $robust visual tracking$ is proposed .	0	used-for	used-for	1
the basic assumption is that the $parameterized object state$ lies on a #low dimensional manifold# and can be learned from training data .	5	feature-of	feature-of	1
based on this assumption , firstly we derived the #dimensionality reduction and density estimation algorithm# for $unsupervised learning of object intrinsic representation$ , the obtained non-rigid part of object state reduces even to 2 dimensions .	0	used-for	used-for	1
secondly the $dynamical model$ is derived and trained based on this #intrinsic representation# .	0	used-for	used-for	1
thirdly the learned #intrinsic object structure# is integrated into a $particle-filter style tracker$ .	4	part-of	part-of	1
we will show that this intrinsic object representation has some interesting properties and based on which the newly derived #dynamical model# makes $particle-filter style tracker$ more robust and reliable .	0	used-for	used-for	1
experiments show that the learned #tracker# performs much better than existing $trackers$ on the tracking of complex non-rigid motions such as fish twisting with self-occlusion and large inter-frame lip motion .	6	compare	compare	1
experiments show that the learned #tracker# performs much better than existing trackers on the $tracking of complex non-rigid motions$ such as fish twisting with self-occlusion and large inter-frame lip motion .	0	used-for	used-for	1
experiments show that the learned tracker performs much better than existing #trackers# on the $tracking of complex non-rigid motions$ such as fish twisting with self-occlusion and large inter-frame lip motion .	0	used-for	used-for	1
experiments show that the learned tracker performs much better than existing trackers on the tracking of $complex non-rigid motions$ such as #fish twisting# with self-occlusion and large inter-frame lip motion .	3	hyponym-of	hyponym-of	1
experiments show that the learned tracker performs much better than existing trackers on the tracking of complex non-rigid motions such as $fish twisting$ with #self-occlusion# and large inter-frame lip motion .	5	feature-of	feature-of	1
experiments show that the learned tracker performs much better than existing trackers on the tracking of complex non-rigid motions such as fish twisting with #self-occlusion# and large $inter-frame lip motion$ .	1	conjunction	conjunction	1
experiments show that the learned tracker performs much better than existing trackers on the tracking of complex non-rigid motions such as $fish twisting$ with self-occlusion and large #inter-frame lip motion# .	5	feature-of	feature-of	1
the proposed #method# also has the potential to solve other type of $tracking problems$ .	0	used-for	used-for	1
in this paper , we present a #digital signal processor -lrb- dsp -rrb- implementation# of $real-time statistical voice conversion -lrb- vc -rrb-$ for silent speech enhancement and electrolaryngeal speech enhancement .	0	used-for	used-for	1
in this paper , we present a digital signal processor -lrb- dsp -rrb- implementation of #real-time statistical voice conversion -lrb- vc -rrb-# for $silent speech enhancement$ and electrolaryngeal speech enhancement .	0	used-for	used-for	1
in this paper , we present a digital signal processor -lrb- dsp -rrb- implementation of #real-time statistical voice conversion -lrb- vc -rrb-# for silent speech enhancement and $electrolaryngeal speech enhancement$ .	0	used-for	used-for	1
in this paper , we present a digital signal processor -lrb- dsp -rrb- implementation of real-time statistical voice conversion -lrb- vc -rrb- for #silent speech enhancement# and $electrolaryngeal speech enhancement$ .	1	conjunction	conjunction	1
#electrolaryngeal speech# is one of the typical types of $alaryngeal speech$ produced by an alternative speaking method for laryngectomees .	3	hyponym-of	hyponym-of	1
electrolaryngeal speech is one of the typical types of $alaryngeal speech$ produced by an alternative #speaking method# for laryngectomees .	0	used-for	used-for	1
electrolaryngeal speech is one of the typical types of alaryngeal speech produced by an alternative #speaking method# for $laryngectomees$ .	0	used-for	used-for	1
however , the #sound quality# of $nam and electrolaryngeal speech$ suffers from lack of naturalness .	2	evaluate-for	evaluate-for	1
vc has proven to be one of the promising approaches to address this problem , and $it$ has been successfully implemented on #devices# with sufficient computational resources .	0	used-for	used-for	1
vc has proven to be one of the promising approaches to address this problem , and it has been successfully implemented on $devices$ with #sufficient computational resources# .	5	feature-of	feature-of	1
an implementation on $devices$ that are highly portable but have #limited computational resources# would greatly contribute to its practical use .	5	feature-of	feature-of	1
in this paper we further implement $real-time vc$ on a #dsp# .	0	used-for	used-for	1
to implement the two $speech enhancement systems$ based on #real-time vc# , one from nam to a whispered voice and the other from electrolaryngeal speech to a natural voice , we propose several methods for reducing computational cost while preserving conversion accuracy .	0	used-for	used-for	1
to implement the two $speech enhancement systems$ based on real-time vc , #one# from nam to a whispered voice and the other from electrolaryngeal speech to a natural voice , we propose several methods for reducing computational cost while preserving conversion accuracy .	3	hyponym-of	hyponym-of	1
to implement the two speech enhancement systems based on real-time vc , #one# from nam to a whispered voice and the $other$ from electrolaryngeal speech to a natural voice , we propose several methods for reducing computational cost while preserving conversion accuracy .	1	conjunction	conjunction	1
to implement the two $speech enhancement systems$ based on real-time vc , one from nam to a whispered voice and the #other# from electrolaryngeal speech to a natural voice , we propose several methods for reducing computational cost while preserving conversion accuracy .	3	hyponym-of	hyponym-of	1
to implement the two speech enhancement systems based on real-time vc , one from nam to a whispered voice and the other from electrolaryngeal speech to a natural voice , we propose several $methods$ for reducing #computational cost# while preserving conversion accuracy .	2	evaluate-for	evaluate-for	1
to implement the two speech enhancement systems based on real-time vc , one from nam to a whispered voice and the other from electrolaryngeal speech to a natural voice , we propose several methods for reducing #computational cost# while preserving $conversion accuracy$ .	1	conjunction	conjunction	1
to implement the two speech enhancement systems based on real-time vc , one from nam to a whispered voice and the other from electrolaryngeal speech to a natural voice , we propose several $methods$ for reducing computational cost while preserving #conversion accuracy# .	2	evaluate-for	evaluate-for	1
we conduct experimental evaluations and show that $real-time vc$ is capable of running on a #dsp# with little degradation .	0	used-for	used-for	1
we propose a #method# that automatically generates $paraphrase$ sets from seed sentences to be used as reference sets in objective machine translation evaluation measures like bleu and nist .	0	used-for	used-for	1
we propose a method that automatically generates #paraphrase# sets from seed sentences to be used as reference sets in objective $machine translation evaluation measures$ like bleu and nist .	0	used-for	used-for	1
we propose a method that automatically generates paraphrase sets from seed sentences to be used as reference sets in objective $machine translation evaluation measures$ like #bleu# and nist .	3	hyponym-of	hyponym-of	1
we propose a method that automatically generates paraphrase sets from seed sentences to be used as reference sets in objective machine translation evaluation measures like #bleu# and $nist$ .	1	conjunction	conjunction	1
we propose a method that automatically generates paraphrase sets from seed sentences to be used as reference sets in objective $machine translation evaluation measures$ like bleu and #nist# .	3	hyponym-of	hyponym-of	1
we measured the quality of the paraphrases produced in an experiment , i.e. , -lrb- i -rrb- their $grammaticality$ : at least 99 % correct sentences ; -lrb- ii -rrb- their #equivalence in meaning# : at least 96 % correct paraphrases either by meaning equivalence or entailment ; and , -lrb- iii -rrb- the amount of internal lexical and syntactical variation in a set of paraphrases : slightly superior to that of hand-produced sets .	1	conjunction	conjunction	1
we measured the quality of the paraphrases produced in an experiment , i.e. , -lrb- i -rrb- their grammaticality : at least 99 % correct sentences ; -lrb- ii -rrb- their equivalence in meaning : at least 96 % correct $paraphrases$ either by #meaning equivalence# or entailment ; and , -lrb- iii -rrb- the amount of internal lexical and syntactical variation in a set of paraphrases : slightly superior to that of hand-produced sets .	0	used-for	used-for	1
we measured the quality of the paraphrases produced in an experiment , i.e. , -lrb- i -rrb- their grammaticality : at least 99 % correct sentences ; -lrb- ii -rrb- their equivalence in meaning : at least 96 % correct paraphrases either by #meaning equivalence# or $entailment$ ; and , -lrb- iii -rrb- the amount of internal lexical and syntactical variation in a set of paraphrases : slightly superior to that of hand-produced sets .	1	conjunction	conjunction	1
we measured the quality of the paraphrases produced in an experiment , i.e. , -lrb- i -rrb- their grammaticality : at least 99 % correct sentences ; -lrb- ii -rrb- their equivalence in meaning : at least 96 % correct $paraphrases$ either by meaning equivalence or #entailment# ; and , -lrb- iii -rrb- the amount of internal lexical and syntactical variation in a set of paraphrases : slightly superior to that of hand-produced sets .	0	used-for	used-for	1
we measured the quality of the paraphrases produced in an experiment , i.e. , -lrb- i -rrb- their grammaticality : at least 99 % correct sentences ; -lrb- ii -rrb- their $equivalence in meaning$ : at least 96 % correct paraphrases either by meaning equivalence or entailment ; and , -lrb- iii -rrb- the amount of #internal lexical and syntactical variation# in a set of paraphrases : slightly superior to that of hand-produced sets .	1	conjunction	conjunction	1
we measured the quality of the paraphrases produced in an experiment , i.e. , -lrb- i -rrb- their grammaticality : at least 99 % correct sentences ; -lrb- ii -rrb- their equivalence in meaning : at least 96 % correct paraphrases either by meaning equivalence or entailment ; and , -lrb- iii -rrb- the amount of internal lexical and syntactical variation in a set of #paraphrases# : slightly superior to that of $hand-produced sets$ .	6	compare	compare	1
the $paraphrase$ sets produced by this #method# thus seem adequate as reference sets to be used for mt evaluation .	0	used-for	used-for	1
#graph unification# remains the most expensive part of $unification-based grammar parsing$ .	4	part-of	part-of	1
we focus on one #speed-up element# in the design of $unification algorithms$ : avoidance of copying of unmodified subgraphs .	4	part-of	part-of	1
we propose a $method$ of attaining such a design through a method of #structure-sharing# which avoids log -lrb- d -rrb- overheads often associated with structure-sharing of graphs without any use of costly dependency pointers .	0	used-for	used-for	1
the proposed #scheme# eliminates redundant copying while maintaining the quasi-destructive scheme 's ability to avoid over copying and early copying combined with its ability to handle $cyclic structures$ without algorithmic additions .	0	used-for	used-for	1
the proposed $scheme$ eliminates redundant copying while maintaining the #quasi-destructive scheme 's ability# to avoid over copying and early copying combined with its ability to handle cyclic structures without algorithmic additions .	5	feature-of	feature-of	1
the proposed scheme eliminates redundant copying while maintaining the quasi-destructive scheme 's ability to avoid #over copying# and $early copying$ combined with its ability to handle cyclic structures without algorithmic additions .	1	conjunction	conjunction	1
we describe a novel technique and implemented #system# for constructing a $subcategorization dictionary$ from textual corpora .	0	used-for	used-for	1
we describe a novel technique and implemented $system$ for constructing a subcategorization dictionary from #textual corpora# .	0	used-for	used-for	1
we also demonstrate that a $subcategorization dictionary$ built with the #system# improves the accuracy of a parser by an appreciable amount	0	used-for	used-for	1
we also demonstrate that a subcategorization dictionary built with the system improves the #accuracy# of a $parser$ by an appreciable amount	2	evaluate-for	evaluate-for	1
we also demonstrate that a $subcategorization dictionary$ built with the system improves the accuracy of a #parser# by an appreciable amount	2	evaluate-for	evaluate-for	1
a number of powerful $registration criteria$ have been developed in the last decade , most prominently the criterion of #maximum mutual information# .	3	hyponym-of	hyponym-of	1
although this criterion provides for good registration results in many applications , $it$ remains a purely #low-level criterion# .	5	feature-of	feature-of	1
in this paper , we will develop a #bayesian framework# that allows to impose statistically learned prior knowledge about the joint intensity distribution into $image registration methods$ .	0	used-for	used-for	1
in this paper , we will develop a bayesian framework that allows to impose #statistically learned prior knowledge# about the joint intensity distribution into $image registration methods$ .	0	used-for	used-for	1
in this paper , we will develop a bayesian framework that allows to impose $statistically learned prior knowledge$ about the #joint intensity distribution# into image registration methods .	5	feature-of	feature-of	1
the $prior$ is given by a #kernel density estimate# on the space of joint intensity distributions computed from a representative set of pre-registered image pairs .	0	used-for	used-for	1
the prior is given by a #kernel density estimate# on the space of $joint intensity distributions$ computed from a representative set of pre-registered image pairs .	0	used-for	used-for	1
the prior is given by a kernel density estimate on the space of $joint intensity distributions$ computed from a representative set of #pre-registered image pairs# .	0	used-for	used-for	1
experimental results demonstrate that the resulting #registration process# is more robust to $missing low-level information$ as it favors intensity correspondences statistically consistent with the learned intensity distributions .	0	used-for	used-for	1
experimental results demonstrate that the resulting registration process is more robust to missing low-level information as #it# favors $intensity correspondences$ statistically consistent with the learned intensity distributions .	0	used-for	used-for	1
we present a #method# for $synthesizing complex , photo-realistic facade images$ , from a single example .	0	used-for	used-for	1
after parsing the example image into its $semantic components$ , a #tiling# for it is generated .	0	used-for	used-for	1
novel tilings can then be created , yielding $facade textures$ with different dimensions or with #occluded parts inpainted# .	5	feature-of	feature-of	1
a #genetic algorithm# guides the novel $facades$ as well as inpainted parts to be consistent with the example , both in terms of their overall structure and their detailed textures .	0	used-for	used-for	1
a #genetic algorithm# guides the novel facades as well as $inpainted parts$ to be consistent with the example , both in terms of their overall structure and their detailed textures .	0	used-for	used-for	1
promising results for #multiple standard datasets# -- in particular for the different building styles they contain -- demonstrate the potential of the $method$ .	2	evaluate-for	evaluate-for	1
we introduce a new $interactive corpus exploration tool$ called #infomagnets# .	3	hyponym-of	hyponym-of	1
#infomagnets# aims at making $exploratory corpus analysis$ accessible to researchers who are not experts in text mining .	0	used-for	used-for	1
as evidence of its usefulness and usability , #it# has been used successfully in a research context to uncover relationships between language and behavioral patterns in two distinct $domains$ : tutorial dialogue -lrb- kumar et al. , submitted -rrb- and on-line communities -lrb- arguello et al. , 2006 -rrb- .	0	used-for	used-for	1
as evidence of its usefulness and usability , it has been used successfully in a research context to uncover relationships between language and behavioral patterns in two distinct $domains$ : #tutorial dialogue# -lrb- kumar et al. , submitted -rrb- and on-line communities -lrb- arguello et al. , 2006 -rrb- .	3	hyponym-of	hyponym-of	1
as evidence of its usefulness and usability , it has been used successfully in a research context to uncover relationships between language and behavioral patterns in two distinct domains : #tutorial dialogue# -lrb- kumar et al. , submitted -rrb- and $on-line communities$ -lrb- arguello et al. , 2006 -rrb- .	1	conjunction	conjunction	1
as evidence of its usefulness and usability , it has been used successfully in a research context to uncover relationships between language and behavioral patterns in two distinct $domains$ : tutorial dialogue -lrb- kumar et al. , submitted -rrb- and #on-line communities# -lrb- arguello et al. , 2006 -rrb- .	3	hyponym-of	hyponym-of	1
as an #educational tool# , it has been used as part of a unit on $protocol analysis$ in an educational research methods course .	0	used-for	used-for	1
sources of training data suitable for $language modeling$ of #conversational speech# are limited .	0	used-for	used-for	1
in this paper , we show how training data can be supplemented with text from the web filtered to match the style and/or topic of the target $recognition task$ , but also that it is possible to get bigger performance gains from the data by using #class-dependent interpolation of n-grams# .	0	used-for	used-for	1
we present a #method# for $detecting 3d objects$ using multi-modalities .	0	used-for	used-for	1
we present a $method$ for detecting 3d objects using #multi-modalities# .	0	used-for	used-for	1
while #it# is generic , we demonstrate $it$ on the combination of an image and a dense depth map which give complementary object information .	0	used-for	used-for	1
while it is generic , we demonstrate $it$ on the combination of an #image# and a dense depth map which give complementary object information .	0	used-for	used-for	1
while it is generic , we demonstrate it on the combination of an #image# and a $dense depth map$ which give complementary object information .	1	conjunction	conjunction	1
while it is generic , we demonstrate $it$ on the combination of an image and a #dense depth map# which give complementary object information .	0	used-for	used-for	1
while it is generic , we demonstrate it on the combination of an image and a $dense depth map$ which give #complementary object information# .	5	feature-of	feature-of	1
it is based on an efficient representation of #templates# that capture the different $modalities$ , and we show in many experiments on commodity hardware that our approach significantly outperforms state-of-the-art methods on single modalities .	0	used-for	used-for	1
it is based on an efficient representation of templates that capture the different modalities , and we show in many experiments on commodity hardware that our #approach# significantly outperforms $state-of-the-art methods$ on single modalities .	6	compare	compare	1
it is based on an efficient representation of templates that capture the different modalities , and we show in many experiments on commodity hardware that our #approach# significantly outperforms state-of-the-art methods on $single modalities$ .	0	used-for	used-for	1
it is based on an efficient representation of templates that capture the different modalities , and we show in many experiments on commodity hardware that our approach significantly outperforms #state-of-the-art methods# on $single modalities$ .	0	used-for	used-for	1
the #compact description of a video sequence# through a single image map and a dominant motion has applications in several $domains$ , including video browsing and retrieval , compression , mosaicing , and visual summarization .	0	used-for	used-for	1
the $compact description of a video sequence$ through a single #image map# and a dominant motion has applications in several domains , including video browsing and retrieval , compression , mosaicing , and visual summarization .	0	used-for	used-for	1
the compact description of a video sequence through a single #image map# and a $dominant motion$ has applications in several domains , including video browsing and retrieval , compression , mosaicing , and visual summarization .	1	conjunction	conjunction	1
the $compact description of a video sequence$ through a single image map and a #dominant motion# has applications in several domains , including video browsing and retrieval , compression , mosaicing , and visual summarization .	0	used-for	used-for	1
the compact description of a video sequence through a single image map and a dominant motion has applications in several $domains$ , including #video browsing and retrieval# , compression , mosaicing , and visual summarization .	3	hyponym-of	hyponym-of	1
the compact description of a video sequence through a single image map and a dominant motion has applications in several domains , including #video browsing and retrieval# , $compression$ , mosaicing , and visual summarization .	1	conjunction	conjunction	1
the compact description of a video sequence through a single image map and a dominant motion has applications in several $domains$ , including video browsing and retrieval , #compression# , mosaicing , and visual summarization .	3	hyponym-of	hyponym-of	1
the compact description of a video sequence through a single image map and a dominant motion has applications in several domains , including video browsing and retrieval , #compression# , $mosaicing$ , and visual summarization .	1	conjunction	conjunction	1
the compact description of a video sequence through a single image map and a dominant motion has applications in several $domains$ , including video browsing and retrieval , compression , #mosaicing# , and visual summarization .	3	hyponym-of	hyponym-of	1
the compact description of a video sequence through a single image map and a dominant motion has applications in several domains , including video browsing and retrieval , compression , #mosaicing# , and $visual summarization$ .	1	conjunction	conjunction	1
building such a representation requires the capability to register all the frames with respect to the dominant object in the scene , a $task$ which has been , in the past , addressed through temporally #localized motion estimates# .	0	used-for	used-for	1
to avoid this oscillation , we augment the $motion model$ with a #generic temporal constraint# which increases the robustness against competing interpretations , leading to more meaningful content summarization .	0	used-for	used-for	1
to avoid this oscillation , we augment the motion model with a #generic temporal constraint# which increases the robustness against competing interpretations , leading to more meaningful $content summarization$ .	0	used-for	used-for	1
to avoid this oscillation , we augment the motion model with a $generic temporal constraint$ which increases the #robustness# against competing interpretations , leading to more meaningful content summarization .	2	evaluate-for	evaluate-for	1
in cross-domain learning , there is a more challenging problem that the $domain divergence$ involves more than one #dominant factors# , e.g. , different viewpoints , various resolutions and changing illuminations .	4	part-of	part-of	1
in cross-domain learning , there is a more challenging problem that the domain divergence involves more than one $dominant factors$ , e.g. , different #viewpoints# , various resolutions and changing illuminations .	3	hyponym-of	hyponym-of	1
in cross-domain learning , there is a more challenging problem that the domain divergence involves more than one dominant factors , e.g. , different #viewpoints# , various $resolutions$ and changing illuminations .	1	conjunction	conjunction	1
in cross-domain learning , there is a more challenging problem that the domain divergence involves more than one $dominant factors$ , e.g. , different viewpoints , various #resolutions# and changing illuminations .	3	hyponym-of	hyponym-of	1
in cross-domain learning , there is a more challenging problem that the domain divergence involves more than one dominant factors , e.g. , different viewpoints , various #resolutions# and changing $illuminations$ .	1	conjunction	conjunction	1
fortunately , an #intermediate domain# could often be found to build a bridge across them to facilitate the $learning problem$ .	0	used-for	used-for	1
in this paper , we propose a #coupled marginalized denoising auto-encoders framework# to address the $cross-domain problem$ .	0	used-for	used-for	1
specifically , we design two $marginalized denoising auto-encoders$ , #one# for the target and the other for source as well as the intermediate one .	3	hyponym-of	hyponym-of	1
specifically , we design two marginalized denoising auto-encoders , #one# for the target and the $other$ for source as well as the intermediate one .	1	conjunction	conjunction	1
specifically , we design two $marginalized denoising auto-encoders$ , one for the target and the #other# for source as well as the intermediate one .	3	hyponym-of	hyponym-of	1
to better couple the two $denoising auto-encoders learning$ , we incorporate a #feature mapping# , which tends to transfer knowledge between the intermediate domain and the target one .	4	part-of	part-of	1
to better couple the two denoising auto-encoders learning , we incorporate a #feature mapping# , which tends to transfer knowledge between the $intermediate domain$ and the target one .	0	used-for	used-for	1
furthermore , the $maximum margin criterion$ , e.g. , #intra-class com-pactness# and inter-class penalty , on the output layer is imposed to seek more discriminative features across different domains .	3	hyponym-of	hyponym-of	1
furthermore , the maximum margin criterion , e.g. , #intra-class com-pactness# and $inter-class penalty$ , on the output layer is imposed to seek more discriminative features across different domains .	1	conjunction	conjunction	1
furthermore , the $maximum margin criterion$ , e.g. , intra-class com-pactness and #inter-class penalty# , on the output layer is imposed to seek more discriminative features across different domains .	3	hyponym-of	hyponym-of	1
extensive experiments on two #tasks# have demonstrated the superiority of our $method$ over the state-of-the-art methods .	2	evaluate-for	evaluate-for	1
extensive experiments on two tasks have demonstrated the superiority of our #method# over the $state-of-the-art methods$ .	6	compare	compare	1
basically , a set of $age-group specific dictionaries$ are learned , where the #dictionary bases# corresponding to the same index yet from different dictionaries form a particular aging process pattern cross different age groups , and a linear combination of these patterns expresses a particular personalized aging process .	4	part-of	part-of	1
basically , a set of age-group specific dictionaries are learned , where the dictionary bases corresponding to the same index yet from different dictionaries form a particular aging process pattern cross different age groups , and a #linear combination# of these patterns expresses a particular $personalized aging process$ .	0	used-for	used-for	1
basically , a set of age-group specific dictionaries are learned , where the dictionary bases corresponding to the same index yet from different dictionaries form a particular aging process pattern cross different age groups , and a $linear combination$ of these #patterns# expresses a particular personalized aging process .	0	used-for	used-for	1
first , beyond the aging dictionaries , each subject may have extra $personalized facial characteristics$ , e.g. #mole# , which are invariant in the aging process .	3	hyponym-of	hyponym-of	1
thus a #personality-aware coupled reconstruction loss# is utilized to learn the $dictionaries$ based on face pairs from neighboring age groups .	0	used-for	used-for	1
extensive experiments well demonstrate the advantages of our proposed #solution# over other $state-of-the-arts$ in term of personalized aging progression , as well as the performance gain for cross-age face verification by synthesizing aging faces .	6	compare	compare	1
extensive experiments well demonstrate the advantages of our proposed #solution# over other state-of-the-arts in term of $personalized aging progression$ , as well as the performance gain for cross-age face verification by synthesizing aging faces .	0	used-for	used-for	1
extensive experiments well demonstrate the advantages of our proposed solution over other #state-of-the-arts# in term of $personalized aging progression$ , as well as the performance gain for cross-age face verification by synthesizing aging faces .	0	used-for	used-for	1
extensive experiments well demonstrate the advantages of our proposed solution over other state-of-the-arts in term of personalized aging progression , as well as the performance gain for $cross-age face verification$ by #synthesizing aging faces# .	0	used-for	used-for	1
we propose a draft scheme of the #model# formalizing the $structure of communicative context$ in dialogue interaction .	0	used-for	used-for	1
we propose a draft scheme of the model formalizing the $structure of communicative context$ in #dialogue interaction# .	5	feature-of	feature-of	1
visitors who browse the web from wireless pdas , cell phones , and pagers are frequently stymied by #web interfaces# optimized for $desktop pcs$ .	0	used-for	used-for	1
in this paper we develop an #algorithm# , minpath , that automatically improves $wireless web navigation$ by suggesting useful shortcut links in real time .	0	used-for	used-for	1
in this paper we develop an #algorithm# , minpath , that automatically improves $wireless web navigation$ by suggesting useful shortcut links in real time .	0	used-for	used-for	1
$minpath$ finds shortcuts by using a learned #model# of web visitor behavior to estimate the savings of shortcut links , and suggests only the few best links .	0	used-for	used-for	1
minpath finds shortcuts by using a learned #model# of $web visitor behavior$ to estimate the savings of shortcut links , and suggests only the few best links .	0	used-for	used-for	1
minpath finds shortcuts by using a learned #model# of web visitor behavior to estimate the $savings of shortcut links$ , and suggests only the few best links .	0	used-for	used-for	1
we explore a variety of $predictive models$ , including #na ¨ ıve bayes mixture models# and mixtures of markov models , and report empirical evidence that minpath finds useful shortcuts that save substantial navigational effort .	3	hyponym-of	hyponym-of	1
we explore a variety of predictive models , including #na ¨ ıve bayes mixture models# and $mixtures of markov models$ , and report empirical evidence that minpath finds useful shortcuts that save substantial navigational effort .	1	conjunction	conjunction	1
we explore a variety of $predictive models$ , including na ¨ ıve bayes mixture models and #mixtures of markov models# , and report empirical evidence that minpath finds useful shortcuts that save substantial navigational effort .	3	hyponym-of	hyponym-of	1
this paper describes a particular #approach# to $parsing$ that utilizes recent advances in unification-based parsing and in classification-based knowledge representation .	0	used-for	used-for	1
this paper describes a particular $approach$ to parsing that utilizes recent advances in #unification-based parsing# and in classification-based knowledge representation .	0	used-for	used-for	1
this paper describes a particular $approach$ to parsing that utilizes recent advances in unification-based parsing and in #classification-based knowledge representation# .	0	used-for	used-for	1
this paper describes a particular approach to parsing that utilizes recent advances in $unification-based parsing$ and in #classification-based knowledge representation# .	1	conjunction	conjunction	1
as #unification-based grammatical frameworks# are extended to handle richer descriptions of $linguistic information$ , they begin to share many of the properties that have been developed in kl-one-like knowledge representation systems .	0	used-for	used-for	1
as unification-based grammatical frameworks are extended to handle richer descriptions of linguistic information , $they$ begin to share many of the properties that have been developed in #kl-one-like knowledge representation systems# .	0	used-for	used-for	1
this commonality suggests that some of the #classification-based representation techniques# can be applied to $unification-based linguistic descriptions$ .	0	used-for	used-for	1
this merging supports the integration of #semantic and syntactic information# into the same $system$ , simultaneously subject to the same types of processes , in an efficient manner .	0	used-for	used-for	1
the use of a #kl-one style representation# for $parsing$ and semantic interpretation was first explored in the psi-klone system -lsb- 2 -rsb- , in which parsing is characterized as an inference process called incremental description refinement .	0	used-for	used-for	1
the use of a #kl-one style representation# for parsing and $semantic interpretation$ was first explored in the psi-klone system -lsb- 2 -rsb- , in which parsing is characterized as an inference process called incremental description refinement .	0	used-for	used-for	1
the use of a kl-one style representation for #parsing# and $semantic interpretation$ was first explored in the psi-klone system -lsb- 2 -rsb- , in which parsing is characterized as an inference process called incremental description refinement .	1	conjunction	conjunction	1
the use of a $kl-one style representation$ for parsing and semantic interpretation was first explored in the #psi-klone system# -lsb- 2 -rsb- , in which parsing is characterized as an inference process called incremental description refinement .	0	used-for	used-for	1
the use of a kl-one style representation for parsing and semantic interpretation was first explored in the psi-klone system -lsb- 2 -rsb- , in which $parsing$ is characterized as an inference process called #incremental description refinement# .	0	used-for	used-for	1
the use of a kl-one style representation for parsing and semantic interpretation was first explored in the psi-klone system -lsb- 2 -rsb- , in which parsing is characterized as an $inference process$ called #incremental description refinement# .	3	hyponym-of	hyponym-of	1
in this paper we discuss a proposed #user knowledge modeling architecture# for the $icicle system$ , a language tutoring application for deaf learners of written english .	0	used-for	used-for	1
in this paper we discuss a proposed user knowledge modeling architecture for the #icicle system# , a $language tutoring application$ for deaf learners of written english .	3	hyponym-of	hyponym-of	1
in this paper we discuss a proposed user knowledge modeling architecture for the icicle system , a #language tutoring application# for $deaf learners$ of written english .	0	used-for	used-for	1
in this paper we discuss a proposed user knowledge modeling architecture for the icicle system , a $language tutoring application$ for deaf learners of #written english# .	0	used-for	used-for	1
the #model# will represent the language proficiency of the user and is designed to be referenced during both $writing analysis$ and feedback production .	0	used-for	used-for	1
the #model# will represent the language proficiency of the user and is designed to be referenced during both writing analysis and $feedback production$ .	0	used-for	used-for	1
the model will represent the language proficiency of the user and is designed to be referenced during both #writing analysis# and $feedback production$ .	1	conjunction	conjunction	1
we motivate our $model design$ by citing relevant research on #second language and cognitive skill acquisition# , and briefly discuss preliminary empirical evidence supporting the design .	0	used-for	used-for	1
we conclude by showing how our #design# can provide a rich and robust information base to a $language assessment / correction application$ by modeling user proficiency at a high level of granularity and specificity .	0	used-for	used-for	1
we conclude by showing how our #design# can provide a rich and robust information base to a language assessment / correction application by modeling $user proficiency$ at a high level of granularity and specificity .	0	used-for	used-for	1
we conclude by showing how our design can provide a rich and robust information base to a language assessment / correction application by modeling $user proficiency$ at a high level of #granularity# and specificity .	2	evaluate-for	evaluate-for	1
we conclude by showing how our design can provide a rich and robust information base to a language assessment / correction application by modeling user proficiency at a high level of #granularity# and $specificity$ .	1	conjunction	conjunction	1
we conclude by showing how our design can provide a rich and robust information base to a language assessment / correction application by modeling $user proficiency$ at a high level of granularity and #specificity# .	2	evaluate-for	evaluate-for	1
#constraint propagation# is one of the key techniques in $constraint programming$ , and a large body of work has built up around it .	4	part-of	part-of	1
in this paper we present $shortstr2$ , a development of the #simple tabular reduction algorithm str2 +# .	0	used-for	used-for	1
we show that #shortstr2# is complementary to the existing algorithms $shortgac$ and haggisgac that exploit short supports , while being much simpler .	6	compare	compare	1
we show that #shortstr2# is complementary to the existing algorithms shortgac and $haggisgac$ that exploit short supports , while being much simpler .	6	compare	compare	1
we show that shortstr2 is complementary to the existing algorithms #shortgac# and $haggisgac$ that exploit short supports , while being much simpler .	1	conjunction	conjunction	1
when a constraint is amenable to short supports , the #short support set# can be exponentially smaller than the $full-length support set$ .	6	compare	compare	1
we also show that #shortstr2# can be combined with a simple algorithm to identify $short supports$ from full-length supports , to provide a superior drop-in replacement for str2 + .	0	used-for	used-for	1
we also show that #shortstr2# can be combined with a simple algorithm to identify short supports from full-length supports , to provide a superior $drop-in replacement$ for str2 + .	0	used-for	used-for	1
we also show that $shortstr2$ can be combined with a simple #algorithm# to identify short supports from full-length supports , to provide a superior drop-in replacement for str2 + .	1	conjunction	conjunction	1
we also show that shortstr2 can be combined with a simple #algorithm# to identify $short supports$ from full-length supports , to provide a superior drop-in replacement for str2 + .	0	used-for	used-for	1
we also show that $shortstr2$ can be combined with a simple algorithm to identify short supports from #full-length supports# , to provide a superior drop-in replacement for str2 + .	0	used-for	used-for	1
we also show that $shortstr2$ can be combined with a simple algorithm to identify short supports from #full-length supports# , to provide a superior drop-in replacement for str2 + .	0	used-for	used-for	1
we also show that shortstr2 can be combined with a simple $algorithm$ to identify short supports from #full-length supports# , to provide a superior drop-in replacement for str2 + .	0	used-for	used-for	1
we also show that shortstr2 can be combined with a simple $algorithm$ to identify short supports from #full-length supports# , to provide a superior drop-in replacement for str2 + .	0	used-for	used-for	1
we also show that shortstr2 can be combined with a simple algorithm to identify short supports from full-length supports , to provide a superior #drop-in replacement# for $str2 +$ .	0	used-for	used-for	1
we propose a #detection method# for $orthographic variants$ caused by transliteration in a large corpus .	0	used-for	used-for	1
the $method$ employs two #similarities# .	0	used-for	used-for	1
one is $string similarity$ based on #edit distance# .	0	used-for	used-for	1
the other is $contextual similarity$ by a #vector space model# .	0	used-for	used-for	1
experimental results show that the $method$ performed a 0.889 #f-measure# in an open test .	2	evaluate-for	evaluate-for	1
#uncertainty handling# plays an important role during $shape tracking$ .	0	used-for	used-for	1
we have recently shown that the #fusion of measurement information with system dynamics and shape priors# greatly improves the $tracking$ performance for very noisy images such as ultrasound sequences -lsb- 22 -rsb- .	0	used-for	used-for	1
we have recently shown that the fusion of measurement information with system dynamics and shape priors greatly improves the #tracking# performance for very $noisy images$ such as ultrasound sequences -lsb- 22 -rsb- .	0	used-for	used-for	1
we have recently shown that the fusion of measurement information with system dynamics and shape priors greatly improves the tracking performance for very $noisy images$ such as #ultrasound sequences# -lsb- 22 -rsb- .	3	hyponym-of	hyponym-of	1
nevertheless , this $approach$ required #user initialization# of the tracking process .	0	used-for	used-for	1
nevertheless , this approach required #user initialization# of the $tracking process$ .	0	used-for	used-for	1
this paper solves the $automatic initial-ization problem$ by performing #boosted shape detection# as a generic measurement process and integrating it in our tracking framework .	0	used-for	used-for	1
this paper solves the automatic initial-ization problem by performing $boosted shape detection$ as a #generic measurement process# and integrating it in our tracking framework .	0	used-for	used-for	1
this paper solves the automatic initial-ization problem by performing boosted shape detection as a generic measurement process and integrating #it# in our $tracking framework$ .	4	part-of	part-of	1
as a result , we treat all sources of information in a unified way and derive the $posterior shape model$ as the shape with the #maximum likelihood# .	0	used-for	used-for	1
our #framework# is applied for the $automatic tracking of endocardium$ in ultrasound sequences of the human heart .	0	used-for	used-for	1
our framework is applied for the automatic tracking of #endocardium# in $ultrasound sequences of the human heart$ .	4	part-of	part-of	1
reliable #detection# and robust $tracking$ results are achieved when compared to existing approaches and inter-expert variations .	1	conjunction	conjunction	1
reliable detection and robust tracking results are achieved when compared to existing #approaches# and $inter-expert variations$ .	1	conjunction	conjunction	1
we present a #syntax-based constraint# for $word alignment$ , known as the cohesion constraint .	0	used-for	used-for	1
we present a $syntax-based constraint$ for word alignment , known as the #cohesion constraint# .	3	hyponym-of	hyponym-of	1
$it$ requires disjoint #english phrases# to be mapped to non-overlapping intervals in the french sentence .	0	used-for	used-for	1
we evaluate the utility of this $constraint$ in two different #algorithms# .	2	evaluate-for	evaluate-for	1
the results show that $it$ can provide a significant improvement in #alignment quality# .	2	evaluate-for	evaluate-for	1
we present a novel $entity-based representation of discourse$ which is inspired by #centering theory# and can be computed automatically from raw text .	0	used-for	used-for	1
we present a novel $entity-based representation of discourse$ which is inspired by centering theory and can be computed automatically from #raw text# .	0	used-for	used-for	1
we view $coherence assessment$ as a #ranking learning problem# and show that the proposed discourse representation supports the effective learning of a ranking function .	0	used-for	used-for	1
we view coherence assessment as a ranking learning problem and show that the proposed #discourse representation# supports the effective learning of a $ranking function$ .	0	used-for	used-for	1
our experiments demonstrate that the #induced model# achieves significantly higher accuracy than a state-of-the-art $coherence model$ .	6	compare	compare	1
our experiments demonstrate that the $induced model$ achieves significantly higher #accuracy# than a state-of-the-art coherence model .	2	evaluate-for	evaluate-for	1
our experiments demonstrate that the induced model achieves significantly higher #accuracy# than a state-of-the-art $coherence model$ .	2	evaluate-for	evaluate-for	1
this paper introduces a #robust interactive method# for $speech understanding$ .	0	used-for	used-for	1
the $generalized lr parsing$ is enhanced in this #approach# .	0	used-for	used-for	1
when a very noisy portion is detected , the $parser$ skips that portion using a fake #non-terminal symbol# .	0	used-for	used-for	1
this #method# is also capable of handling $unknown words$ , which is important in practical systems .	0	used-for	used-for	1
this paper shows that it is very often possible to identify the source language of #medium-length speeches# in the $europarl corpus$ on the basis of frequency counts of word n-grams -lrb- 87.2 % -96.7 % accuracy depending on classification method -rrb- .	4	part-of	part-of	1
this paper shows that it is very often possible to identify the source language of medium-length speeches in the europarl corpus on the basis of frequency counts of word n-grams -lrb- 87.2 % -96.7 % #accuracy# depending on $classification method$ -rrb- .	2	evaluate-for	evaluate-for	1
we investigated whether #automatic phonetic transcriptions -lrb- apts -rrb-# can replace $manually verified phonetic transcriptions$ -lrb- mpts -rrb- in a large corpus-based study on pronunciation variation .	6	compare	compare	1
we investigated whether #automatic phonetic transcriptions -lrb- apts -rrb-# can replace manually verified phonetic transcriptions -lrb- mpts -rrb- in a large corpus-based study on $pronunciation variation$ .	0	used-for	used-for	1
we investigated whether automatic phonetic transcriptions -lrb- apts -rrb- can replace #manually verified phonetic transcriptions# -lrb- mpts -rrb- in a large corpus-based study on $pronunciation variation$ .	0	used-for	used-for	1
we trained $classifiers$ on the #speech processes# extracted from the alignments of an apt and an mpt with a canonical transcription .	0	used-for	used-for	1
we trained classifiers on the $speech processes$ extracted from the #alignments# of an apt and an mpt with a canonical transcription .	0	used-for	used-for	1
we trained classifiers on the speech processes extracted from the #alignments# of an $apt$ and an mpt with a canonical transcription .	0	used-for	used-for	1
we trained classifiers on the speech processes extracted from the #alignments# of an apt and an $mpt$ with a canonical transcription .	0	used-for	used-for	1
we trained classifiers on the speech processes extracted from the alignments of an #apt# and an $mpt$ with a canonical transcription .	1	conjunction	conjunction	1
we trained classifiers on the speech processes extracted from the $alignments$ of an apt and an mpt with a #canonical transcription# .	0	used-for	used-for	1
we tested whether the #classifiers# were equally good at verifying whether $unknown transcriptions$ represent read speech or telephone dialogues , and whether the same speech processes were identified to distinguish between transcriptions of the two situational settings .	0	used-for	used-for	1
we tested whether the classifiers were equally good at verifying whether #unknown transcriptions# represent $read speech$ or telephone dialogues , and whether the same speech processes were identified to distinguish between transcriptions of the two situational settings .	0	used-for	used-for	1
we tested whether the classifiers were equally good at verifying whether #unknown transcriptions# represent read speech or $telephone dialogues$ , and whether the same speech processes were identified to distinguish between transcriptions of the two situational settings .	0	used-for	used-for	1
we tested whether the classifiers were equally good at verifying whether unknown transcriptions represent #read speech# or $telephone dialogues$ , and whether the same speech processes were identified to distinguish between transcriptions of the two situational settings .	1	conjunction	conjunction	1
our results not only show that similar distinguishing speech processes were identified ; our #apt-based classifier# yielded better classification accuracy than the $mpt-based classifier$ whilst using fewer classification features .	6	compare	compare	1
our results not only show that similar distinguishing speech processes were identified ; our $apt-based classifier$ yielded better #classification accuracy# than the mpt-based classifier whilst using fewer classification features .	2	evaluate-for	evaluate-for	1
our results not only show that similar distinguishing speech processes were identified ; our apt-based classifier yielded better #classification accuracy# than the $mpt-based classifier$ whilst using fewer classification features .	2	evaluate-for	evaluate-for	1
our results not only show that similar distinguishing speech processes were identified ; our $apt-based classifier$ yielded better classification accuracy than the mpt-based classifier whilst using fewer #classification features# .	0	used-for	used-for	1
our results not only show that similar distinguishing speech processes were identified ; our apt-based classifier yielded better classification accuracy than the $mpt-based classifier$ whilst using fewer #classification features# .	0	used-for	used-for	1
machine reading is a relatively new field that features #computer programs# designed to read $flowing text$ and extract fact assertions expressed by the narrative content .	0	used-for	used-for	1
machine reading is a relatively new field that features #computer programs# designed to read flowing text and extract $fact assertions$ expressed by the narrative content .	0	used-for	used-for	1
machine reading is a relatively new field that features computer programs designed to read flowing text and extract #fact assertions# expressed by the $narrative content$ .	5	feature-of	feature-of	1
this $task$ involves two core technologies : #natural language processing -lrb- nlp -rrb-# and information extraction -lrb- ie -rrb- .	4	part-of	part-of	1
this $task$ involves two core technologies : natural language processing -lrb- nlp -rrb- and #information extraction -lrb- ie -rrb-# .	4	part-of	part-of	1
in this paper we describe a $machine reading system$ that we have developed within a #cognitive architecture# .	5	feature-of	feature-of	1
we show how we have integrated into the framework several levels of knowledge for a particular domain , ideas from #cognitive semantics# and $construction grammar$ , plus tools from prior nlp and ie research .	1	conjunction	conjunction	1
we show how we have integrated into the framework several levels of knowledge for a particular domain , ideas from cognitive semantics and construction grammar , plus tools from #prior nlp# and $ie research$ .	1	conjunction	conjunction	1
the result is a #system# that is capable of reading and interpreting complex and fairly $idiosyncratic texts$ in the family history domain .	0	used-for	used-for	1
the result is a system that is capable of reading and interpreting complex and fairly $idiosyncratic texts$ in the #family history domain# .	5	feature-of	feature-of	1
we present two #methods# for capturing $nonstationary chaos$ , then present a few examples including biological signals , ocean waves and traffic flow .	0	used-for	used-for	1
we present two methods for capturing nonstationary chaos , then present a few $examples$ including #biological signals# , ocean waves and traffic flow .	3	hyponym-of	hyponym-of	1
we present two methods for capturing nonstationary chaos , then present a few examples including #biological signals# , $ocean waves$ and traffic flow .	1	conjunction	conjunction	1
we present two methods for capturing nonstationary chaos , then present a few $examples$ including biological signals , #ocean waves# and traffic flow .	3	hyponym-of	hyponym-of	1
we present two methods for capturing nonstationary chaos , then present a few examples including biological signals , #ocean waves# and $traffic flow$ .	1	conjunction	conjunction	1
we present two methods for capturing nonstationary chaos , then present a few $examples$ including biological signals , ocean waves and #traffic flow# .	3	hyponym-of	hyponym-of	1
this paper presents a #formal analysis# for a large class of words called $alternative markers$ , which includes other -lrb- than -rrb- , such -lrb- as -rrb- , and besides .	0	used-for	used-for	1
these #words# appear frequently enough in $dialog$ to warrant serious attention , yet present natural language search engines perform poorly on queries containing them .	4	part-of	part-of	1
i show that the performance of a $search engine$ can be improved dramatically by incorporating an #approximation of the formal analysis# that is compatible with the search engine 's operational semantics .	4	part-of	part-of	1
i show that the performance of a search engine can be improved dramatically by incorporating an approximation of the formal analysis that is compatible with the $search engine$ 's #operational semantics# .	4	part-of	part-of	1
the value of this approach is that as the #operational semantics# of $natural language applications$ improve , even larger improvements are possible .	4	part-of	part-of	1
we find that simple $interpolation methods$ , like #log-linear and linear interpolation# , improve the performance but fall short of the performance of an oracle .	3	hyponym-of	hyponym-of	1
actually , the oracle acts like a $dynamic combiner$ with #hard decisions# using the reference .	5	feature-of	feature-of	1
we suggest a $method$ that mimics the behavior of the oracle using a #neural network# or a decision tree .	0	used-for	used-for	1
we suggest a $method$ that mimics the behavior of the oracle using a neural network or a #decision tree# .	0	used-for	used-for	1
we suggest a method that mimics the behavior of the oracle using a $neural network$ or a #decision tree# .	1	conjunction	conjunction	1
the #method# amounts to tagging $lms$ with confidence measures and picking the best hypothesis corresponding to the lm with the best confidence .	0	used-for	used-for	1
the $method$ amounts to tagging lms with #confidence measures# and picking the best hypothesis corresponding to the lm with the best confidence .	0	used-for	used-for	1
we describe a new #method# for the representation of $nlp structures$ within reranking approaches .	0	used-for	used-for	1
we describe a new method for the representation of $nlp structures$ within #reranking approaches# .	5	feature-of	feature-of	1
we make use of a $conditional log-linear model$ , with #hidden variables# representing the assignment of lexical items to word clusters or word senses .	0	used-for	used-for	1
we make use of a conditional log-linear model , with hidden variables representing the assignment of lexical items to #word clusters# or $word senses$ .	1	conjunction	conjunction	1
the $model$ learns to automatically make these assignments based on a #discriminative training criterion# .	0	used-for	used-for	1
training and decoding with the model requires summing over an exponential number of hidden-variable assignments : the required $summations$ can be computed efficiently and exactly using #dynamic programming# .	0	used-for	used-for	1
as a case study , we apply the #model# to $parse reranking$ .	0	used-for	used-for	1
the #model# gives an f-measure improvement of ~ 1.25 % beyond the $base parser$ , and an ~ 0.25 % improvement beyond collins -lrb- 2000 -rrb- reranker .	6	compare	compare	1
the $model$ gives an #f-measure# improvement of ~ 1.25 % beyond the base parser , and an ~ 0.25 % improvement beyond collins -lrb- 2000 -rrb- reranker .	2	evaluate-for	evaluate-for	1
the model gives an f-measure improvement of ~ 1.25 % beyond the #base parser# , and an ~ 0.25 % improvement beyond $collins -lrb- 2000 -rrb- reranker$ .	6	compare	compare	1
although our experiments are focused on $parsing$ , the #techniques# described generalize naturally to nlp structures other than parse trees .	0	used-for	used-for	1
although our experiments are focused on parsing , the #techniques# described generalize naturally to $nlp structures$ other than parse trees .	0	used-for	used-for	1
although our experiments are focused on parsing , the #techniques# described generalize naturally to nlp structures other than $parse trees$ .	0	used-for	used-for	1
although our experiments are focused on parsing , the techniques described generalize naturally to $nlp structures$ other than #parse trees# .	1	conjunction	conjunction	1
this paper presents an #algorithm# for $learning the time-varying shape of a non-rigid 3d object$ from uncalibrated 2d tracking data .	0	used-for	used-for	1
we constrain the problem by assuming that the $object shape$ at each time instant is drawn from a #gaussian distribution# .	0	used-for	used-for	1
based on this assumption , the #algorithm# simultaneously estimates $3d shape and motion$ for each time frame , learns the parameters of the gaussian , and robustly fills-in missing data points .	0	used-for	used-for	1
we then extend the #algorithm# to model $temporal smoothness in object shape$ , thus allowing it to handle severe cases of missing data .	0	used-for	used-for	1
we then extend the algorithm to model temporal smoothness in object shape , thus allowing #it# to handle severe cases of $missing data$ .	0	used-for	used-for	1
#automatic summarization# and $information extraction$ are two important internet services .	1	conjunction	conjunction	1
#muc# and $summac$ play their appropriate roles in the next generation internet .	1	conjunction	conjunction	1
this paper focuses on the automatic summarization and proposes two different #models# to extract sentences for $summary generation$ under two tasks initiated by summac-1 .	0	used-for	used-for	1
this paper focuses on the automatic summarization and proposes two different #models# to extract sentences for summary generation under two $tasks$ initiated by summac-1 .	0	used-for	used-for	1
this paper focuses on the automatic summarization and proposes two different models to extract sentences for summary generation under two #tasks# initiated by $summac-1$ .	4	part-of	part-of	1
for $categorization task$ , #positive feature vectors# and negative feature vectors are used cooperatively to construct generic , indicative summaries .	0	used-for	used-for	1
for categorization task , #positive feature vectors# and $negative feature vectors$ are used cooperatively to construct generic , indicative summaries .	1	conjunction	conjunction	1
for categorization task , #positive feature vectors# and negative feature vectors are used cooperatively to construct $generic , indicative summaries$ .	0	used-for	used-for	1
for $categorization task$ , positive feature vectors and #negative feature vectors# are used cooperatively to construct generic , indicative summaries .	0	used-for	used-for	1
for categorization task , positive feature vectors and #negative feature vectors# are used cooperatively to construct $generic , indicative summaries$ .	0	used-for	used-for	1
for $adhoc task$ , a #text model# based on relationship between nouns and verbs is used to filter out irrelevant discourse segment , to rank relevant sentences , and to generate the user-directed summaries .	0	used-for	used-for	1
for adhoc task , a #text model# based on relationship between nouns and verbs is used to filter out irrelevant $discourse segment$ , to rank relevant sentences , and to generate the user-directed summaries .	0	used-for	used-for	1
for adhoc task , a #text model# based on relationship between nouns and verbs is used to filter out irrelevant discourse segment , to rank relevant sentences , and to generate the $user-directed summaries$ .	0	used-for	used-for	1
the result shows that the #normf# of the best summary and that of the fixed summary for $adhoc tasks$ are 0.456 and 0 .	2	evaluate-for	evaluate-for	1
the #normf# of the best summary and that of the fixed summary for $categorization task$ are 0.4090 and 0.4023 .	2	evaluate-for	evaluate-for	1
our #system# outperforms the average $system$ in categorization task but does a common job in adhoc task .	6	compare	compare	1
our $system$ outperforms the average system in #categorization task# but does a common job in adhoc task .	2	evaluate-for	evaluate-for	1
our system outperforms the average $system$ in #categorization task# but does a common job in adhoc task .	2	evaluate-for	evaluate-for	1
our $system$ outperforms the average system in categorization task but does a common job in #adhoc task# .	2	evaluate-for	evaluate-for	1
our system outperforms the average system in $categorization task$ but does a common job in #adhoc task# .	2	evaluate-for	evaluate-for	1
in real-world action recognition problems , low-level features can not adequately characterize the #rich spatial-temporal structures# in $action videos$ .	5	feature-of	feature-of	1
the second type is $data-driven attributes$ , which are learned from data using #dictionary learning methods# .	0	used-for	used-for	1
we propose a $discriminative and compact attribute-based representation$ by selecting a subset of #discriminative attributes# from a large attribute set .	0	used-for	used-for	1
three $attribute selection criteria$ are proposed and formulated as a #submodular optimization problem# .	0	used-for	used-for	1
experimental results on the #olympic sports and ucf101 datasets# demonstrate that the proposed $attribute-based representation$ can significantly boost the performance of action recognition algorithms and outperform most recently proposed recognition approaches .	2	evaluate-for	evaluate-for	1
experimental results on the olympic sports and ucf101 datasets demonstrate that the proposed #attribute-based representation# can significantly boost the performance of $action recognition algorithms$ and outperform most recently proposed recognition approaches .	0	used-for	used-for	1
experimental results on the olympic sports and ucf101 datasets demonstrate that the proposed attribute-based representation can significantly boost the performance of #action recognition algorithms# and outperform most recently proposed $recognition approaches$ .	6	compare	compare	1
landsbergen 's advocacy of #analytical inverses# for $compositional syntax rules$ encourages the application of definite clause grammar techniques to the construction of a parser returning montague analysis trees .	0	used-for	used-for	1
landsbergen 's advocacy of #analytical inverses# for compositional syntax rules encourages the application of $definite clause grammar techniques$ to the construction of a parser returning montague analysis trees .	0	used-for	used-for	1
landsbergen 's advocacy of analytical inverses for compositional syntax rules encourages the application of #definite clause grammar techniques# to the construction of a $parser returning montague analysis trees$ .	0	used-for	used-for	1
a $parser mdcc$ is presented which implements an #augmented friedman - warren algorithm# permitting post referencing * and interfaces with a language of intenslonal logic translator lilt so as to display the derivational history of corresponding reduced il formulae .	0	used-for	used-for	1
a parser mdcc is presented which implements an $augmented friedman - warren algorithm$ permitting #post referencing# * and interfaces with a language of intenslonal logic translator lilt so as to display the derivational history of corresponding reduced il formulae .	5	feature-of	feature-of	1
a parser mdcc is presented which implements an augmented friedman - warren algorithm permitting post referencing * and interfaces with a language of $intenslonal logic translator lilt$ so as to display the #derivational history# of corresponding reduced il formulae .	0	used-for	used-for	1
a parser mdcc is presented which implements an augmented friedman - warren algorithm permitting post referencing * and interfaces with a language of intenslonal logic translator lilt so as to display the $derivational history$ of corresponding #reduced il formulae# .	5	feature-of	feature-of	1
some familiarity with #montague 's ptq# and the $basic dcg mechanism$ is assumed .	1	conjunction	conjunction	1
$stochastic attention-based models$ have been shown to improve #computational efficiency# at test time , but they remain difficult to train because of intractable posterior inference and high variance in the stochastic gradient estimates .	2	evaluate-for	evaluate-for	1
stochastic attention-based models have been shown to improve computational efficiency at test time , but they remain difficult to train because of #intractable posterior inference# and high variance in the $stochastic gradient estimates$ .	1	conjunction	conjunction	1
#borrowing techniques# from the literature on training $deep generative models$ , we present the wake-sleep recurrent attention model , a method for training stochastic attention networks which improves posterior inference and which reduces the variability in the stochastic gradients .	0	used-for	used-for	1
borrowing techniques from the literature on training deep generative models , we present the wake-sleep recurrent attention model , a #method# for training $stochastic attention networks$ which improves posterior inference and which reduces the variability in the stochastic gradients .	0	used-for	used-for	1
borrowing techniques from the literature on training deep generative models , we present the wake-sleep recurrent attention model , a method for training #stochastic attention networks# which improves $posterior inference$ and which reduces the variability in the stochastic gradients .	0	used-for	used-for	1
we show that our $method$ can greatly speed up the #training time# for stochastic attention networks in the domains of image classification and caption generation .	2	evaluate-for	evaluate-for	1
we show that our method can greatly speed up the #training time# for $stochastic attention networks$ in the domains of image classification and caption generation .	5	feature-of	feature-of	1
we show that our $method$ can greatly speed up the training time for stochastic attention networks in the domains of #image classification# and caption generation .	2	evaluate-for	evaluate-for	1
we show that our method can greatly speed up the training time for stochastic attention networks in the domains of #image classification# and $caption generation$ .	1	conjunction	conjunction	1
we show that our $method$ can greatly speed up the training time for stochastic attention networks in the domains of image classification and #caption generation# .	2	evaluate-for	evaluate-for	1
a new #exemplar-based framework# unifying $image completion$ , texture synthesis and image inpainting is presented in this work .	0	used-for	used-for	1
a new #exemplar-based framework# unifying image completion , $texture synthesis$ and image inpainting is presented in this work .	0	used-for	used-for	1
a new #exemplar-based framework# unifying image completion , texture synthesis and $image inpainting$ is presented in this work .	0	used-for	used-for	1
a new exemplar-based framework unifying #image completion# , $texture synthesis$ and image inpainting is presented in this work .	1	conjunction	conjunction	1
a new exemplar-based framework unifying image completion , #texture synthesis# and $image inpainting$ is presented in this work .	1	conjunction	conjunction	1
contrary to existing #greedy techniques# , these $tasks$ are posed in the form of a discrete global optimization problem with a well defined objective function .	6	compare	compare	1
contrary to existing greedy techniques , these $tasks$ are posed in the form of a #discrete global optimization problem# with a well defined objective function .	5	feature-of	feature-of	1
contrary to existing greedy techniques , these tasks are posed in the form of a $discrete global optimization problem$ with a #well defined objective function# .	5	feature-of	feature-of	1
for solving this $problem$ a novel #optimization scheme# , called priority-bp , is proposed which carries two very important extensions over standard belief propagation -lrb- bp -rrb- : '' priority-based message scheduling '' and '' dynamic label pruning '' .	0	used-for	used-for	1
for solving this problem a novel $optimization scheme$ , called #priority-bp# , is proposed which carries two very important extensions over standard belief propagation -lrb- bp -rrb- : '' priority-based message scheduling '' and '' dynamic label pruning '' .	3	hyponym-of	hyponym-of	1
for solving this problem a novel $optimization scheme$ , called priority-bp , is proposed which carries two very important #extensions# over standard belief propagation -lrb- bp -rrb- : '' priority-based message scheduling '' and '' dynamic label pruning '' .	4	part-of	part-of	1
for solving this problem a novel optimization scheme , called priority-bp , is proposed which carries two very important $extensions$ over standard #belief propagation -lrb- bp -rrb-# : '' priority-based message scheduling '' and '' dynamic label pruning '' .	0	used-for	used-for	1
for solving this problem a novel optimization scheme , called priority-bp , is proposed which carries two very important $extensions$ over standard belief propagation -lrb- bp -rrb- : '' #priority-based message scheduling# '' and '' dynamic label pruning '' .	3	hyponym-of	hyponym-of	1
for solving this problem a novel optimization scheme , called priority-bp , is proposed which carries two very important extensions over standard belief propagation -lrb- bp -rrb- : '' #priority-based message scheduling# '' and '' $dynamic label pruning$ '' .	1	conjunction	conjunction	1
for solving this problem a novel optimization scheme , called priority-bp , is proposed which carries two very important $extensions$ over standard belief propagation -lrb- bp -rrb- : '' priority-based message scheduling '' and '' #dynamic label pruning# '' .	3	hyponym-of	hyponym-of	1
these two #extensions# work in cooperation to deal with the $intolerable computational cost of bp$ caused by the huge number of existing labels .	0	used-for	used-for	1
moreover , both #extensions# are generic and can therefore be applied to any $mrf energy function$ as well .	0	used-for	used-for	1
the effectiveness of our $method$ is demonstrated on a wide variety of #image completion examples# .	0	used-for	used-for	1
in this paper , we compare the relative effects of #segment order# , $segmentation$ and segment contiguity on the retrieval performance of a translation memory system .	1	conjunction	conjunction	1
in this paper , we compare the relative effects of #segment order# , segmentation and segment contiguity on the retrieval performance of a $translation memory system$ .	0	used-for	used-for	1
in this paper , we compare the relative effects of segment order , #segmentation# and $segment contiguity$ on the retrieval performance of a translation memory system .	1	conjunction	conjunction	1
in this paper , we compare the relative effects of segment order , #segmentation# and segment contiguity on the retrieval performance of a $translation memory system$ .	0	used-for	used-for	1
in this paper , we compare the relative effects of segment order , segmentation and #segment contiguity# on the retrieval performance of a $translation memory system$ .	0	used-for	used-for	1
in this paper , we compare the relative effects of segment order , segmentation and segment contiguity on the #retrieval# performance of a $translation memory system$ .	2	evaluate-for	evaluate-for	1
we take a selection of both $bag-of-words and segment order-sensitive string comparison methods$ , and run each over both #character - and word-segmented data# , in combination with a range of local segment contiguity models -lrb- in the form of n-grams -rrb- .	0	used-for	used-for	1
we take a selection of both $bag-of-words and segment order-sensitive string comparison methods$ , and run each over both character - and word-segmented data , in combination with a range of #local segment contiguity models# -lrb- in the form of n-grams -rrb- .	1	conjunction	conjunction	1
we take a selection of both bag-of-words and segment order-sensitive string comparison methods , and run each over both character - and word-segmented data , in combination with a range of $local segment contiguity models$ -lrb- in the form of #n-grams# -rrb- .	5	feature-of	feature-of	1
over two distinct datasets , we find that $indexing$ according to simple #character bigrams# produces a retrieval accuracy superior to any of the tested word n-gram models .	0	used-for	used-for	1
over two distinct datasets , we find that indexing according to simple #character bigrams# produces a retrieval accuracy superior to any of the tested $word n-gram models$ .	6	compare	compare	1
over two distinct datasets , we find that indexing according to simple $character bigrams$ produces a #retrieval accuracy# superior to any of the tested word n-gram models .	2	evaluate-for	evaluate-for	1
over two distinct datasets , we find that indexing according to simple character bigrams produces a #retrieval accuracy# superior to any of the tested $word n-gram models$ .	2	evaluate-for	evaluate-for	1
further , in their optimum configuration , #bag-of-words methods# are shown to be equivalent to $segment order-sensitive methods$ in terms of retrieval accuracy , but much faster .	6	compare	compare	1
further , in their optimum configuration , $bag-of-words methods$ are shown to be equivalent to segment order-sensitive methods in terms of #retrieval accuracy# , but much faster .	2	evaluate-for	evaluate-for	1
further , in their optimum configuration , bag-of-words methods are shown to be equivalent to $segment order-sensitive methods$ in terms of #retrieval accuracy# , but much faster .	2	evaluate-for	evaluate-for	1
in this paper we show how two standard #outputs# from information extraction -lrb- ie -rrb- systems - named entity annotations and scenario templates - can be used to enhance access to $text collections$ via a standard text browser .	0	used-for	used-for	1
in this paper we show how two standard $outputs$ from information extraction -lrb- ie -rrb- systems - #named entity annotations# and scenario templates - can be used to enhance access to text collections via a standard text browser .	3	hyponym-of	hyponym-of	1
in this paper we show how two standard outputs from information extraction -lrb- ie -rrb- systems - #named entity annotations# and $scenario templates$ - can be used to enhance access to text collections via a standard text browser .	1	conjunction	conjunction	1
in this paper we show how two standard $outputs$ from information extraction -lrb- ie -rrb- systems - named entity annotations and #scenario templates# - can be used to enhance access to text collections via a standard text browser .	3	hyponym-of	hyponym-of	1
in this paper we show how two standard outputs from information extraction -lrb- ie -rrb- systems - named entity annotations and scenario templates - can be used to enhance access to $text collections$ via a standard #text browser# .	0	used-for	used-for	1
we describe how this information is used in a #prototype system# designed to support information workers ' access to a $pharmaceutical news archive$ as part of their industry watch function .	0	used-for	used-for	1
we also report results of a preliminary , #qualitative user evaluation# of the $system$ , which while broadly positive indicates further work needs to be done on the interface to make users aware of the increased potential of ie-enhanced text browsers .	2	evaluate-for	evaluate-for	1
we present a new #model-based bundle adjustment algorithm# to recover the $3d model$ of a scene/object from a sequence of images with unknown motions .	0	used-for	used-for	1
we present a new model-based bundle adjustment algorithm to recover the $3d model$ of a scene/object from a sequence of #images# with unknown motions .	0	used-for	used-for	1
we present a new model-based bundle adjustment algorithm to recover the 3d model of a scene/object from a sequence of $images$ with #unknown motions# .	4	part-of	part-of	1
instead of representing scene/object by a collection of isolated 3d features -lrb- usually points -rrb- , our $algorithm$ uses a #surface# controlled by a small set of parameters .	0	used-for	used-for	1
compared with previous #model-based approaches# , our $approach$ has the following advantages .	6	compare	compare	1
first , instead of using the #model space# as a $regular-izer$ , we directly use it as our search space , thus resulting in a more elegant formulation with fewer unknowns and fewer equations .	0	used-for	used-for	1
first , instead of using the model space as a #regular-izer# , we directly use it as our $search space$ , thus resulting in a more elegant formulation with fewer unknowns and fewer equations .	6	compare	compare	1
first , instead of using the model space as a regular-izer , we directly use #it# as our $search space$ , thus resulting in a more elegant formulation with fewer unknowns and fewer equations .	0	used-for	used-for	1
third , regarding $face modeling$ , we use a very small set of #face metrics# -lrb- meaningful deformations -rrb- to parame-terize the face geometry , resulting in a smaller search space and a better posed system .	0	used-for	used-for	1
third , regarding face modeling , we use a very small set of #face metrics# -lrb- meaningful deformations -rrb- to parame-terize the $face geometry$ , resulting in a smaller search space and a better posed system .	0	used-for	used-for	1
third , regarding face modeling , we use a very small set of #face metrics# -lrb- meaningful deformations -rrb- to parame-terize the face geometry , resulting in a smaller $search space$ and a better posed system .	0	used-for	used-for	1
third , regarding face modeling , we use a very small set of #face metrics# -lrb- meaningful deformations -rrb- to parame-terize the face geometry , resulting in a smaller search space and a better $posed system$ .	0	used-for	used-for	1
experiments with both #synthetic and real data# show that this new $algorithm$ is faster , more accurate and more stable than existing ones .	2	evaluate-for	evaluate-for	1
experiments with both #synthetic and real data# show that this new algorithm is faster , more accurate and more stable than existing $ones$ .	2	evaluate-for	evaluate-for	1
experiments with both synthetic and real data show that this new #algorithm# is faster , more accurate and more stable than existing $ones$ .	6	compare	compare	1
this paper presents an #approach# to the $unsupervised learning of parts of speech$ which uses both morphological and syntactic information .	0	used-for	used-for	1
this paper presents an $approach$ to the unsupervised learning of parts of speech which uses both #morphological and syntactic information# .	0	used-for	used-for	1
while the #model# is more complex than $those$ which have been employed for unsupervised learning of pos tags in english , which use only syntactic information , the variety of languages in the world requires that we consider morphology as well .	6	compare	compare	1
while the model is more complex than #those# which have been employed for $unsupervised learning of pos tags in english$ , which use only syntactic information , the variety of languages in the world requires that we consider morphology as well .	0	used-for	used-for	1
while the model is more complex than $those$ which have been employed for unsupervised learning of pos tags in english , which use only #syntactic information# , the variety of languages in the world requires that we consider morphology as well .	0	used-for	used-for	1
in many languages , #morphology# provides better clues to a word 's category than $word order$ .	6	compare	compare	1
we present the #computational model# for $pos learning$ , and present results for applying it to bulgarian , a slavic language with relatively free word order and rich morphology .	0	used-for	used-for	1
we present the computational model for pos learning , and present results for applying $it$ to #bulgarian# , a slavic language with relatively free word order and rich morphology .	0	used-for	used-for	1
we present the computational model for pos learning , and present results for applying it to #bulgarian# , a $slavic language$ with relatively free word order and rich morphology .	3	hyponym-of	hyponym-of	1
we present the computational model for pos learning , and present results for applying it to $bulgarian$ , a slavic language with relatively #free word order# and rich morphology .	5	feature-of	feature-of	1
we present the computational model for pos learning , and present results for applying it to bulgarian , a slavic language with relatively #free word order# and $rich morphology$ .	1	conjunction	conjunction	1
we present the computational model for pos learning , and present results for applying it to $bulgarian$ , a slavic language with relatively free word order and #rich morphology# .	5	feature-of	feature-of	1
in $mt$ , the widely used approach is to apply a #chinese word segmenter# trained from manually annotated data , using a fixed lexicon .	0	used-for	used-for	1
in mt , the widely used approach is to apply a $chinese word segmenter$ trained from #manually annotated data# , using a fixed lexicon .	0	used-for	used-for	1
such #word segmentation# is not necessarily optimal for $translation$ .	0	used-for	used-for	1
we propose a #bayesian semi-supervised chinese word segmentation model# which uses both monolingual and bilingual information to derive a $segmentation$ suitable for mt .	0	used-for	used-for	1
we propose a $bayesian semi-supervised chinese word segmentation model$ which uses both #monolingual and bilingual information# to derive a segmentation suitable for mt .	0	used-for	used-for	1
we propose a bayesian semi-supervised chinese word segmentation model which uses both monolingual and bilingual information to derive a #segmentation# suitable for $mt$ .	0	used-for	used-for	1
experiments show that our #method# improves a state-of-the-art $mt system$ in a small and a large data environment .	6	compare	compare	1
in this paper we compare two competing #approaches# to $part-of-speech tagging$ , statistical and constraint-based disambiguation , using french as our test language .	0	used-for	used-for	1
in this paper we compare two competing $approaches$ to part-of-speech tagging , statistical and constraint-based disambiguation , using #french# as our test language .	0	used-for	used-for	1
we imposed a time limit on our experiment : the amount of time spent on the design of our #constraint system# was about the same as the time we used to train and test the easy-to-implement $statistical model$ .	6	compare	compare	1
the #accuracy# of the $statistical method$ is reasonably good , comparable to taggers for english .	2	evaluate-for	evaluate-for	1
the #accuracy# of the statistical method is reasonably good , comparable to $taggers$ for english .	2	evaluate-for	evaluate-for	1
the accuracy of the #statistical method# is reasonably good , comparable to $taggers$ for english .	6	compare	compare	1
the accuracy of the statistical method is reasonably good , comparable to #taggers# for $english$ .	0	used-for	used-for	1
#structured-light methods# actively generate $geometric correspondence data$ between projectors and cameras in order to facilitate robust 3d reconstruction .	0	used-for	used-for	1
structured-light methods actively generate #geometric correspondence data# between projectors and cameras in order to facilitate $robust 3d reconstruction$ .	0	used-for	used-for	1
in this paper , we present $photogeometric structured light$ whereby a standard #structured light method# is extended to include photometric methods .	4	part-of	part-of	1
in this paper , we present $photogeometric structured light$ whereby a standard structured light method is extended to include #photometric methods# .	4	part-of	part-of	1
#photometric processing# serves the double purpose of increasing the amount of $recovered surface detail$ and of enabling the structured-light setup to be robustly self-calibrated .	0	used-for	used-for	1
#photometric processing# serves the double purpose of increasing the amount of recovered surface detail and of enabling the $structured-light setup$ to be robustly self-calibrated .	0	used-for	used-for	1
further , our $framework$ uses a #photogeometric optimization# that supports the simultaneous use of multiple cameras and projectors and yields a single and accurate multi-view 3d model which best complies with photometric and geometric data .	0	used-for	used-for	1
further , our framework uses a photogeometric optimization that supports the simultaneous use of multiple cameras and projectors and yields a single and accurate $multi-view 3d model$ which best complies with #photometric and geometric data# .	0	used-for	used-for	1
in this paper , a discrimination and robustness oriented #adaptive learning procedure# is proposed to deal with the task of $syntactic ambiguity resolution$ .	0	used-for	used-for	1
owing to the problem of #insufficient training data# and $approximation error$ introduced by the language model , traditional statistical approaches , which resolve ambiguities by indirectly and implicitly using maximum likelihood method , fail to achieve high performance in real applications .	1	conjunction	conjunction	1
owing to the problem of insufficient training data and approximation error introduced by the language model , traditional #statistical approaches# , which resolve $ambiguities$ by indirectly and implicitly using maximum likelihood method , fail to achieve high performance in real applications .	0	used-for	used-for	1
owing to the problem of insufficient training data and approximation error introduced by the language model , traditional $statistical approaches$ , which resolve ambiguities by indirectly and implicitly using #maximum likelihood method# , fail to achieve high performance in real applications .	0	used-for	used-for	1
the #accuracy rate# of $syntactic disambiguation$ is raised from 46.0 % to 60.62 % by using this novel approach .	2	evaluate-for	evaluate-for	1
the accuracy rate of #syntactic disambiguation# is raised from 46.0 % to 60.62 % by using this novel $approach$ .	2	evaluate-for	evaluate-for	1
this paper presents a new #approach# to $statistical sentence generation$ in which alternative phrases are represented as packed sets of trees , or forests , and then ranked statistically to choose the best one .	0	used-for	used-for	1
#it# also facilitates more efficient $statistical ranking$ than a previous approach to statistical generation .	0	used-for	used-for	1
#it# also facilitates more efficient statistical ranking than a previous $approach$ to statistical generation .	6	compare	compare	1
it also facilitates more efficient statistical ranking than a previous #approach# to $statistical generation$ .	0	used-for	used-for	1
an efficient #ranking algorithm# is described , together with experimental results showing significant improvements over simple $enumeration$ or a lattice-based approach .	6	compare	compare	1
an efficient #ranking algorithm# is described , together with experimental results showing significant improvements over simple enumeration or a $lattice-based approach$ .	6	compare	compare	1
an efficient ranking algorithm is described , together with experimental results showing significant improvements over simple #enumeration# or a $lattice-based approach$ .	1	conjunction	conjunction	1
this article deals with the interpretation of conceptual operations underlying the communicative use of #natural language -lrb- nl -rrb-# within the $structured inheritance network -lrb- si-nets -rrb- paradigm$ .	0	used-for	used-for	1
the operations are reduced to functions of a formal language , thus changing the level of abstraction of the #operations# to be performed on $si-nets$ .	0	used-for	used-for	1
in this sense , #operations# on $si-nets$ are not merely isomorphic to single epistemological objects , but can be viewed as a simulation of processes on a different level , that pertaining to the conceptual system of nl .	0	used-for	used-for	1
in this sense , operations on si-nets are not merely isomorphic to single epistemological objects , but can be viewed as a simulation of processes on a different level , that pertaining to the $conceptual system$ of #nl# .	0	used-for	used-for	1
for this purpose , we have designed a version of #kl-one# which represents the epistemological level , while the new experimental language , $kl-conc$ , represents the conceptual level .	6	compare	compare	1
for this purpose , we have designed a version of $kl-one$ which represents the #epistemological level# , while the new experimental language , kl-conc , represents the conceptual level .	5	feature-of	feature-of	1
for this purpose , we have designed a version of kl-one which represents the epistemological level , while the new experimental language , $kl-conc$ , represents the #conceptual level# .	5	feature-of	feature-of	1
we present an #algorithm# for $calibrated camera relative pose estimation$ from lines .	0	used-for	used-for	1
we evaluate the performance of the $algorithm$ using #synthetic and real data# .	0	used-for	used-for	1
the intended use of the #algorithm# is with robust $hypothesize-and-test frameworks$ such as ransac .	1	conjunction	conjunction	1
the intended use of the algorithm is with robust $hypothesize-and-test frameworks$ such as #ransac# .	3	hyponym-of	hyponym-of	1
our #approach# is suitable for $urban and indoor environments$ where most lines are either parallel or orthogonal to each other .	0	used-for	used-for	1
in this paper , we present a #fully automated extraction system# , named intex , to identify $gene and protein interactions$ in biomedical text .	0	used-for	used-for	1
in this paper , we present a $fully automated extraction system$ , named #intex# , to identify gene and protein interactions in biomedical text .	3	hyponym-of	hyponym-of	1
in this paper , we present a fully automated extraction system , named intex , to identify $gene and protein interactions$ in #biomedical text# .	0	used-for	used-for	1
then , tagging $biological entities$ with the help of #biomedical and linguistic ontologies# .	0	used-for	used-for	1
our #extraction system# handles complex sentences and extracts $multiple and nested interactions$ specified in a sentence .	0	used-for	used-for	1
experimental evaluations with two other state of the art $extraction systems$ indicate that the #intex system# achieves better performance without the labor intensive pattern engineering requirement .	6	compare	compare	1
this paper introduces a #method# for $computational analysis of move structures$ in abstracts of research articles .	0	used-for	used-for	1
this paper introduces a method for $computational analysis of move structures$ in #abstracts of research articles# .	0	used-for	used-for	1
the method involves automatically gathering a large number of $abstracts$ from the #web# and building a language model of abstract moves .	0	used-for	used-for	1
the method involves automatically gathering a large number of abstracts from the web and building a $language model$ of #abstract moves# .	0	used-for	used-for	1
we also present a $prototype concordancer$ , #care# , which exploits the move-tagged abstracts for digital learning .	3	hyponym-of	hyponym-of	1
we also present a prototype concordancer , #care# , which exploits the $move-tagged abstracts$ for digital learning .	0	used-for	used-for	1
we also present a prototype concordancer , care , which exploits the #move-tagged abstracts# for $digital learning$ .	0	used-for	used-for	1
this #system# provides a promising $approach$ to web-based computer-assisted academic writing .	0	used-for	used-for	1
this system provides a promising #approach# to $web-based computer-assisted academic writing$ .	0	used-for	used-for	1
this work presents a #real-time system# for $multiple object tracking in dynamic scenes$ .	0	used-for	used-for	1
a unique characteristic of the #system# is its ability to cope with $long-duration and complete occlusion$ without a prior knowledge about the shape or motion of objects .	0	used-for	used-for	1
a unique characteristic of the system is its ability to cope with long-duration and complete occlusion without a #prior knowledge# about the $shape$ or motion of objects .	5	feature-of	feature-of	1
a unique characteristic of the system is its ability to cope with long-duration and complete occlusion without a #prior knowledge# about the shape or $motion of objects$ .	5	feature-of	feature-of	1
a unique characteristic of the system is its ability to cope with long-duration and complete occlusion without a prior knowledge about the #shape# or $motion of objects$ .	1	conjunction	conjunction	1
the $system$ produces good segment and #tracking# results at a frame rate of 15-20 fps for image size of 320x240 , as demonstrated by extensive experiments performed using video sequences under different conditions indoor and outdoor with long-duration and complete occlusions in changing background .	2	evaluate-for	evaluate-for	1
we propose a #method# of $organizing reading materials$ for vocabulary learning .	0	used-for	used-for	1
we propose a method of #organizing reading materials# for $vocabulary learning$ .	0	used-for	used-for	1
we used a specialized vocabulary for an english certification test as the target vocabulary and used #english wikipedia# , a $free-content encyclopedia$ , as the target corpus .	3	hyponym-of	hyponym-of	1
a novel #bootstrapping approach# to $named entity -lrb- ne -rrb- tagging$ using concept-based seeds and successive learners is presented .	0	used-for	used-for	1
a novel $bootstrapping approach$ to named entity -lrb- ne -rrb- tagging using #concept-based seeds# and successive learners is presented .	0	used-for	used-for	1
a novel bootstrapping approach to named entity -lrb- ne -rrb- tagging using #concept-based seeds# and $successive learners$ is presented .	1	conjunction	conjunction	1
a novel $bootstrapping approach$ to named entity -lrb- ne -rrb- tagging using concept-based seeds and #successive learners# is presented .	0	used-for	used-for	1
this approach only requires a few common noun or pronoun seeds that correspond to the concept for the targeted $ne$ , e.g. he/she/man / woman for #person ne# .	3	hyponym-of	hyponym-of	1
the $bootstrapping procedure$ is implemented as training two #successive learners# .	0	used-for	used-for	1
first , #decision list# is used to learn the $parsing-based ne rules$ .	0	used-for	used-for	1
the resulting #ne system# approaches $supervised ne$ performance for some ne types .	0	used-for	used-for	1
we present the first known empirical test of an increasingly common speculative claim , by evaluating a representative $chinese-to-english smt model$ directly on #word sense disambiguation# performance , using standard wsd evaluation methodology and datasets from the senseval-3 chinese lexical sample task .	2	evaluate-for	evaluate-for	1
we present the first known empirical test of an increasingly common speculative claim , by evaluating a representative $chinese-to-english smt model$ directly on word sense disambiguation performance , using standard #wsd evaluation methodology# and datasets from the senseval-3 chinese lexical sample task .	2	evaluate-for	evaluate-for	1
we present the first known empirical test of an increasingly common speculative claim , by evaluating a representative $chinese-to-english smt model$ directly on word sense disambiguation performance , using standard wsd evaluation methodology and datasets from the #senseval-3 chinese lexical sample task# .	2	evaluate-for	evaluate-for	1
much effort has been put in designing and evaluating $dedicated word sense disambiguation -lrb- wsd -rrb- models$ , in particular with the #senseval series of workshops# .	2	evaluate-for	evaluate-for	1
at the same time , the recent improvements in the #bleu scores# of $statistical machine translation -lrb- smt -rrb-$ suggests that smt models are good at predicting the right translation of the words in source language sentences .	2	evaluate-for	evaluate-for	1
at the same time , the recent improvements in the bleu scores of statistical machine translation -lrb- smt -rrb- suggests that #smt models# are good at predicting the right $translation$ of the words in source language sentences .	0	used-for	used-for	1
surprisingly however , the #wsd accuracy# of $smt models$ has never been evaluated and compared with that of the dedicated wsd models .	2	evaluate-for	evaluate-for	1
surprisingly however , the $wsd accuracy$ of smt models has never been evaluated and compared with #that# of the dedicated wsd models .	6	compare	compare	1
we present controlled experiments showing the #wsd accuracy# of current typical $smt models$ to be significantly lower than that of all the dedicated wsd models considered .	2	evaluate-for	evaluate-for	1
we present controlled experiments showing the $wsd accuracy$ of current typical smt models to be significantly lower than #that# of all the dedicated wsd models considered .	6	compare	compare	1
this tends to support the view that despite recent speculative claims to the contrary , current #smt models# do have limitations in comparison with $dedicated wsd models$ , and that smt should benefit from the better predictions made by the wsd models .	6	compare	compare	1
this tends to support the view that despite recent speculative claims to the contrary , current smt models do have limitations in comparison with dedicated wsd models , and that $smt$ should benefit from the better predictions made by the #wsd models# .	0	used-for	used-for	1
in this paper we present a novel , customizable : $ie paradigm$ that takes advantage of #predicate-argument structures# .	0	used-for	used-for	1
$it$ is based on : -lrb- 1 -rrb- an extended set of #features# ; and -lrb- 2 -rrb- inductive decision tree learning .	0	used-for	used-for	1
it is based on : -lrb- 1 -rrb- an extended set of #features# ; and -lrb- 2 -rrb- $inductive decision tree learning$ .	1	conjunction	conjunction	1
$it$ is based on : -lrb- 1 -rrb- an extended set of features ; and -lrb- 2 -rrb- #inductive decision tree learning# .	0	used-for	used-for	1
the experimental results prove our claim that accurate #predicate-argument structures# enable high quality $ie$ results .	0	used-for	used-for	1
in this paper we present a #statistical profile# of the $named entity task$ , a specific information extraction task for which corpora in several languages are available .	0	used-for	used-for	1
in this paper we present a statistical profile of the #named entity task# , a specific $information extraction task$ for which corpora in several languages are available .	3	hyponym-of	hyponym-of	1
using the results of the #statistical analysis# , we propose an $algorithm$ for lower bound estimation for named entity corpora and discuss the significance of the cross-lingual comparisons provided by the analysis .	0	used-for	used-for	1
using the results of the statistical analysis , we propose an #algorithm# for $lower bound estimation$ for named entity corpora and discuss the significance of the cross-lingual comparisons provided by the analysis .	0	used-for	used-for	1
using the results of the statistical analysis , we propose an algorithm for #lower bound estimation# for $named entity corpora$ and discuss the significance of the cross-lingual comparisons provided by the analysis .	0	used-for	used-for	1
we attack an inexplicably $under-explored language genre of spoken language$ -- #lyrics in music# -- via completely unsuper-vised induction of an smt-style stochastic transduction grammar for hip hop lyrics , yielding a fully-automatically learned challenge-response system that produces rhyming lyrics given an input .	3	hyponym-of	hyponym-of	1
we attack an inexplicably $under-explored language genre of spoken language$ -- lyrics in music -- via completely #unsuper-vised induction# of an smt-style stochastic transduction grammar for hip hop lyrics , yielding a fully-automatically learned challenge-response system that produces rhyming lyrics given an input .	0	used-for	used-for	1
we attack an inexplicably under-explored language genre of spoken language -- lyrics in music -- via completely #unsuper-vised induction# of an $smt-style stochastic transduction grammar$ for hip hop lyrics , yielding a fully-automatically learned challenge-response system that produces rhyming lyrics given an input .	0	used-for	used-for	1
we attack an inexplicably under-explored language genre of spoken language -- lyrics in music -- via completely #unsuper-vised induction# of an smt-style stochastic transduction grammar for hip hop lyrics , yielding a $fully-automatically learned challenge-response system$ that produces rhyming lyrics given an input .	0	used-for	used-for	1
we attack an inexplicably under-explored language genre of spoken language -- lyrics in music -- via completely unsuper-vised induction of an $smt-style stochastic transduction grammar$ for #hip hop lyrics# , yielding a fully-automatically learned challenge-response system that produces rhyming lyrics given an input .	5	feature-of	feature-of	1
we attack an inexplicably under-explored language genre of spoken language -- lyrics in music -- via completely unsuper-vised induction of an smt-style stochastic transduction grammar for hip hop lyrics , yielding a #fully-automatically learned challenge-response system# that produces $rhyming lyrics$ given an input .	0	used-for	used-for	1
in spite of the level of difficulty of the challenge , the #model# nevertheless produces fluent output as judged by human evaluators , and performs significantly better than widely used $phrase-based smt models$ upon the same task .	6	compare	compare	1
in spite of the level of difficulty of the challenge , the $model$ nevertheless produces fluent output as judged by human evaluators , and performs significantly better than widely used phrase-based smt models upon the same #task# .	2	evaluate-for	evaluate-for	1
in spite of the level of difficulty of the challenge , the model nevertheless produces fluent output as judged by human evaluators , and performs significantly better than widely used $phrase-based smt models$ upon the same #task# .	2	evaluate-for	evaluate-for	1
in this paper , we investigate the problem of automatically $predicting segment boundaries$ in #spoken multiparty dialogue# .	0	used-for	used-for	1
we first apply #approaches# that have been proposed for $predicting top-level topic shifts$ to the problem of identifying subtopic boundaries .	0	used-for	used-for	1
we first apply #approaches# that have been proposed for predicting top-level topic shifts to the problem of $identifying subtopic boundaries$ .	0	used-for	used-for	1
we first apply approaches that have been proposed for #predicting top-level topic shifts# to the problem of $identifying subtopic boundaries$ .	5	feature-of	feature-of	1
we then explore the impact on performance of using #asr output# as opposed to $human transcription$ .	6	compare	compare	1
examination of the effect of features shows that $predicting top-level and predicting subtopic boundaries$ are two distinct tasks : -lrb- 1 -rrb- for #predicting subtopic boundaries# , the lexical cohesion-based approach alone can achieve competitive results , -lrb- 2 -rrb- for predicting top-level boundaries , the machine learning approach that combines lexical-cohesion and conversational features performs best , and -lrb- 3 -rrb- conversational cues , such as cue phrases and overlapping speech , are better indicators for the top-level prediction task .	4	part-of	part-of	1
examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks : -lrb- 1 -rrb- for $predicting subtopic boundaries$ , the #lexical cohesion-based approach# alone can achieve competitive results , -lrb- 2 -rrb- for predicting top-level boundaries , the machine learning approach that combines lexical-cohesion and conversational features performs best , and -lrb- 3 -rrb- conversational cues , such as cue phrases and overlapping speech , are better indicators for the top-level prediction task .	0	used-for	used-for	1
examination of the effect of features shows that $predicting top-level and predicting subtopic boundaries$ are two distinct tasks : -lrb- 1 -rrb- for predicting subtopic boundaries , the lexical cohesion-based approach alone can achieve competitive results , -lrb- 2 -rrb- for #predicting top-level boundaries# , the machine learning approach that combines lexical-cohesion and conversational features performs best , and -lrb- 3 -rrb- conversational cues , such as cue phrases and overlapping speech , are better indicators for the top-level prediction task .	4	part-of	part-of	1
examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks : -lrb- 1 -rrb- for predicting subtopic boundaries , the lexical cohesion-based approach alone can achieve competitive results , -lrb- 2 -rrb- for $predicting top-level boundaries$ , the #machine learning approach# that combines lexical-cohesion and conversational features performs best , and -lrb- 3 -rrb- conversational cues , such as cue phrases and overlapping speech , are better indicators for the top-level prediction task .	0	used-for	used-for	1
examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks : -lrb- 1 -rrb- for predicting subtopic boundaries , the lexical cohesion-based approach alone can achieve competitive results , -lrb- 2 -rrb- for predicting top-level boundaries , the $machine learning approach$ that combines #lexical-cohesion and conversational features# performs best , and -lrb- 3 -rrb- conversational cues , such as cue phrases and overlapping speech , are better indicators for the top-level prediction task .	1	conjunction	conjunction	1
examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks : -lrb- 1 -rrb- for predicting subtopic boundaries , the lexical cohesion-based approach alone can achieve competitive results , -lrb- 2 -rrb- for predicting top-level boundaries , the machine learning approach that combines lexical-cohesion and conversational features performs best , and -lrb- 3 -rrb- $conversational cues$ , such as #cue phrases# and overlapping speech , are better indicators for the top-level prediction task .	3	hyponym-of	hyponym-of	1
examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks : -lrb- 1 -rrb- for predicting subtopic boundaries , the lexical cohesion-based approach alone can achieve competitive results , -lrb- 2 -rrb- for predicting top-level boundaries , the machine learning approach that combines lexical-cohesion and conversational features performs best , and -lrb- 3 -rrb- $conversational cues$ , such as cue phrases and #overlapping speech# , are better indicators for the top-level prediction task .	3	hyponym-of	hyponym-of	1
examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks : -lrb- 1 -rrb- for predicting subtopic boundaries , the lexical cohesion-based approach alone can achieve competitive results , -lrb- 2 -rrb- for predicting top-level boundaries , the machine learning approach that combines lexical-cohesion and conversational features performs best , and -lrb- 3 -rrb- conversational cues , such as $cue phrases$ and #overlapping speech# , are better indicators for the top-level prediction task .	1	conjunction	conjunction	1
examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks : -lrb- 1 -rrb- for predicting subtopic boundaries , the lexical cohesion-based approach alone can achieve competitive results , -lrb- 2 -rrb- for predicting top-level boundaries , the machine learning approach that combines lexical-cohesion and conversational features performs best , and -lrb- 3 -rrb- conversational cues , such as cue phrases and overlapping speech , are better #indicators# for the $top-level prediction task$ .	0	used-for	used-for	1
we also find that the #transcription errors# inevitable in $asr output$ have a negative impact on models that combine lexical-cohesion and conversational features , but do not change the general preference of approach for the two tasks .	5	feature-of	feature-of	1
we also find that the transcription errors inevitable in asr output have a negative impact on #models# that combine $lexical-cohesion and conversational features$ , but do not change the general preference of approach for the two tasks .	1	conjunction	conjunction	1
we describe a simple #unsupervised technique# for learning $morphology$ by identifying hubs in an automaton .	0	used-for	used-for	1
we describe a simple $unsupervised technique$ for learning morphology by identifying #hubs# in an automaton .	0	used-for	used-for	1
we describe a simple unsupervised technique for learning morphology by identifying #hubs# in an $automaton$ .	4	part-of	part-of	1
for our purposes , a #hub# is a $node$ in a graph with in-degree greater than one and out-degree greater than one .	3	hyponym-of	hyponym-of	1
for our purposes , a hub is a #node# in a $graph$ with in-degree greater than one and out-degree greater than one .	4	part-of	part-of	1
we create a #word-trie# , transform it into a minimal dfa , then identify $hubs$ .	0	used-for	used-for	1
we create a word-trie , transform it into a #minimal dfa# , then identify $hubs$ .	0	used-for	used-for	1
in $bayesian machine learning$ , #conjugate priors# are popular , mostly due to mathematical convenience .	4	part-of	part-of	1
specifically , we formulate the $conjugate prior$ in the form of #bregman divergence# and show that it is the inherent geometry of conjugate priors that makes them appropriate and intuitive .	5	feature-of	feature-of	1
we use this #geometric understanding of conjugate priors# to derive the $hyperparameters$ and expression of the prior used to couple the generative and discriminative components of a hybrid model for semi-supervised learning .	0	used-for	used-for	1
we use this geometric understanding of conjugate priors to derive the hyperparameters and expression of the #prior# used to couple the $generative and discriminative components$ of a hybrid model for semi-supervised learning .	0	used-for	used-for	1
we use this geometric understanding of conjugate priors to derive the hyperparameters and expression of the prior used to couple the #generative and discriminative components# of a $hybrid model$ for semi-supervised learning .	4	part-of	part-of	1
we use this geometric understanding of conjugate priors to derive the hyperparameters and expression of the prior used to couple the generative and discriminative components of a #hybrid model# for $semi-supervised learning$ .	0	used-for	used-for	1
this paper defines a $generative probabilistic model of parse trees$ , which we call #pcfg-la# .	3	hyponym-of	hyponym-of	1
this $model$ is an extension of #pcfg# in which non-terminal symbols are augmented with latent variables .	0	used-for	used-for	1
this model is an extension of $pcfg$ in which #non-terminal symbols# are augmented with latent variables .	4	part-of	part-of	1
this model is an extension of pcfg in which $non-terminal symbols$ are augmented with #latent variables# .	0	used-for	used-for	1
$finegrained cfg rules$ are automatically induced from a #parsed corpus# by training a pcfg-la model using an em-algorithm .	0	used-for	used-for	1
finegrained cfg rules are automatically induced from a parsed corpus by training a $pcfg-la model$ using an #em-algorithm# .	0	used-for	used-for	1
because $exact parsing$ with a #pcfg-la# is np-hard , several approximations are described and empirically compared .	0	used-for	used-for	1
in experiments using the #penn wsj corpus# , our automatically trained $model$ gave a performance of 86.6 % -lrb- f1 , sentences < 40 words -rrb- , which is comparable to that of an unlexicalized pcfg parser created using extensive manual feature selection .	2	evaluate-for	evaluate-for	1
in experiments using the #penn wsj corpus# , our automatically trained model gave a performance of 86.6 % -lrb- f1 , sentences < 40 words -rrb- , which is comparable to that of an $unlexicalized pcfg parser$ created using extensive manual feature selection .	2	evaluate-for	evaluate-for	1
in experiments using the penn wsj corpus , our automatically trained #model# gave a performance of 86.6 % -lrb- f1 , sentences < 40 words -rrb- , which is comparable to that of an $unlexicalized pcfg parser$ created using extensive manual feature selection .	6	compare	compare	1
in experiments using the penn wsj corpus , our automatically trained $model$ gave a performance of 86.6 % -lrb- #f1# , sentences < 40 words -rrb- , which is comparable to that of an unlexicalized pcfg parser created using extensive manual feature selection .	2	evaluate-for	evaluate-for	1
in experiments using the penn wsj corpus , our automatically trained model gave a performance of 86.6 % -lrb- #f1# , sentences < 40 words -rrb- , which is comparable to that of an $unlexicalized pcfg parser$ created using extensive manual feature selection .	2	evaluate-for	evaluate-for	1
in experiments using the penn wsj corpus , our automatically trained model gave a performance of 86.6 % -lrb- f1 , sentences < 40 words -rrb- , which is comparable to that of an $unlexicalized pcfg parser$ created using extensive #manual feature selection# .	0	used-for	used-for	1
first , we present a new paradigm for $speaker-independent -lrb- si -rrb- training of hidden markov models -lrb- hmm -rrb-$ , which uses a large amount of #speech# from a few speakers instead of the traditional practice of using a little speech from many speakers .	0	used-for	used-for	1
in addition , combination of the training speakers is done by #averaging the statistics of independently trained models# rather than the usual $pooling of all the speech data$ from many speakers prior to training .	6	compare	compare	1
with only 12 training speakers for $si recognition$ , we achieved a 7.5 % #word error rate# on a standard grammar and test set from the darpa resource management corpus .	2	evaluate-for	evaluate-for	1
with only 12 training speakers for $si recognition$ , we achieved a 7.5 % word error rate on a standard grammar and test set from the #darpa resource management corpus# .	2	evaluate-for	evaluate-for	1
second , we show a significant improvement for $speaker adaptation -lrb- sa -rrb-$ using the new #si corpus# and a small amount of speech from the new -lrb- target -rrb- speaker .	2	evaluate-for	evaluate-for	1
using only 40 utterances from the target speaker for $adaptation$ , the #error rate# dropped to 4.1 % -- a 45 % reduction in error compared to the si result .	2	evaluate-for	evaluate-for	1
#dictionary construction# , one of the most difficult tasks in developing a $machine translation system$ , is expensive .	4	part-of	part-of	1
to avoid this problem , we investigate how we build a $dictionary$ using existing #linguistic resources# .	0	used-for	used-for	1
our algorithm can be applied to any language pairs , but for the present we focus on building a $korean-to-japanese dictionary$ using #english# as a pivot .	0	used-for	used-for	1
we attempt three ways of #automatic construction# to corroborate the effect of the $directionality of dictionaries$ .	2	evaluate-for	evaluate-for	1
first , we introduce $`` one-time look up '' method$ using a #korean-to-english and a japanese-to-english dictionary# .	0	used-for	used-for	1
second , we show a $method$ using #`` overlapping constraint ''# with a korean-to-english dictionary and an english-to-japanese dictionary .	0	used-for	used-for	1
second , we show a $method$ using `` overlapping constraint '' with a #korean-to-english dictionary# and an english-to-japanese dictionary .	0	used-for	used-for	1
second , we show a method using `` overlapping constraint '' with a #korean-to-english dictionary# and an $english-to-japanese dictionary$ .	1	conjunction	conjunction	1
second , we show a $method$ using `` overlapping constraint '' with a korean-to-english dictionary and an #english-to-japanese dictionary# .	0	used-for	used-for	1
third , we consider another alternative #method# rarely used for building a $dictionary$ : an english-to-korean dictionary and english-to-japanese dictionary .	0	used-for	used-for	1
third , we consider another alternative method rarely used for building a $dictionary$ : an #english-to-korean dictionary# and english-to-japanese dictionary .	3	hyponym-of	hyponym-of	1
third , we consider another alternative method rarely used for building a dictionary : an #english-to-korean dictionary# and $english-to-japanese dictionary$ .	1	conjunction	conjunction	1
third , we consider another alternative method rarely used for building a $dictionary$ : an english-to-korean dictionary and #english-to-japanese dictionary# .	3	hyponym-of	hyponym-of	1
an empirical comparison of #cfg filtering techniques# for $ltag$ and hpsg is presented .	0	used-for	used-for	1
an empirical comparison of #cfg filtering techniques# for ltag and $hpsg$ is presented .	0	used-for	used-for	1
an empirical comparison of cfg filtering techniques for #ltag# and $hpsg$ is presented .	6	compare	compare	1
we demonstrate that an #approximation of hpsg# produces a more effective $cfg filter$ than that of ltag .	0	used-for	used-for	1
we demonstrate that an approximation of hpsg produces a more effective #cfg filter# than $that$ of ltag .	6	compare	compare	1
we demonstrate that an approximation of hpsg produces a more effective cfg filter than #that# of $ltag$ .	0	used-for	used-for	1
$syntax-based statistical machine translation -lrb- mt -rrb-$ aims at applying #statistical models# to structured data .	0	used-for	used-for	1
syntax-based statistical machine translation -lrb- mt -rrb- aims at applying $statistical models$ to #structured data# .	0	used-for	used-for	1
in this paper , we present a $syntax-based statistical machine translation system$ based on a #probabilistic synchronous dependency insertion grammar# .	0	used-for	used-for	1
#synchronous dependency insertion grammars# are a version of $synchronous grammars$ defined on dependency trees .	3	hyponym-of	hyponym-of	1
$synchronous dependency insertion grammars$ are a version of synchronous grammars defined on #dependency trees# .	5	feature-of	feature-of	1
we first introduce our #approach# to inducing such a $grammar$ from parallel corpora .	0	used-for	used-for	1
we first introduce our approach to inducing such a $grammar$ from #parallel corpora# .	0	used-for	used-for	1
second , we describe the #graphical model# for the $machine translation task$ , which can also be viewed as a stochastic tree-to-tree transducer .	0	used-for	used-for	1
second , we describe the $graphical model$ for the machine translation task , which can also be viewed as a #stochastic tree-to-tree transducer# .	0	used-for	used-for	1
we introduce a #polynomial time decoding algorithm# for the $model$ .	0	used-for	used-for	1
we evaluate the outputs of our $mt system$ using the #nist and bleu automatic mt evaluation software# .	0	used-for	used-for	1
the result shows that our #system# outperforms the $baseline system$ based on the ibm models in both translation speed and quality .	6	compare	compare	1
the result shows that our system outperforms the $baseline system$ based on the #ibm models# in both translation speed and quality .	0	used-for	used-for	1
the result shows that our $system$ outperforms the baseline system based on the ibm models in both #translation speed and quality# .	2	evaluate-for	evaluate-for	1
the result shows that our system outperforms the $baseline system$ based on the ibm models in both #translation speed and quality# .	2	evaluate-for	evaluate-for	1
we propose a $framework$ to derive the distance between concepts from #distributional measures of word co-occurrences# .	0	used-for	used-for	1
we show that the newly proposed #concept-distance measures# outperform traditional distributional word-distance measures in the $tasks$ of -lrb- 1 -rrb- ranking word pairs in order of semantic distance , and -lrb- 2 -rrb- correcting real-word spelling errors .	0	used-for	used-for	1
we show that the newly proposed $concept-distance measures$ outperform traditional #distributional word-distance measures# in the tasks of -lrb- 1 -rrb- ranking word pairs in order of semantic distance , and -lrb- 2 -rrb- correcting real-word spelling errors .	6	compare	compare	1
we show that the newly proposed concept-distance measures outperform traditional #distributional word-distance measures# in the $tasks$ of -lrb- 1 -rrb- ranking word pairs in order of semantic distance , and -lrb- 2 -rrb- correcting real-word spelling errors .	0	used-for	used-for	1
we show that the newly proposed concept-distance measures outperform traditional distributional word-distance measures in the $tasks$ of -lrb- 1 -rrb- #ranking word pairs in order of semantic distance# , and -lrb- 2 -rrb- correcting real-word spelling errors .	3	hyponym-of	hyponym-of	1
we show that the newly proposed concept-distance measures outperform traditional distributional word-distance measures in the $tasks$ of -lrb- 1 -rrb- ranking word pairs in order of semantic distance , and -lrb- 2 -rrb- #correcting real-word spelling errors# .	3	hyponym-of	hyponym-of	1
we show that the newly proposed concept-distance measures outperform traditional distributional word-distance measures in the tasks of -lrb- 1 -rrb- $ranking word pairs in order of semantic distance$ , and -lrb- 2 -rrb- #correcting real-word spelling errors# .	1	conjunction	conjunction	1
in the latter #task# , of all the $wordnet-based measures$ , only that proposed by jiang and conrath outperforms the best distributional concept-distance measures .	2	evaluate-for	evaluate-for	1
in the latter #task# , of all the wordnet-based measures , only that proposed by jiang and conrath outperforms the best $distributional concept-distance measures$ .	2	evaluate-for	evaluate-for	1
in the latter task , of all the $wordnet-based measures$ , only that proposed by jiang and conrath outperforms the best #distributional concept-distance measures# .	6	compare	compare	1
one of the main results of this work is the definition of a relation between #broad semantic classes# and $lcs meaning components$ .	1	conjunction	conjunction	1
our #acquisition program - lexicall -# takes , as input , the result of previous work on verb classification and thematic grid tagging , and outputs $lcs representations$ for different languages .	0	used-for	used-for	1
our $acquisition program - lexicall -$ takes , as input , the result of previous work on #verb classification# and thematic grid tagging , and outputs lcs representations for different languages .	0	used-for	used-for	1
our acquisition program - lexicall - takes , as input , the result of previous work on #verb classification# and $thematic grid tagging$ , and outputs lcs representations for different languages .	1	conjunction	conjunction	1
our $acquisition program - lexicall -$ takes , as input , the result of previous work on verb classification and #thematic grid tagging# , and outputs lcs representations for different languages .	0	used-for	used-for	1
these #representations# have been ported into $english , arabic and spanish lexicons$ , each containing approximately 9000 verbs .	0	used-for	used-for	1
we are currently using these #lexicons# in an $operational foreign language tutoring$ and machine translation .	0	used-for	used-for	1
we are currently using these #lexicons# in an operational foreign language tutoring and $machine translation$ .	0	used-for	used-for	1
we are currently using these lexicons in an #operational foreign language tutoring# and $machine translation$ .	1	conjunction	conjunction	1
the theoretical study of the #range concatenation grammar -lsb- rcg -rsb- formalism# has revealed many attractive properties which may be used in $nlp$ .	0	used-for	used-for	1
in particular , $range concatenation languages -lsb- rcl -rsb-$ can be parsed in #polynomial time# and many classical grammatical formalisms can be translated into equivalent rcgs without increasing their worst-case parsing time complexity .	5	feature-of	feature-of	1
in particular , range concatenation languages -lsb- rcl -rsb- can be parsed in polynomial time and many classical $grammatical formalisms$ can be translated into equivalent rcgs without increasing their #worst-case parsing time complexity# .	2	evaluate-for	evaluate-for	1
for example , after translation into an equivalent rcg , any $tree adjoining grammar$ can be parsed in #o -lrb- n6 -rrb- time# .	5	feature-of	feature-of	1
in this paper , we study a #parsing technique# whose purpose is to improve the practical efficiency of $rcl parsers$ .	0	used-for	used-for	1
the non-deterministic parsing choices of the #main parser# for a $language l$ are directed by a guide which uses the shared derivation forest output by a prior rcl parser for a suitable superset of l .	0	used-for	used-for	1
the non-deterministic parsing choices of the main parser for a language l are directed by a guide which uses the $shared derivation forest$ output by a prior #rcl parser# for a suitable superset of l .	0	used-for	used-for	1
the results of a practical evaluation of this $method$ on a #wide coverage english grammar# are given .	2	evaluate-for	evaluate-for	1
in this paper we introduce #ant-q# , a family of algorithms which present many similarities with q-learning -lrb- watkins , 1989 -rrb- , and which we apply to the solution of $symmetric and asym-metric instances of the traveling salesman problem -lrb- tsp -rrb-$ .	0	used-for	used-for	1
$ant-q algorithms$ were inspired by work on the #ant system -lrb- as -rrb-# , a distributed algorithm for combinatorial optimization based on the metaphor of ant colonies which was recently proposed in -lrb- dorigo , 1992 ; dorigo , maniezzo and colorni , 1996 -rrb- .	0	used-for	used-for	1
ant-q algorithms were inspired by work on the #ant system -lrb- as -rrb-# , a $distributed algorithm$ for combinatorial optimization based on the metaphor of ant colonies which was recently proposed in -lrb- dorigo , 1992 ; dorigo , maniezzo and colorni , 1996 -rrb- .	3	hyponym-of	hyponym-of	1
ant-q algorithms were inspired by work on the ant system -lrb- as -rrb- , a #distributed algorithm# for $combinatorial optimization$ based on the metaphor of ant colonies which was recently proposed in -lrb- dorigo , 1992 ; dorigo , maniezzo and colorni , 1996 -rrb- .	0	used-for	used-for	1
we show that #as# is a particular instance of the $ant-q family$ , and that there are instances of this family which perform better than as .	3	hyponym-of	hyponym-of	1
we show that as is a particular instance of the ant-q family , and that there are #instances# of this $family$ which perform better than as .	4	part-of	part-of	1
we show that as is a particular instance of the ant-q family , and that there are #instances# of this family which perform better than $as$ .	6	compare	compare	1
we experimentally investigate the functioning of ant-q and we show that the results obtained by #ant-q# on $symmetric tsp$ 's are competitive with those obtained by other heuristic approaches based on neural networks or local search .	0	used-for	used-for	1
we experimentally investigate the functioning of ant-q and we show that the results obtained by #ant-q# on symmetric tsp 's are competitive with those obtained by other $heuristic approaches$ based on neural networks or local search .	6	compare	compare	1
we experimentally investigate the functioning of ant-q and we show that the results obtained by ant-q on symmetric tsp 's are competitive with those obtained by other $heuristic approaches$ based on #neural networks# or local search .	0	used-for	used-for	1
we experimentally investigate the functioning of ant-q and we show that the results obtained by ant-q on symmetric tsp 's are competitive with those obtained by other heuristic approaches based on #neural networks# or $local search$ .	1	conjunction	conjunction	1
we experimentally investigate the functioning of ant-q and we show that the results obtained by ant-q on symmetric tsp 's are competitive with those obtained by other $heuristic approaches$ based on neural networks or #local search# .	0	used-for	used-for	1
finally , we apply #ant-q# to some difficult $asymmetric tsp$ 's obtaining very good results : ant-q was able to find solutions of a quality which usually can be found only by very specialized algorithms .	0	used-for	used-for	1
in this paper , we develop a #geometric framework# for $linear or nonlinear discriminant subspace learning and classification$ .	0	used-for	used-for	1
in our framework , the $structures of classes$ are conceptualized as a #semi-riemannian manifold# which is considered as a submanifold embedded in an ambient semi-riemannian space .	0	used-for	used-for	1
in our framework , the structures of classes are conceptualized as a semi-riemannian manifold which is considered as a #submanifold# embedded in an $ambient semi-riemannian space$ .	4	part-of	part-of	1
the $class structures$ of original samples can be characterized and deformed by #local metrics of the semi-riemannian space# .	0	used-for	used-for	1
$semi-riemannian metrics$ are uniquely determined by the #smoothing of discrete functions# and the nullity of the semi-riemannian space .	0	used-for	used-for	1
semi-riemannian metrics are uniquely determined by the #smoothing of discrete functions# and the $nullity of the semi-riemannian space$ .	1	conjunction	conjunction	1
$semi-riemannian metrics$ are uniquely determined by the smoothing of discrete functions and the #nullity of the semi-riemannian space# .	0	used-for	used-for	1
based on the geometrization of class structures , optimizing $class structures$ in the #feature space# is equivalent to maximizing the quadratic quantities of metric tensors in the semi-riemannian space .	5	feature-of	feature-of	1
based on the geometrization of class structures , optimizing class structures in the feature space is equivalent to maximizing the $quadratic quantities of metric tensors$ in the #semi-riemannian space# .	5	feature-of	feature-of	1
based on the proposed #framework# , a novel $algorithm$ , dubbed as semi-riemannian discriminant analysis -lrb- srda -rrb- , is presented for subspace-based classification .	0	used-for	used-for	1
based on the proposed framework , a novel #algorithm# , dubbed as semi-riemannian discriminant analysis -lrb- srda -rrb- , is presented for $subspace-based classification$ .	0	used-for	used-for	1
the performance of #srda# is tested on face recognition -lrb- singular case -rrb- and handwritten capital letter classification -lrb- nonsingular case -rrb- against existing $algorithms$ .	6	compare	compare	1
the performance of $srda$ is tested on #face recognition -lrb- singular case# -rrb- and handwritten capital letter classification -lrb- nonsingular case -rrb- against existing algorithms .	2	evaluate-for	evaluate-for	1
the performance of srda is tested on #face recognition -lrb- singular case# -rrb- and $handwritten capital letter classification -lrb- nonsingular case -rrb-$ against existing algorithms .	1	conjunction	conjunction	1
the performance of srda is tested on #face recognition -lrb- singular case# -rrb- and handwritten capital letter classification -lrb- nonsingular case -rrb- against existing $algorithms$ .	2	evaluate-for	evaluate-for	1
the performance of $srda$ is tested on face recognition -lrb- singular case -rrb- and #handwritten capital letter classification -lrb- nonsingular case -rrb-# against existing algorithms .	2	evaluate-for	evaluate-for	1
the performance of srda is tested on face recognition -lrb- singular case -rrb- and #handwritten capital letter classification -lrb- nonsingular case -rrb-# against existing $algorithms$ .	2	evaluate-for	evaluate-for	1
the experimental results show that #srda# works well on $recognition$ and classification , implying that semi-riemannian geometry is a promising new tool for pattern recognition and machine learning .	0	used-for	used-for	1
the experimental results show that #srda# works well on recognition and $classification$ , implying that semi-riemannian geometry is a promising new tool for pattern recognition and machine learning .	0	used-for	used-for	1
the experimental results show that srda works well on #recognition# and $classification$ , implying that semi-riemannian geometry is a promising new tool for pattern recognition and machine learning .	1	conjunction	conjunction	1
the experimental results show that srda works well on recognition and classification , implying that #semi-riemannian geometry# is a promising new tool for $pattern recognition$ and machine learning .	0	used-for	used-for	1
the experimental results show that srda works well on recognition and classification , implying that #semi-riemannian geometry# is a promising new tool for pattern recognition and $machine learning$ .	0	used-for	used-for	1
the experimental results show that srda works well on recognition and classification , implying that semi-riemannian geometry is a promising new tool for #pattern recognition# and $machine learning$ .	1	conjunction	conjunction	1
a #deterministic parser# is under development which represents a departure from traditional $deterministic parsers$ in that it combines both symbolic and connectionist components .	6	compare	compare	1
a deterministic parser is under development which represents a departure from traditional deterministic parsers in that $it$ combines both #symbolic and connectionist components# .	4	part-of	part-of	1
the $connectionist component$ is trained either from #patterns# derived from the rules of a deterministic grammar .	0	used-for	used-for	1
the connectionist component is trained either from $patterns$ derived from the #rules of a deterministic grammar# .	0	used-for	used-for	1
the development and evolution of such a #hybrid architecture# has lead to a $parser$ which is superior to any known deterministic parser .	0	used-for	used-for	1
the development and evolution of such a hybrid architecture has lead to a #parser# which is superior to any known $deterministic parser$ .	6	compare	compare	1
experiments are described and powerful #training techniques# are demonstrated that permit $decision-making$ by the connectionist component in the parsing process .	0	used-for	used-for	1
experiments are described and powerful training techniques are demonstrated that permit $decision-making$ by the #connectionist component# in the parsing process .	0	used-for	used-for	1
experiments are described and powerful training techniques are demonstrated that permit decision-making by the #connectionist component# in the $parsing process$ .	4	part-of	part-of	1
data are presented which show how a #connectionist -lrb- neural -rrb- network# trained with linguistic rules can parse both $expected -lrb- grammatical -rrb- sentences$ as well as some novel -lrb- ungrammatical or lexically ambiguous -rrb- sentences .	0	used-for	used-for	1
data are presented which show how a #connectionist -lrb- neural -rrb- network# trained with linguistic rules can parse both expected -lrb- grammatical -rrb- sentences as well as some novel $-lrb- ungrammatical or lexically ambiguous -rrb- sentences$ .	0	used-for	used-for	1
data are presented which show how a $connectionist -lrb- neural -rrb- network$ trained with #linguistic rules# can parse both expected -lrb- grammatical -rrb- sentences as well as some novel -lrb- ungrammatical or lexically ambiguous -rrb- sentences .	0	used-for	used-for	1
data are presented which show how a connectionist -lrb- neural -rrb- network trained with linguistic rules can parse both #expected -lrb- grammatical -rrb- sentences# as well as some novel $-lrb- ungrammatical or lexically ambiguous -rrb- sentences$ .	1	conjunction	conjunction	1
robust $natural language interpretation$ requires strong #semantic domain models# , fail-soft recovery heuristics , and very flexible control structures .	0	used-for	used-for	1
robust natural language interpretation requires strong #semantic domain models# , $fail-soft recovery heuristics$ , and very flexible control structures .	1	conjunction	conjunction	1
robust $natural language interpretation$ requires strong semantic domain models , #fail-soft recovery heuristics# , and very flexible control structures .	0	used-for	used-for	1
robust natural language interpretation requires strong semantic domain models , #fail-soft recovery heuristics# , and very flexible $control structures$ .	1	conjunction	conjunction	1
robust $natural language interpretation$ requires strong semantic domain models , fail-soft recovery heuristics , and very flexible #control structures# .	0	used-for	used-for	1
although #single-strategy parsers# have met with a measure of success , a $multi-strategy approach$ is shown to provide a much higher degree of flexibility , redundancy , and ability to bring task-specific domain knowledge -lrb- in addition to general linguistic knowledge -rrb- to bear on both grammatical and ungrammatical input .	6	compare	compare	1
although single-strategy parsers have met with a measure of success , a multi-strategy approach is shown to provide a much higher degree of flexibility , redundancy , and ability to bring #task-specific domain knowledge# -lrb- in addition to $general linguistic knowledge$ -rrb- to bear on both grammatical and ungrammatical input .	1	conjunction	conjunction	1
a $parsing algorithm$ is presented that integrates several different #parsing strategies# , with case-frame instantiation dominating .	4	part-of	part-of	1
a parsing algorithm is presented that integrates several different $parsing strategies$ , with #case-frame instantiation# dominating .	3	hyponym-of	hyponym-of	1
each of these #parsing strategies# exploits different types of knowledge ; and their combination provides a strong framework in which to process $conjunctions$ , fragmentary input , and ungrammatical structures , as well as less exotic , grammatically correct input .	0	used-for	used-for	1
each of these #parsing strategies# exploits different types of knowledge ; and their combination provides a strong framework in which to process conjunctions , $fragmentary input$ , and ungrammatical structures , as well as less exotic , grammatically correct input .	0	used-for	used-for	1
each of these #parsing strategies# exploits different types of knowledge ; and their combination provides a strong framework in which to process conjunctions , fragmentary input , and $ungrammatical structures$ , as well as less exotic , grammatically correct input .	0	used-for	used-for	1
each of these #parsing strategies# exploits different types of knowledge ; and their combination provides a strong framework in which to process conjunctions , fragmentary input , and ungrammatical structures , as well as less $exotic , grammatically correct input$ .	0	used-for	used-for	1
each of these parsing strategies exploits different types of knowledge ; and their combination provides a strong framework in which to process #conjunctions# , $fragmentary input$ , and ungrammatical structures , as well as less exotic , grammatically correct input .	1	conjunction	conjunction	1
each of these parsing strategies exploits different types of knowledge ; and their combination provides a strong framework in which to process conjunctions , #fragmentary input# , and $ungrammatical structures$ , as well as less exotic , grammatically correct input .	1	conjunction	conjunction	1
each of these parsing strategies exploits different types of knowledge ; and their combination provides a strong framework in which to process conjunctions , fragmentary input , and #ungrammatical structures# , as well as less $exotic , grammatically correct input$ .	1	conjunction	conjunction	1
several #specific heuristics# for handling $ungrammatical input$ are presented within this multi-strategy framework .	0	used-for	used-for	1
several #specific heuristics# for handling ungrammatical input are presented within this $multi-strategy framework$ .	4	part-of	part-of	1
recently , #stacked auto-encoders -lrb- sae -rrb-# have been successfully used for $learning imbalanced datasets$ .	0	used-for	used-for	1
in this paper , for the first time , we propose to use a #neural network classifier# furnished by an sae structure for detecting the errors made by a strong $automatic speech recognition -lrb- asr -rrb- system$ .	0	used-for	used-for	1
in this paper , for the first time , we propose to use a $neural network classifier$ furnished by an #sae structure# for detecting the errors made by a strong automatic speech recognition -lrb- asr -rrb- system .	0	used-for	used-for	1
#error detection# on an $automatic transcription$ provided by a '' strong '' asr system , i.e. exhibiting a small word error rate , is difficult due to the limited number of '' positive '' examples -lrb- i.e. words erroneously recognized -rrb- available for training a binary classi-fier .	0	used-for	used-for	1
in this paper we investigate and compare different types of #classifiers# for $automatically detecting asr errors$ , including the one based on a stacked auto-encoder architecture .	0	used-for	used-for	1
in this paper we investigate and compare different types of $classifiers$ for automatically detecting asr errors , including the #one# based on a stacked auto-encoder architecture .	3	hyponym-of	hyponym-of	1
in this paper we investigate and compare different types of classifiers for automatically detecting asr errors , including the $one$ based on a #stacked auto-encoder architecture# .	0	used-for	used-for	1
we show the effectiveness of the latter by measuring and comparing performance on the $automatic transcriptions$ of an #english corpus# collected from ted talks .	5	feature-of	feature-of	1
we show the effectiveness of the latter by measuring and comparing performance on the automatic transcriptions of an $english corpus$ collected from #ted talks# .	0	used-for	used-for	1
performance of each investigated $classifier$ is evaluated both via #receiving operating curve# and via a measure , called mean absolute error , related to the quality in predicting the corresponding word error rate .	2	evaluate-for	evaluate-for	1
performance of each investigated classifier is evaluated both via #receiving operating curve# and via a $measure$ , called mean absolute error , related to the quality in predicting the corresponding word error rate .	1	conjunction	conjunction	1
performance of each investigated $classifier$ is evaluated both via receiving operating curve and via a #measure# , called mean absolute error , related to the quality in predicting the corresponding word error rate .	2	evaluate-for	evaluate-for	1
the results demonstrates that the #classifier# based on sae detects the $asr errors$ better than the other classification methods .	0	used-for	used-for	1
the results demonstrates that the #classifier# based on sae detects the asr errors better than the other $classification methods$ .	6	compare	compare	1
the results demonstrates that the $classifier$ based on #sae# detects the asr errors better than the other classification methods .	0	used-for	used-for	1
the results demonstrates that the classifier based on sae detects the $asr errors$ better than the other #classification methods# .	0	used-for	used-for	1
within the eu network of excellence pascal , a challenge was organized to design a #statistical machine learning algorithm# that segments words into the $smallest meaning-bearing units of language$ , morphemes .	0	used-for	used-for	1
within the eu network of excellence pascal , a challenge was organized to design a statistical machine learning algorithm that segments words into the $smallest meaning-bearing units of language$ , #morphemes# .	3	hyponym-of	hyponym-of	1
ideally , #these# are basic vocabulary units suitable for different $tasks$ , such as speech and text understanding , machine translation , information retrieval , and statistical language modeling .	0	used-for	used-for	1
ideally , these are basic vocabulary units suitable for different $tasks$ , such as #speech and text understanding# , machine translation , information retrieval , and statistical language modeling .	3	hyponym-of	hyponym-of	1
ideally , these are basic vocabulary units suitable for different tasks , such as #speech and text understanding# , $machine translation$ , information retrieval , and statistical language modeling .	1	conjunction	conjunction	1
ideally , these are basic vocabulary units suitable for different $tasks$ , such as speech and text understanding , #machine translation# , information retrieval , and statistical language modeling .	3	hyponym-of	hyponym-of	1
ideally , these are basic vocabulary units suitable for different tasks , such as speech and text understanding , #machine translation# , $information retrieval$ , and statistical language modeling .	1	conjunction	conjunction	1
ideally , these are basic vocabulary units suitable for different $tasks$ , such as speech and text understanding , machine translation , #information retrieval# , and statistical language modeling .	3	hyponym-of	hyponym-of	1
ideally , these are basic vocabulary units suitable for different tasks , such as speech and text understanding , machine translation , #information retrieval# , and $statistical language modeling$ .	1	conjunction	conjunction	1
ideally , these are basic vocabulary units suitable for different $tasks$ , such as speech and text understanding , machine translation , information retrieval , and #statistical language modeling# .	3	hyponym-of	hyponym-of	1
in this paper , we evaluate the application of these #segmen-tation algorithms# to $large vocabulary speech recognition$ using statistical n-gram language models based on the proposed word segments instead of entire words .	0	used-for	used-for	1
in this paper , we evaluate the application of these $segmen-tation algorithms$ to large vocabulary speech recognition using #statistical n-gram language models# based on the proposed word segments instead of entire words .	2	evaluate-for	evaluate-for	1
experiments were done for two $ag-glutinative and morphologically rich languages$ : #finnish# and turk-ish .	3	hyponym-of	hyponym-of	1
experiments were done for two ag-glutinative and morphologically rich languages : #finnish# and $turk-ish$ .	1	conjunction	conjunction	1
experiments were done for two $ag-glutinative and morphologically rich languages$ : finnish and #turk-ish# .	3	hyponym-of	hyponym-of	1
this paper describes a recently collected #spoken language corpus# for the $atis -lrb- air travel information system -rrb- domain$ .	0	used-for	used-for	1
we summarize the motivation for this effort , the goals , the implementation of a multi-site data collection paradigm , and the accomplishments of madcow in monitoring the collection and distribution of 12,000 utterances of #spontaneous speech# from five sites for use in a $multi-site common evaluation of speech , natural language and spoken language$ .	2	evaluate-for	evaluate-for	1
this paper proposes the #hierarchical directed acyclic graph -lrb- hdag -rrb- kernel# for $structured natural language data$ .	0	used-for	used-for	1
we applied the proposed #method# to $question classification and sentence alignment tasks$ to evaluate its performance as a similarity measure and a kernel function .	0	used-for	used-for	1
we applied the proposed $method$ to question classification and sentence alignment tasks to evaluate its performance as a #similarity measure# and a kernel function .	2	evaluate-for	evaluate-for	1
we applied the proposed method to question classification and sentence alignment tasks to evaluate its performance as a #similarity measure# and a $kernel function$ .	1	conjunction	conjunction	1
we applied the proposed $method$ to question classification and sentence alignment tasks to evaluate its performance as a similarity measure and a #kernel function# .	2	evaluate-for	evaluate-for	1
the results of the experiments demonstrate that the #hdag kernel# is superior to other $kernel functions$ and baseline methods .	6	compare	compare	1
the results of the experiments demonstrate that the #hdag kernel# is superior to other kernel functions and $baseline methods$ .	6	compare	compare	1
the results of the experiments demonstrate that the hdag kernel is superior to other #kernel functions# and $baseline methods$ .	1	conjunction	conjunction	1
we propose a solution to the challenge of the $conll 2008 shared task$ that uses a #generative history-based latent variable model# to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies .	0	used-for	used-for	1
we propose a solution to the challenge of the conll 2008 shared task that uses a #generative history-based latent variable model# to predict the most likely derivation of a $synchronous dependency parser$ for both syntactic and semantic dependencies .	0	used-for	used-for	1
we propose a solution to the challenge of the conll 2008 shared task that uses a generative history-based latent variable model to predict the most likely derivation of a #synchronous dependency parser# for both $syntactic and semantic dependencies$ .	0	used-for	used-for	1
the submitted $model$ yields 79.1 % #macro-average f1 performance# , for the joint task , 86.9 % syntactic dependencies las and 71.0 % semantic dependencies f1 .	2	evaluate-for	evaluate-for	1
the submitted model yields 79.1 % #macro-average f1 performance# , for the joint $task$ , 86.9 % syntactic dependencies las and 71.0 % semantic dependencies f1 .	2	evaluate-for	evaluate-for	1
the submitted model yields 79.1 % macro-average f1 performance , for the joint $task$ , 86.9 % #syntactic dependencies las# and 71.0 % semantic dependencies f1 .	2	evaluate-for	evaluate-for	1
the submitted model yields 79.1 % macro-average f1 performance , for the joint task , 86.9 % #syntactic dependencies las# and 71.0 % $semantic dependencies f1$ .	1	conjunction	conjunction	1
the submitted model yields 79.1 % macro-average f1 performance , for the joint $task$ , 86.9 % syntactic dependencies las and 71.0 % #semantic dependencies f1# .	2	evaluate-for	evaluate-for	1
a larger $model$ trained after the deadline achieves 80.5 % #macro-average f1# , 87.6 % syntactic dependencies las , and 73.1 % semantic dependencies f1 .	2	evaluate-for	evaluate-for	1
a larger model trained after the deadline achieves 80.5 % #macro-average f1# , 87.6 % $syntactic dependencies las$ , and 73.1 % semantic dependencies f1 .	1	conjunction	conjunction	1
a larger $model$ trained after the deadline achieves 80.5 % macro-average f1 , 87.6 % #syntactic dependencies las# , and 73.1 % semantic dependencies f1 .	2	evaluate-for	evaluate-for	1
a larger model trained after the deadline achieves 80.5 % macro-average f1 , 87.6 % #syntactic dependencies las# , and 73.1 % $semantic dependencies f1$ .	1	conjunction	conjunction	1
a larger $model$ trained after the deadline achieves 80.5 % macro-average f1 , 87.6 % syntactic dependencies las , and 73.1 % #semantic dependencies f1# .	2	evaluate-for	evaluate-for	1
we present an #approach# to annotating a level of $discourse structure$ that is based on identifying discourse connectives and their arguments .	0	used-for	used-for	1
we present an $approach$ to annotating a level of discourse structure that is based on identifying #discourse connectives# and their arguments .	0	used-for	used-for	1
the #pdtb# is being built directly on top of the penn treebank and propbank , thus supporting the $extraction of useful syntactic and semantic features$ and providing a richer substrate for the development and evaluation of practical algorithms .	0	used-for	used-for	1
the #pdtb# is being built directly on top of the penn treebank and propbank , thus supporting the extraction of useful syntactic and semantic features and providing a richer substrate for the development and evaluation of $practical algorithms$ .	2	evaluate-for	evaluate-for	1
the $pdtb$ is being built directly on top of the #penn treebank# and propbank , thus supporting the extraction of useful syntactic and semantic features and providing a richer substrate for the development and evaluation of practical algorithms .	0	used-for	used-for	1
the pdtb is being built directly on top of the #penn treebank# and $propbank$ , thus supporting the extraction of useful syntactic and semantic features and providing a richer substrate for the development and evaluation of practical algorithms .	1	conjunction	conjunction	1
the $pdtb$ is being built directly on top of the penn treebank and #propbank# , thus supporting the extraction of useful syntactic and semantic features and providing a richer substrate for the development and evaluation of practical algorithms .	0	used-for	used-for	1
we provide a detailed preliminary analysis of $inter-annotator agreement$ - both the #level of agreement# and the types of inter-annotator variation .	5	feature-of	feature-of	1
we provide a detailed preliminary analysis of inter-annotator agreement - both the #level of agreement# and the types of $inter-annotator variation$ .	1	conjunction	conjunction	1
we provide a detailed preliminary analysis of $inter-annotator agreement$ - both the level of agreement and the types of #inter-annotator variation# .	5	feature-of	feature-of	1
currently , #n-gram models# are the most common and widely used models for $statistical language modeling$ .	0	used-for	used-for	1
in this paper , we investigated an alternative way to build language models , i.e. , using #artificial neural networks# to learn the $language model$ .	0	used-for	used-for	1
our experiment result shows that the #neural network# can learn a $language model$ that has performance even better than standard statistical methods .	0	used-for	used-for	1
our experiment result shows that the #neural network# can learn a language model that has performance even better than standard $statistical methods$ .	6	compare	compare	1
existing works in the field usually do not encode either the $temporal evolution$ or the #intensity of the observed facial displays# .	1	conjunction	conjunction	1
in this paper , $intrinsic topology of multidimensional continuous facial$ affect data is first modeled by an #ordinal man-ifold# .	0	used-for	used-for	1
this #topology# is then incorporated into the $hidden conditional ordinal random field -lrb- h-corf -rrb- framework$ for dynamic ordinal regression by constraining h-corf parameters to lie on the ordinal manifold .	4	part-of	part-of	1
this topology is then incorporated into the #hidden conditional ordinal random field -lrb- h-corf -rrb- framework# for $dynamic ordinal regression$ by constraining h-corf parameters to lie on the ordinal manifold .	0	used-for	used-for	1
the resulting #model# attains $simultaneous dynamic recognition$ and intensity estimation of facial expressions of multiple emotions .	0	used-for	used-for	1
the resulting #model# attains simultaneous dynamic recognition and $intensity estimation of facial expressions$ of multiple emotions .	0	used-for	used-for	1
the resulting model attains #simultaneous dynamic recognition# and $intensity estimation of facial expressions$ of multiple emotions .	1	conjunction	conjunction	1
to the best of our knowledge , $the proposed method$ is the first one to achieve this on both deliberate as well as #spontaneous facial affect data# .	2	evaluate-for	evaluate-for	1
recent advances in linear classification have shown that for $applications$ such as #document classification# , the training can be extremely efficient .	3	hyponym-of	hyponym-of	1
these methods can not be easily applied to #data# larger than the $memory capacity$ due to the random access to the disk .	6	compare	compare	1
we propose and analyze a #block minimization framework# for $data$ larger than the memory size .	0	used-for	used-for	1
we propose and analyze a block minimization framework for #data# larger than the $memory size$ .	6	compare	compare	1
we investigate two implementations of the proposed #framework# for $primal and dual svms$ , respectively .	0	used-for	used-for	1
this in turn affects the #accuracy# of $word sense disambiguation -lrb- wsd -rrb- systems$ trained and applied on different domains .	2	evaluate-for	evaluate-for	1
this paper presents a #method# to estimate the $sense priors of words$ drawn from a new domain , and highlights the importance of using well calibrated probabilities when performing these estimations .	0	used-for	used-for	1
this paper presents a method to estimate the $sense priors of words$ drawn from a #new domain# , and highlights the importance of using well calibrated probabilities when performing these estimations .	5	feature-of	feature-of	1
this paper presents a method to estimate the sense priors of words drawn from a new domain , and highlights the importance of using #well calibrated probabilities# when performing these $estimations$ .	0	used-for	used-for	1
by using #well calibrated probabilities# , we are able to estimate the $sense priors$ effectively to achieve significant improvements in wsd accuracy .	0	used-for	used-for	1
$it$ was compiled from various resources such as #encyclopedias# and dictionaries , public databases of proper names and toponyms , collocations obtained from czech wordnet , lists of botanical and zoological terms and others .	0	used-for	used-for	1
it was compiled from various resources such as #encyclopedias# and $dictionaries$ , public databases of proper names and toponyms , collocations obtained from czech wordnet , lists of botanical and zoological terms and others .	1	conjunction	conjunction	1
$it$ was compiled from various resources such as encyclopedias and #dictionaries# , public databases of proper names and toponyms , collocations obtained from czech wordnet , lists of botanical and zoological terms and others .	0	used-for	used-for	1
$it$ was compiled from various resources such as encyclopedias and dictionaries , #public databases of proper names and toponyms# , collocations obtained from czech wordnet , lists of botanical and zoological terms and others .	0	used-for	used-for	1
$it$ was compiled from various resources such as encyclopedias and dictionaries , public databases of proper names and toponyms , #collocations# obtained from czech wordnet , lists of botanical and zoological terms and others .	0	used-for	used-for	1
$it$ was compiled from various resources such as encyclopedias and dictionaries , public databases of proper names and toponyms , collocations obtained from czech wordnet , #lists of botanical and zoological terms# and others .	0	used-for	used-for	1
it was compiled from various resources such as encyclopedias and dictionaries , public databases of proper names and toponyms , $collocations$ obtained from czech wordnet , #lists of botanical and zoological terms# and others .	1	conjunction	conjunction	1
we compare the built $mwes database$ with the corpus data from #czech national corpus# -lrb- approx .	0	used-for	used-for	1
to obtain a more complete list of mwes we propose and use a #technique# exploiting the $word sketch engine$ , which allows us to work with statistical parameters such as frequency of mwes and their components as well as with the salience for the whole mwes .	0	used-for	used-for	1
to obtain a more complete list of mwes we propose and use a technique exploiting the $word sketch engine$ , which allows us to work with #statistical parameters# such as frequency of mwes and their components as well as with the salience for the whole mwes .	5	feature-of	feature-of	1
we also discuss exploitation of the #database# for working out a more adequate $tagging$ and lemmatization .	0	used-for	used-for	1
we also discuss exploitation of the #database# for working out a more adequate tagging and $lemmatization$ .	0	used-for	used-for	1
we also discuss exploitation of the database for working out a more adequate #tagging# and $lemmatization$ .	1	conjunction	conjunction	1
the final goal is to be able to recognize #mwes# in corpus text and lemmatize them as complete lexical units , i. e. to make $tagging$ and lemmatization more adequate .	0	used-for	used-for	1
the final goal is to be able to recognize #mwes# in corpus text and lemmatize them as complete lexical units , i. e. to make tagging and $lemmatization$ more adequate .	0	used-for	used-for	1
the final goal is to be able to recognize mwes in corpus text and lemmatize them as complete lexical units , i. e. to make #tagging# and $lemmatization$ more adequate .	1	conjunction	conjunction	1
we describe the ongoing construction of a large , #semantically annotated corpus# resource as reliable basis for the $large-scale acquisition of word-semantic information$ , e.g. the construction of domain-independent lexica .	0	used-for	used-for	1
we describe the ongoing construction of a large , semantically annotated corpus resource as reliable basis for the $large-scale acquisition of word-semantic information$ , e.g. the #construction of domain-independent lexica# .	3	hyponym-of	hyponym-of	1
the backbone of the annotation are #semantic roles# in the $frame semantics paradigm$ .	4	part-of	part-of	1
on this basis , we discuss the problems of #vagueness# and $ambiguity$ in semantic annotation .	1	conjunction	conjunction	1
on this basis , we discuss the problems of #vagueness# and ambiguity in $semantic annotation$ .	5	feature-of	feature-of	1
on this basis , we discuss the problems of vagueness and #ambiguity# in $semantic annotation$ .	5	feature-of	feature-of	1
#statistical machine translation -lrb- smt -rrb-# is currently one of the hot spots in $natural language processing$ .	3	hyponym-of	hyponym-of	1
over the last few years dramatic improvements have been made , and a number of comparative evaluations have shown , that #smt# gives competitive results to $rule-based translation systems$ , requiring significantly less development time .	6	compare	compare	1
this is particularly important when building #translation systems# for $new language pairs$ or new domains .	0	used-for	used-for	1
this is particularly important when building #translation systems# for new language pairs or $new domains$ .	0	used-for	used-for	1
this is particularly important when building translation systems for #new language pairs# or $new domains$ .	1	conjunction	conjunction	1
#sttk# , a $statistical machine translation tool kit$ , will be introduced and used to build a working translation system .	3	hyponym-of	hyponym-of	1
#sttk# , a statistical machine translation tool kit , will be introduced and used to build a working $translation system$ .	0	used-for	used-for	1
#sttk# has been developed by the presenter and co-workers over a number of years and is currently used as the basis of cmu 's $smt system$ .	0	used-for	used-for	1
#it# has also successfully been coupled with $rule-based and example based machine translation modules$ to build a multi engine machine translation system .	1	conjunction	conjunction	1
#it# has also successfully been coupled with rule-based and example based machine translation modules to build a $multi engine machine translation system$ .	0	used-for	used-for	1
it has also successfully been coupled with #rule-based and example based machine translation modules# to build a $multi engine machine translation system$ .	0	used-for	used-for	1
this paper presents an #unsupervised learning approach# to building a $non-english -lrb- arabic -rrb- stemmer$ .	0	used-for	used-for	1
the $stemming model$ is based on #statistical machine translation# and it uses an english stemmer and a small -lrb- 10k sentences -rrb- parallel corpus as its sole training resources .	0	used-for	used-for	1
the stemming model is based on statistical machine translation and $it$ uses an #english stemmer# and a small -lrb- 10k sentences -rrb- parallel corpus as its sole training resources .	0	used-for	used-for	1
the stemming model is based on statistical machine translation and $it$ uses an english stemmer and a small -lrb- 10k sentences -rrb- #parallel corpus# as its sole training resources .	0	used-for	used-for	1
#monolingual , unannotated text# can be used to further improve the $stemmer$ by allowing it to adapt to a desired domain or genre .	0	used-for	used-for	1
our #resource-frugal approach# results in 87.5 % agreement with a state of the art , proprietary $arabic stemmer$ built using rules , affix lists , and human annotated text , in addition to an unsupervised component .	6	compare	compare	1
our $resource-frugal approach$ results in 87.5 % #agreement# with a state of the art , proprietary arabic stemmer built using rules , affix lists , and human annotated text , in addition to an unsupervised component .	2	evaluate-for	evaluate-for	1
our resource-frugal approach results in 87.5 % #agreement# with a state of the art , proprietary $arabic stemmer$ built using rules , affix lists , and human annotated text , in addition to an unsupervised component .	2	evaluate-for	evaluate-for	1
our resource-frugal approach results in 87.5 % agreement with a state of the art , proprietary $arabic stemmer$ built using #rules# , affix lists , and human annotated text , in addition to an unsupervised component .	0	used-for	used-for	1
our resource-frugal approach results in 87.5 % agreement with a state of the art , proprietary arabic stemmer built using #rules# , $affix lists$ , and human annotated text , in addition to an unsupervised component .	1	conjunction	conjunction	1
our resource-frugal approach results in 87.5 % agreement with a state of the art , proprietary $arabic stemmer$ built using rules , #affix lists# , and human annotated text , in addition to an unsupervised component .	0	used-for	used-for	1
our resource-frugal approach results in 87.5 % agreement with a state of the art , proprietary arabic stemmer built using rules , #affix lists# , and $human annotated text$ , in addition to an unsupervised component .	1	conjunction	conjunction	1
our resource-frugal approach results in 87.5 % agreement with a state of the art , proprietary $arabic stemmer$ built using rules , affix lists , and #human annotated text# , in addition to an unsupervised component .	0	used-for	used-for	1
our resource-frugal approach results in 87.5 % agreement with a state of the art , proprietary arabic stemmer built using rules , affix lists , and #human annotated text# , in addition to an $unsupervised component$ .	1	conjunction	conjunction	1
our resource-frugal approach results in 87.5 % agreement with a state of the art , proprietary $arabic stemmer$ built using rules , affix lists , and human annotated text , in addition to an #unsupervised component# .	0	used-for	used-for	1
$task-based evaluation$ using #arabic information retrieval# indicates an improvement of 22-38 % in average precision over unstemmed text , and 96 % of the performance of the proprietary stemmer above .	0	used-for	used-for	1
$task-based evaluation$ using arabic information retrieval indicates an improvement of 22-38 % in #average precision# over unstemmed text , and 96 % of the performance of the proprietary stemmer above .	2	evaluate-for	evaluate-for	1
task-based evaluation using arabic information retrieval indicates an improvement of 22-38 % in #average precision# over $unstemmed text$ , and 96 % of the performance of the proprietary stemmer above .	2	evaluate-for	evaluate-for	1
the paper assesses the capability of an #hmm-based tts system# to produce $german speech$ .	0	used-for	used-for	1
in addition , the #system# is adapted to a small set of $football announcements$ , in an exploratory attempt to synthe-sise expressive speech .	0	used-for	used-for	1
in addition , the #system# is adapted to a small set of football announcements , in an exploratory attempt to synthe-sise $expressive speech$ .	0	used-for	used-for	1
we conclude that the #hmms# are able to produce highly $intelligible neutral german speech$ , with a stable quality , and that the expressivity is partially captured in spite of the small size of the football dataset .	0	used-for	used-for	1
furthermore , in contrast to the approach of dalrymple et al. -lsb- 1991 -rsb- , the treatment directly encodes the intuitive distinction between #full nps# and the $referential elements$ that corefer with them through what we term role linking .	1	conjunction	conjunction	1
finally , the #analysis# extends directly to other $discourse copying phenomena$ .	0	used-for	used-for	1
how to obtain #hierarchical relations# -lrb- e.g. superordinate - hyponym relation , synonym relation -rrb- is one of the most important problems for $thesaurus construction$ .	4	part-of	part-of	1
how to obtain $hierarchical relations$ -lrb- e.g. #superordinate - hyponym relation# , synonym relation -rrb- is one of the most important problems for thesaurus construction .	3	hyponym-of	hyponym-of	1
how to obtain hierarchical relations -lrb- e.g. #superordinate - hyponym relation# , $synonym relation$ -rrb- is one of the most important problems for thesaurus construction .	1	conjunction	conjunction	1
how to obtain $hierarchical relations$ -lrb- e.g. superordinate - hyponym relation , #synonym relation# -rrb- is one of the most important problems for thesaurus construction .	3	hyponym-of	hyponym-of	1
a pilot system for extracting these $relations$ automatically from an ordinary #japanese language dictionary# -lrb- shinmeikai kokugojiten , published by sansei-do , in machine readable form -rrb- is given .	0	used-for	used-for	1
the $features$ of the #definition sentences# in the dictionary , the mechanical extraction of the hierarchical relations and the estimation of the results are discussed .	0	used-for	used-for	1
the features of the #definition sentences# in the $dictionary$ , the mechanical extraction of the hierarchical relations and the estimation of the results are discussed .	4	part-of	part-of	1
this is evident most compellingly by the very low #recognition rate# of all existing $face recognition systems$ when applied to live cctv camera input .	2	evaluate-for	evaluate-for	1
this is evident most compellingly by the very low recognition rate of all existing $face recognition systems$ when applied to #live cctv camera input# .	0	used-for	used-for	1
in this paper , we present a #bayesian framework# to perform multi-modal -lrb- such as variations in viewpoint and illumination -rrb- $face image super-resolution$ for recognition in tensor space .	0	used-for	used-for	1
in this paper , we present a bayesian framework to perform multi-modal -lrb- such as variations in #viewpoint# and $illumination$ -rrb- face image super-resolution for recognition in tensor space .	1	conjunction	conjunction	1
in this paper , we present a bayesian framework to perform multi-modal -lrb- such as variations in viewpoint and illumination -rrb- #face image super-resolution# for $recognition$ in tensor space .	0	used-for	used-for	1
in this paper , we present a bayesian framework to perform multi-modal -lrb- such as variations in viewpoint and illumination -rrb- face image super-resolution for $recognition$ in #tensor space# .	5	feature-of	feature-of	1
given a #single modal low-resolution face image# , we benefit from the multiple factor interactions of training tensor , and super-resolve its $high-resolution reconstructions$ across different modalities for face recognition .	0	used-for	used-for	1
given a single modal low-resolution face image , we benefit from the #multiple factor interactions of training tensor# , and super-resolve its $high-resolution reconstructions$ across different modalities for face recognition .	0	used-for	used-for	1
given a single modal low-resolution face image , we benefit from the multiple factor interactions of training tensor , and super-resolve its #high-resolution reconstructions# across different modalities for $face recognition$ .	0	used-for	used-for	1
given a single modal low-resolution face image , we benefit from the multiple factor interactions of training tensor , and super-resolve its $high-resolution reconstructions$ across different #modalities# for face recognition .	5	feature-of	feature-of	1
instead of performing $pixel-domain super-resolution and recognition$ independently as two separate sequential processes , we integrate the tasks of #super-resolution# and recognition by directly computing a maximum likelihood identity parameter vector in high-resolution tensor space for recognition .	3	hyponym-of	hyponym-of	1
instead of performing pixel-domain super-resolution and recognition independently as two separate sequential processes , we integrate the tasks of #super-resolution# and $recognition$ by directly computing a maximum likelihood identity parameter vector in high-resolution tensor space for recognition .	1	conjunction	conjunction	1
instead of performing $pixel-domain super-resolution and recognition$ independently as two separate sequential processes , we integrate the tasks of super-resolution and #recognition# by directly computing a maximum likelihood identity parameter vector in high-resolution tensor space for recognition .	3	hyponym-of	hyponym-of	1
instead of performing pixel-domain super-resolution and recognition independently as two separate sequential processes , we integrate the tasks of $super-resolution$ and recognition by directly computing a #maximum likelihood identity parameter vector# in high-resolution tensor space for recognition .	0	used-for	used-for	1
instead of performing pixel-domain super-resolution and recognition independently as two separate sequential processes , we integrate the tasks of super-resolution and $recognition$ by directly computing a #maximum likelihood identity parameter vector# in high-resolution tensor space for recognition .	0	used-for	used-for	1
instead of performing pixel-domain super-resolution and recognition independently as two separate sequential processes , we integrate the tasks of super-resolution and recognition by directly computing a #maximum likelihood identity parameter vector# in high-resolution tensor space for $recognition$ .	0	used-for	used-for	1
instead of performing pixel-domain super-resolution and recognition independently as two separate sequential processes , we integrate the tasks of super-resolution and recognition by directly computing a $maximum likelihood identity parameter vector$ in #high-resolution tensor space# for recognition .	5	feature-of	feature-of	1
we show results from $multi-modal super-resolution and face recognition$ experiments across different imaging modalities , using #low-resolution images# as testing inputs and demonstrate improved recognition rates over standard tensorface and eigenface representations .	0	used-for	used-for	1
we show results from $multi-modal super-resolution and face recognition$ experiments across different imaging modalities , using low-resolution images as testing inputs and demonstrate improved #recognition rates# over standard tensorface and eigenface representations .	2	evaluate-for	evaluate-for	1
we show results from multi-modal super-resolution and face recognition experiments across different imaging modalities , using low-resolution images as testing inputs and demonstrate improved #recognition rates# over standard $tensorface and eigenface representations$ .	2	evaluate-for	evaluate-for	1
in this paper , we describe a #phrase-based unigram model# for $statistical machine translation$ that uses a much simpler set of model parameters than similar phrase-based models .	0	used-for	used-for	1
in this paper , we describe a #phrase-based unigram model# for statistical machine translation that uses a much simpler set of model parameters than similar $phrase-based models$ .	6	compare	compare	1
in this paper , we describe a $phrase-based unigram model$ for statistical machine translation that uses a much simpler set of #model parameters# than similar phrase-based models .	0	used-for	used-for	1
during $decoding$ , we use a #block unigram model# and a word-based trigram language model .	0	used-for	used-for	1
during $decoding$ , we use a block unigram model and a #word-based trigram language model# .	0	used-for	used-for	1
during decoding , we use a $block unigram model$ and a #word-based trigram language model# .	1	conjunction	conjunction	1
during training , the $blocks$ are learned from #source interval projections# using an underlying word alignment .	0	used-for	used-for	1
during training , the blocks are learned from $source interval projections$ using an underlying #word alignment# .	0	used-for	used-for	1
we show experimental results on $block selection criteria$ based on #unigram counts# and phrase length .	0	used-for	used-for	1
we show experimental results on block selection criteria based on #unigram counts# and $phrase length$ .	1	conjunction	conjunction	1
we show experimental results on $block selection criteria$ based on unigram counts and #phrase length# .	0	used-for	used-for	1
this paper develops a new #approach# for extremely $fast detection$ in domains where the distribution of positive and negative examples is highly skewed -lrb- e.g. face detection or database retrieval -rrb- .	0	used-for	used-for	1
this paper develops a new approach for extremely fast detection in domains where the distribution of positive and negative examples is highly skewed -lrb- e.g. #face detection# or $database retrieval$ -rrb- .	1	conjunction	conjunction	1
in such domains a #cascade of simple classifiers# each trained to achieve high detection rates and modest false positive rates can yield a final $detector$ with many desirable features : including high detection rates , very low false positive rates , and fast performance .	0	used-for	used-for	1
in such domains a cascade of simple $classifiers$ each trained to achieve high #detection rates# and modest false positive rates can yield a final detector with many desirable features : including high detection rates , very low false positive rates , and fast performance .	2	evaluate-for	evaluate-for	1
in such domains a cascade of simple classifiers each trained to achieve high #detection rates# and $modest false positive rates$ can yield a final detector with many desirable features : including high detection rates , very low false positive rates , and fast performance .	1	conjunction	conjunction	1
in such domains a cascade of simple $classifiers$ each trained to achieve high detection rates and #modest false positive rates# can yield a final detector with many desirable features : including high detection rates , very low false positive rates , and fast performance .	2	evaluate-for	evaluate-for	1
in such domains a cascade of simple classifiers each trained to achieve high detection rates and modest false positive rates can yield a final $detector$ with many desirable #features# : including high detection rates , very low false positive rates , and fast performance .	5	feature-of	feature-of	1
achieving extremely high #detection rates# , rather than $low error$ , is not a task typically addressed by machine learning algorithms .	6	compare	compare	1
we propose a new variant of #adaboost# as a mechanism for training the simple $classifiers$ used in the cascade .	0	used-for	used-for	1
we propose a new variant of adaboost as a mechanism for training the simple #classifiers# used in the $cascade$ .	0	used-for	used-for	1
experimental results in the domain of $face detection$ show the #training algorithm# yields significant improvements in performance over conventional adaboost .	0	used-for	used-for	1
experimental results in the domain of $face detection$ show the training algorithm yields significant improvements in performance over conventional #adaboost# .	0	used-for	used-for	1
experimental results in the domain of face detection show the $training algorithm$ yields significant improvements in performance over conventional #adaboost# .	6	compare	compare	1
the final face detection system can process 15 frames per second , achieves over 90 % #detection# , and a $false positive rate$ of 1 in a 1,000,000 .	1	conjunction	conjunction	1
this paper proposes a #method# for learning $joint embed-dings of images and text$ using a two-branch neural network with multiple layers of linear projections followed by nonlinearities .	0	used-for	used-for	1
this paper proposes a $method$ for learning joint embed-dings of images and text using a #two-branch neural network# with multiple layers of linear projections followed by nonlinearities .	0	used-for	used-for	1
this paper proposes a method for learning joint embed-dings of images and text using a $two-branch neural network$ with #multiple layers of linear projections# followed by nonlinearities .	4	part-of	part-of	1
this paper proposes a method for learning joint embed-dings of images and text using a two-branch neural network with #multiple layers of linear projections# followed by $nonlinearities$ .	1	conjunction	conjunction	1
this paper proposes a method for learning joint embed-dings of images and text using a $two-branch neural network$ with multiple layers of linear projections followed by #nonlinearities# .	4	part-of	part-of	1
the $network$ is trained using a #large-margin objective# that combines cross-view ranking constraints with within-view neighborhood structure preservation constraints inspired by metric learning literature .	0	used-for	used-for	1
the network is trained using a $large-margin objective$ that combines #cross-view ranking constraints# with within-view neighborhood structure preservation constraints inspired by metric learning literature .	5	feature-of	feature-of	1
the network is trained using a large-margin objective that combines #cross-view ranking constraints# with $within-view neighborhood structure preservation constraints$ inspired by metric learning literature .	1	conjunction	conjunction	1
the network is trained using a $large-margin objective$ that combines cross-view ranking constraints with #within-view neighborhood structure preservation constraints# inspired by metric learning literature .	5	feature-of	feature-of	1
extensive experiments show that our $approach$ gains significant improvements in #accuracy# for image-to-text and text-to-image retrieval .	2	evaluate-for	evaluate-for	1
extensive experiments show that our $approach$ gains significant improvements in accuracy for #image-to-text and text-to-image retrieval# .	2	evaluate-for	evaluate-for	1
our $method$ achieves new state-of-the-art results on the #flickr30k and mscoco image-sentence datasets# and shows promise on the new task of phrase lo-calization on the flickr30k entities dataset .	2	evaluate-for	evaluate-for	1
our $method$ achieves new state-of-the-art results on the flickr30k and mscoco image-sentence datasets and shows promise on the new task of #phrase lo-calization# on the flickr30k entities dataset .	2	evaluate-for	evaluate-for	1
our method achieves new state-of-the-art results on the flickr30k and mscoco image-sentence datasets and shows promise on the new task of $phrase lo-calization$ on the #flickr30k entities dataset# .	0	used-for	used-for	1
we investigate that claim by adopting a simple #mt-based paraphrasing technique# and evaluating $qa system$ performance on paraphrased questions .	0	used-for	used-for	1
we investigate that claim by adopting a simple mt-based paraphrasing technique and evaluating $qa system$ performance on #paraphrased questions# .	2	evaluate-for	evaluate-for	1
the $tap-xl automated analyst 's assistant$ is an application designed to help an english - speaking analyst write a topical report , culling information from a large inflow of #multilingual , multimedia data# .	0	used-for	used-for	1
$it$ gives users the ability to spend their time finding more data relevant to their task , and gives them translingual reach into other languages by leveraging #human language technology# .	0	used-for	used-for	1
this paper discusses the application of #unification categorial grammar -lrb- ucg -rrb-# to the framework of $isomorphic grammars$ for machine translation pioneered by landsbergen .	0	used-for	used-for	1
this paper discusses the application of unification categorial grammar -lrb- ucg -rrb- to the framework of #isomorphic grammars# for $machine translation$ pioneered by landsbergen .	0	used-for	used-for	1
the #isomorphic grammars approach# to $mt$ involves developing the grammars of the source and target languages in parallel , in order to ensure that sl and tl expressions which stand in the translation relation have isomorphic derivations .	0	used-for	used-for	1
after introducing this #approach# to $mt system design$ , and the basics of monolingual ucg , we will show how the two can be integrated , and present an example from an implemented bi-directional english-spanish fragment .	0	used-for	used-for	1
after introducing this #approach# to mt system design , and the basics of $monolingual ucg$ , we will show how the two can be integrated , and present an example from an implemented bi-directional english-spanish fragment .	0	used-for	used-for	1
after introducing this approach to #mt system design# , and the basics of $monolingual ucg$ , we will show how the two can be integrated , and present an example from an implemented bi-directional english-spanish fragment .	1	conjunction	conjunction	1
after introducing this approach to #mt system design# , and the basics of monolingual ucg , we will show how the $two$ can be integrated , and present an example from an implemented bi-directional english-spanish fragment .	3	hyponym-of	hyponym-of	1
after introducing this approach to mt system design , and the basics of #monolingual ucg# , we will show how the $two$ can be integrated , and present an example from an implemented bi-directional english-spanish fragment .	3	hyponym-of	hyponym-of	1
in the $security domain$ a key problem is #identifying rare behaviours of interest# .	4	part-of	part-of	1
#training examples# for these $behaviours$ may or may not exist , and if they do exist there will be few examples , quite probably one .	0	used-for	used-for	1
we present a novel #weakly supervised algorithm# that can detect $behaviours$ that either have never before been seen or for which there are few examples .	0	used-for	used-for	1
#global context# is modelled , allowing the $detection of abnormal behaviours$ that in isolation appear normal .	0	used-for	used-for	1
we have developed a #computational model# of the process of describing the layout of an apartment or house , a much-studied $discourse task$ first characterized linguistically by linde -lrb- 1974 -rrb- .	0	used-for	used-for	1
the #model# is embodied in a $program$ , apt , that can reproduce segments of actual tape-recorded descriptions , using organizational and discourse strategies derived through analysis of our corpus .	4	part-of	part-of	1
the model is embodied in a program , $apt$ , that can reproduce segments of actual tape-recorded descriptions , using #organizational and discourse strategies# derived through analysis of our corpus .	0	used-for	used-for	1
this paper proposes a practical #approach# employing n-gram models and error-correction rules for $thai key prediction$ and thai-english language identification .	0	used-for	used-for	1
this paper proposes a practical #approach# employing n-gram models and error-correction rules for thai key prediction and $thai-english language identification$ .	0	used-for	used-for	1
this paper proposes a practical $approach$ employing #n-gram models# and error-correction rules for thai key prediction and thai-english language identification .	0	used-for	used-for	1
this paper proposes a practical approach employing #n-gram models# and $error-correction rules$ for thai key prediction and thai-english language identification .	1	conjunction	conjunction	1
this paper proposes a practical $approach$ employing n-gram models and #error-correction rules# for thai key prediction and thai-english language identification .	0	used-for	used-for	1
this paper proposes a practical approach employing n-gram models and error-correction rules for #thai key prediction# and $thai-english language identification$ .	1	conjunction	conjunction	1
the paper also proposes $rule-reduction algorithm$ applying #mutual information# to reduce the error-correction rules .	0	used-for	used-for	1
the paper also proposes rule-reduction algorithm applying #mutual information# to reduce the $error-correction rules$ .	0	used-for	used-for	1
our #algorithm# reported more than 99 % accuracy in both $language identification$ and key prediction .	0	used-for	used-for	1
our #algorithm# reported more than 99 % accuracy in both language identification and $key prediction$ .	0	used-for	used-for	1
our $algorithm$ reported more than 99 % #accuracy# in both language identification and key prediction .	2	evaluate-for	evaluate-for	1
this paper concerns the #discourse understanding process# in $spoken dialogue systems$ .	0	used-for	used-for	1
this process enables the #system# to understand $user utterances$ based on the context of a dialogue .	0	used-for	used-for	1
this paper proposes a #method# for resolving this $ambiguity$ based on statistical information obtained from dialogue corpora .	0	used-for	used-for	1
this paper proposes a $method$ for resolving this ambiguity based on #statistical information# obtained from dialogue corpora .	0	used-for	used-for	1
this paper proposes a method for resolving this ambiguity based on $statistical information$ obtained from #dialogue corpora# .	0	used-for	used-for	1
unlike conventional $methods$ that use #hand-crafted rules# , the proposed method enables easy design of the discourse understanding process .	0	used-for	used-for	1
experiment results have shown that a #system# that exploits the proposed $method$ performs sufficiently and that holding multiple candidates for understanding results is effective .	0	used-for	used-for	1
we consider the problem of $question-focused sentence retrieval$ from complex #news articles# describing multi-event stories published over time .	0	used-for	used-for	1
we consider the problem of question-focused sentence retrieval from complex $news articles$ describing #multi-event stories# published over time .	5	feature-of	feature-of	1
to address the $sentence retrieval problem$ , we apply a #stochastic , graph-based method# for comparing the relative importance of the textual units , which was previously used successfully for generic summarization .	0	used-for	used-for	1
to address the sentence retrieval problem , we apply a #stochastic , graph-based method# for comparing the relative importance of the textual units , which was previously used successfully for $generic summarization$ .	0	used-for	used-for	1
currently , we present a topic-sensitive version of our method and hypothesize that $it$ can outperform a competitive #baseline# , which compares the similarity of each sentence to the input question via idf-weighted word overlap .	6	compare	compare	1
in our experiments , the #method# achieves a trdr score that is significantly higher than that of the $baseline$ .	6	compare	compare	1
in our experiments , the $method$ achieves a #trdr score# that is significantly higher than that of the baseline .	2	evaluate-for	evaluate-for	1
in our experiments , the method achieves a #trdr score# that is significantly higher than that of the $baseline$ .	2	evaluate-for	evaluate-for	1
this paper proposes that $sentence analysis$ should be treated as #defeasible reasoning# , and presents such a treatment for japanese sentence analyses using an argumentation system by konolige , which is a formalization of defeasible reasoning , that includes arguments and defeat rules that capture defeasibility .	0	used-for	used-for	1
this paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a #treatment# for $japanese sentence analyses$ using an argumentation system by konolige , which is a formalization of defeasible reasoning , that includes arguments and defeat rules that capture defeasibility .	0	used-for	used-for	1
this paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for $japanese sentence analyses$ using an #argumentation system# by konolige , which is a formalization of defeasible reasoning , that includes arguments and defeat rules that capture defeasibility .	0	used-for	used-for	1
this paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for japanese sentence analyses using an argumentation system by konolige , which is a $formalization of defeasible reasoning$ , that includes #arguments# and defeat rules that capture defeasibility .	4	part-of	part-of	1
this paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for japanese sentence analyses using an argumentation system by konolige , which is a formalization of defeasible reasoning , that includes #arguments# and $defeat rules$ that capture defeasibility .	1	conjunction	conjunction	1
this paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for japanese sentence analyses using an argumentation system by konolige , which is a $formalization of defeasible reasoning$ , that includes arguments and #defeat rules# that capture defeasibility .	4	part-of	part-of	1
this paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for japanese sentence analyses using an argumentation system by konolige , which is a formalization of defeasible reasoning , that includes $arguments$ and defeat rules that capture #defeasibility# .	5	feature-of	feature-of	1
this paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for japanese sentence analyses using an argumentation system by konolige , which is a formalization of defeasible reasoning , that includes arguments and $defeat rules$ that capture #defeasibility# .	5	feature-of	feature-of	1
it gives an overview of #methods# used for $visual speech animation$ , parameterization of a human face and a tongue , necessary data sources and a synthesis method .	0	used-for	used-for	1
a #3d animation model# is used for a $pseudo-muscular animation schema$ to create such animation of visual speech which is usable for a lipreading .	0	used-for	used-for	1
a 3d animation model is used for a #pseudo-muscular animation schema# to create such $animation of visual speech$ which is usable for a lipreading .	0	used-for	used-for	1
a 3d animation model is used for a pseudo-muscular animation schema to create such #animation of visual speech# which is usable for a $lipreading$ .	0	used-for	used-for	1
furthermore , a problem of #forming articulatory trajectories# is formulated to solve $labial coarticulation effects$ .	0	used-for	used-for	1
#it# is used for the $synthesis method$ based on a selection of articulatory targets and interpolation technique .	0	used-for	used-for	1
$it$ is used for the synthesis method based on a #selection of articulatory targets# and interpolation technique .	0	used-for	used-for	1
it is used for the synthesis method based on a #selection of articulatory targets# and $interpolation technique$ .	1	conjunction	conjunction	1
$it$ is used for the synthesis method based on a selection of articulatory targets and #interpolation technique# .	0	used-for	used-for	1
however , our experience with tacitus ; especially in the muc-3 evaluation , has shown that principled #techniques# for $syntactic and pragmatic analysis$ can be bolstered with methods for achieving robustness .	0	used-for	used-for	1
however , our experience with tacitus ; especially in the muc-3 evaluation , has shown that principled techniques for syntactic and pragmatic analysis can be bolstered with $methods$ for achieving #robustness# .	2	evaluate-for	evaluate-for	1
we describe #three techniques# for making $syntactic analysis$ more robust -- an agenda-based scheduling parser , a recovery technique for failed parses , and a new technique called terminal substring parsing .	0	used-for	used-for	1
we describe $three techniques$ for making syntactic analysis more robust -- an #agenda-based scheduling parser# , a recovery technique for failed parses , and a new technique called terminal substring parsing .	3	hyponym-of	hyponym-of	1
we describe three techniques for making syntactic analysis more robust -- an #agenda-based scheduling parser# , a $recovery technique$ for failed parses , and a new technique called terminal substring parsing .	1	conjunction	conjunction	1
we describe $three techniques$ for making syntactic analysis more robust -- an agenda-based scheduling parser , a #recovery technique# for failed parses , and a new technique called terminal substring parsing .	3	hyponym-of	hyponym-of	1
we describe three techniques for making $syntactic analysis$ more robust -- an agenda-based scheduling parser , a #recovery technique# for failed parses , and a new technique called terminal substring parsing .	0	used-for	used-for	1
we describe three techniques for making syntactic analysis more robust -- an agenda-based scheduling parser , a #recovery technique# for $failed parses$ , and a new technique called terminal substring parsing .	0	used-for	used-for	1
we describe three techniques for making syntactic analysis more robust -- an agenda-based scheduling parser , a #recovery technique# for failed parses , and a new $technique$ called terminal substring parsing .	1	conjunction	conjunction	1
we describe $three techniques$ for making syntactic analysis more robust -- an agenda-based scheduling parser , a recovery technique for failed parses , and a new #technique# called terminal substring parsing .	3	hyponym-of	hyponym-of	1
for $pragmatics processing$ , we describe how the method of #abductive inference# is inherently robust , in that an interpretation is always possible , so that in the absence of the required world knowledge , performance degrades gracefully .	0	used-for	used-for	1
this paper proposes a #hidden markov model -lrb- hmm -rrb-# and an $hmm-based chunk tagger$ , from which a named entity -lrb- ne -rrb- recognition -lrb- ner -rrb- system is built to recognize and classify names , times and numerical quantities .	1	conjunction	conjunction	1
this paper proposes a #hidden markov model -lrb- hmm -rrb-# and an hmm-based chunk tagger , from which a $named entity -lrb- ne -rrb- recognition -lrb- ner -rrb- system$ is built to recognize and classify names , times and numerical quantities .	0	used-for	used-for	1
this paper proposes a hidden markov model -lrb- hmm -rrb- and an #hmm-based chunk tagger# , from which a $named entity -lrb- ne -rrb- recognition -lrb- ner -rrb- system$ is built to recognize and classify names , times and numerical quantities .	0	used-for	used-for	1
this paper proposes a hidden markov model -lrb- hmm -rrb- and an hmm-based chunk tagger , from which a #named entity -lrb- ne -rrb- recognition -lrb- ner -rrb- system# is built to recognize and classify $names$ , times and numerical quantities .	0	used-for	used-for	1
this paper proposes a hidden markov model -lrb- hmm -rrb- and an hmm-based chunk tagger , from which a #named entity -lrb- ne -rrb- recognition -lrb- ner -rrb- system# is built to recognize and classify names , $times and numerical quantities$ .	0	used-for	used-for	1
this paper proposes a hidden markov model -lrb- hmm -rrb- and an hmm-based chunk tagger , from which a named entity -lrb- ne -rrb- recognition -lrb- ner -rrb- system is built to recognize and classify #names# , $times and numerical quantities$ .	1	conjunction	conjunction	1
through the hmm , our system is able to apply and integrate four types of internal and external evidences : 1 -rrb- simple $deterministic internal feature of the words$ , such as #capitalization# and digitalization ; 2 -rrb- internal semantic feature of important triggers ; 3 -rrb- internal gazetteer feature ; 4 -rrb- external macro context feature .	3	hyponym-of	hyponym-of	1
through the hmm , our system is able to apply and integrate four types of internal and external evidences : 1 -rrb- simple deterministic internal feature of the words , such as #capitalization# and $digitalization$ ; 2 -rrb- internal semantic feature of important triggers ; 3 -rrb- internal gazetteer feature ; 4 -rrb- external macro context feature .	1	conjunction	conjunction	1
through the hmm , our system is able to apply and integrate four types of internal and external evidences : 1 -rrb- simple $deterministic internal feature of the words$ , such as capitalization and #digitalization# ; 2 -rrb- internal semantic feature of important triggers ; 3 -rrb- internal gazetteer feature ; 4 -rrb- external macro context feature .	3	hyponym-of	hyponym-of	1
evaluation of our $system$ on #muc-6 and muc-7 english ne tasks# achieves f-measures of 96.6 % and 94.1 % respectively .	2	evaluate-for	evaluate-for	1
evaluation of our $system$ on muc-6 and muc-7 english ne tasks achieves #f-measures# of 96.6 % and 94.1 % respectively .	2	evaluate-for	evaluate-for	1
two #themes# have evolved in $speech and text image processing$ work at xerox parc that expand and redefine the role of recognition technology in document-oriented applications .	4	part-of	part-of	1
two themes have evolved in speech and text image processing work at xerox parc that expand and redefine the role of #recognition technology# in $document-oriented applications$ .	0	used-for	used-for	1
one is the development of #systems# that provide functionality similar to that of $text processors$ but operate directly on audio and scanned image data .	1	conjunction	conjunction	1
one is the development of $systems$ that provide functionality similar to that of text processors but operate directly on #audio and scanned image data# .	0	used-for	used-for	1
a second , related $theme$ is the use of #speech and text-image recognition# to retrieve arbitrary , user-specified information from documents with signal content .	0	used-for	used-for	1
a second , related theme is the use of $speech and text-image recognition$ to retrieve arbitrary , user-specified information from #documents with signal content# .	0	used-for	used-for	1
this paper discusses three $research$ initiatives at parc that exemplify these themes : a #text-image editor# -lsb- 1 -rsb- , a wordspotter for voice editing and indexing -lsb- 12 -rsb- , and a decoding framework for scanned-document content retrieval -lsb- 4 -rsb- .	3	hyponym-of	hyponym-of	1
this paper discusses three research initiatives at parc that exemplify these themes : a #text-image editor# -lsb- 1 -rsb- , a $wordspotter$ for voice editing and indexing -lsb- 12 -rsb- , and a decoding framework for scanned-document content retrieval -lsb- 4 -rsb- .	1	conjunction	conjunction	1
this paper discusses three $research$ initiatives at parc that exemplify these themes : a text-image editor -lsb- 1 -rsb- , a #wordspotter# for voice editing and indexing -lsb- 12 -rsb- , and a decoding framework for scanned-document content retrieval -lsb- 4 -rsb- .	3	hyponym-of	hyponym-of	1
this paper discusses three research initiatives at parc that exemplify these themes : a text-image editor -lsb- 1 -rsb- , a #wordspotter# for $voice editing and indexing$ -lsb- 12 -rsb- , and a decoding framework for scanned-document content retrieval -lsb- 4 -rsb- .	1	conjunction	conjunction	1
this paper discusses three research initiatives at parc that exemplify these themes : a text-image editor -lsb- 1 -rsb- , a wordspotter for #voice editing and indexing# -lsb- 12 -rsb- , and a $decoding framework$ for scanned-document content retrieval -lsb- 4 -rsb- .	1	conjunction	conjunction	1
this paper discusses three $research$ initiatives at parc that exemplify these themes : a text-image editor -lsb- 1 -rsb- , a wordspotter for voice editing and indexing -lsb- 12 -rsb- , and a #decoding framework# for scanned-document content retrieval -lsb- 4 -rsb- .	3	hyponym-of	hyponym-of	1
this paper discusses three research initiatives at parc that exemplify these themes : a text-image editor -lsb- 1 -rsb- , a wordspotter for voice editing and indexing -lsb- 12 -rsb- , and a #decoding framework# for $scanned-document content retrieval$ -lsb- 4 -rsb- .	0	used-for	used-for	1
the problem of $predicting image or video interestingness$ from their #low-level feature representations# has received increasing interest .	0	used-for	used-for	1
to make the annotation less subjective and more reliable , recent studies employ #crowdsourcing tools# to collect $pairwise comparisons$ -- relying on majority voting to prune the annotation outliers/errors .	0	used-for	used-for	1
to make the annotation less subjective and more reliable , recent studies employ $crowdsourcing tools$ to collect pairwise comparisons -- relying on #majority voting# to prune the annotation outliers/errors .	0	used-for	used-for	1
to make the annotation less subjective and more reliable , recent studies employ crowdsourcing tools to collect pairwise comparisons -- relying on #majority voting# to prune the $annotation outliers/errors$ .	0	used-for	used-for	1
in this paper , we propose a more principled #way# to identify $annotation outliers$ by formulating the interestingness prediction task as a unified robust learning to rank problem , tackling both the outlier detection and interestingness prediction tasks jointly .	0	used-for	used-for	1
in this paper , we propose a more principled #way# to identify annotation outliers by formulating the interestingness prediction task as a unified robust learning to rank problem , tackling both the $outlier detection$ and interestingness prediction tasks jointly .	0	used-for	used-for	1
in this paper , we propose a more principled #way# to identify annotation outliers by formulating the interestingness prediction task as a unified robust learning to rank problem , tackling both the outlier detection and $interestingness prediction tasks$ jointly .	0	used-for	used-for	1
in this paper , we propose a more principled way to identify $annotation outliers$ by formulating the #interestingness prediction task# as a unified robust learning to rank problem , tackling both the outlier detection and interestingness prediction tasks jointly .	0	used-for	used-for	1
in this paper , we propose a more principled way to identify annotation outliers by formulating the $interestingness prediction task$ as a #unified robust learning# to rank problem , tackling both the outlier detection and interestingness prediction tasks jointly .	0	used-for	used-for	1
in this paper , we propose a more principled way to identify annotation outliers by formulating the interestingness prediction task as a #unified robust learning# to $rank problem$ , tackling both the outlier detection and interestingness prediction tasks jointly .	0	used-for	used-for	1
in this paper , we propose a more principled way to identify annotation outliers by formulating the interestingness prediction task as a unified robust learning to rank problem , tackling both the #outlier detection# and $interestingness prediction tasks$ jointly .	1	conjunction	conjunction	1
extensive experiments on both #image and video interestingness benchmark datasets# demonstrate that our new $approach$ significantly outperforms state-of-the-art alternatives .	2	evaluate-for	evaluate-for	1
extensive experiments on both image and video interestingness benchmark datasets demonstrate that our new #approach# significantly outperforms $state-of-the-art alternatives$ .	6	compare	compare	1
many $description logics -lrb- dls -rrb-$ combine #knowledge representation# on an abstract , logical level with an interface to `` concrete '' domains such as numbers and strings .	1	conjunction	conjunction	1
we describe an implementation of #data-driven selection# of emphatic facial displays for an $embodied conversational agent$ in a dialogue system .	0	used-for	used-for	1
we describe an implementation of $data-driven selection$ of #emphatic facial displays# for an embodied conversational agent in a dialogue system .	0	used-for	used-for	1
we describe an implementation of data-driven selection of emphatic facial displays for an #embodied conversational agent# in a $dialogue system$ .	4	part-of	part-of	1
the #data# from those recordings was used in a range of $models$ for generating facial displays , each model making use of a different amount of context or choosing displays differently within a context .	0	used-for	used-for	1
the data from those recordings was used in a range of #models# for generating $facial displays$ , each model making use of a different amount of context or choosing displays differently within a context .	0	used-for	used-for	1
the $models$ were evaluated in two ways : by #cross-validation# against the corpus , and by asking users to rate the output .	2	evaluate-for	evaluate-for	1
when $classifying high-dimensional sequence data$ , traditional methods -lrb- e.g. , #hmms# , crfs -rrb- may require large amounts of training data to avoid overfitting .	0	used-for	used-for	1
when classifying high-dimensional sequence data , traditional methods -lrb- e.g. , #hmms# , $crfs$ -rrb- may require large amounts of training data to avoid overfitting .	1	conjunction	conjunction	1
when $classifying high-dimensional sequence data$ , traditional methods -lrb- e.g. , hmms , #crfs# -rrb- may require large amounts of training data to avoid overfitting .	0	used-for	used-for	1
in such cases #dimensionality reduction# can be employed to find a $low-dimensional representation$ on which classification can be done more efficiently .	0	used-for	used-for	1
in such cases dimensionality reduction can be employed to find a #low-dimensional representation# on which $classification$ can be done more efficiently .	0	used-for	used-for	1
#existing methods# for $supervised dimensionality reduction$ often presume that the data is densely sampled so that a neighborhood graph structure can be formed , or that the data arises from a known distribution .	0	used-for	used-for	1
#sufficient dimension reduction techniques# aim to find a $low dimensional representation$ such that the remaining degrees of freedom become conditionally independent of the output values .	0	used-for	used-for	1
spatial , temporal and periodic information is combined in a principled manner , and an optimal #manifold# is learned for the $end-task$ .	0	used-for	used-for	1
we demonstrate the effectiveness of our $approach$ on several tasks involving the #discrimination of human gesture and motion categories# , as well as on a database of dynamic textures .	2	evaluate-for	evaluate-for	1
we demonstrate the effectiveness of our $approach$ on several tasks involving the discrimination of human gesture and motion categories , as well as on a #database of dynamic textures# .	2	evaluate-for	evaluate-for	1
we present an efficient #algorithm# for $chart-based phrase structure parsing$ of natural language that is tailored to the problem of extracting specific information from unrestricted texts where many of the words are unknown and much of the text is irrelevant to the task .	0	used-for	used-for	1
we present an efficient algorithm for $chart-based phrase structure parsing$ of #natural language# that is tailored to the problem of extracting specific information from unrestricted texts where many of the words are unknown and much of the text is irrelevant to the task .	0	used-for	used-for	1
this is facilitated through the use of $phrase boundary heuristics$ based on the placement of #function words# , and by heuristic rules that permit certain kinds of phrases to be deduced despite the presence of unknown words .	0	used-for	used-for	1
a further $reduction in the search space$ is achieved by using #semantic# rather than syntactic categories on the terminal and non-terminal edges , thereby reducing the amount of ambiguity and thus the number of edges , since only edges with a valid semantic interpretation are ever introduced .	0	used-for	used-for	1
a further reduction in the search space is achieved by using #semantic# rather than $syntactic categories$ on the terminal and non-terminal edges , thereby reducing the amount of ambiguity and thus the number of edges , since only edges with a valid semantic interpretation are ever introduced .	6	compare	compare	1
a further reduction in the search space is achieved by using #semantic# rather than syntactic categories on the $terminal and non-terminal edges$ , thereby reducing the amount of ambiguity and thus the number of edges , since only edges with a valid semantic interpretation are ever introduced .	5	feature-of	feature-of	1
a further reduction in the search space is achieved by using semantic rather than #syntactic categories# on the $terminal and non-terminal edges$ , thereby reducing the amount of ambiguity and thus the number of edges , since only edges with a valid semantic interpretation are ever introduced .	5	feature-of	feature-of	1
#automatic estimation of word significance# oriented for $speech-based information retrieval -lrb- ir -rrb-$ is addressed .	0	used-for	used-for	1
since the significance of words differs in ir , $automatic speech recognition -lrb- asr -rrb-$ performance has been evaluated based on #weighted word error rate -lrb- wwer -rrb-# , which gives a weight on errors from the viewpoint of ir , instead of word error rate -lrb- wer -rrb- , which treats all words uniformly .	2	evaluate-for	evaluate-for	1
since the significance of words differs in ir , automatic speech recognition -lrb- asr -rrb- performance has been evaluated based on $weighted word error rate -lrb- wwer -rrb-$ , which gives a weight on errors from the viewpoint of ir , instead of #word error rate -lrb- wer -rrb-# , which treats all words uniformly .	6	compare	compare	1
a #decoding strategy# that minimizes $wwer$ based on a minimum bayes-risk framework has been shown , and the reduction of errors on both asr and ir has been reported .	0	used-for	used-for	1
a $decoding strategy$ that minimizes wwer based on a #minimum bayes-risk framework# has been shown , and the reduction of errors on both asr and ir has been reported .	0	used-for	used-for	1
a decoding strategy that minimizes wwer based on a minimum bayes-risk framework has been shown , and the reduction of errors on both #asr# and $ir$ has been reported .	1	conjunction	conjunction	1
in this paper , we propose an #automatic estimation method# for $word significance -lrb- weights -rrb-$ based on its influence on ir .	2	evaluate-for	evaluate-for	1
specifically , weights are estimated so that #evaluation measures# of $asr$ and ir are equivalent .	2	evaluate-for	evaluate-for	1
specifically , weights are estimated so that #evaluation measures# of asr and $ir$ are equivalent .	2	evaluate-for	evaluate-for	1
specifically , weights are estimated so that evaluation measures of #asr# and $ir$ are equivalent .	1	conjunction	conjunction	1
we apply the proposed #method# to a $speech-based information retrieval system$ , which is a typical ir system , and show that the method works well .	0	used-for	used-for	1
we apply the proposed method to a #speech-based information retrieval system# , which is a typical $ir system$ , and show that the method works well .	3	hyponym-of	hyponym-of	1
#methods# developed for $spelling correction$ for languages like english -lrb- see the review by kukich -lrb- kukich , 1992 -rrb- -rrb- are not readily applicable to agglutinative languages .	0	used-for	used-for	1
methods developed for #spelling correction# for $languages$ like english -lrb- see the review by kukich -lrb- kukich , 1992 -rrb- -rrb- are not readily applicable to agglutinative languages .	0	used-for	used-for	1
methods developed for spelling correction for $languages$ like #english# -lrb- see the review by kukich -lrb- kukich , 1992 -rrb- -rrb- are not readily applicable to agglutinative languages .	3	hyponym-of	hyponym-of	1
this poster presents an #approach# to $spelling correction$ in agglutinative languages that is based on two-level morphology and a dynamic-programming based search algorithm .	0	used-for	used-for	1
this poster presents an approach to $spelling correction$ in #agglutinative languages# that is based on two-level morphology and a dynamic-programming based search algorithm .	0	used-for	used-for	1
this poster presents an approach to $spelling correction$ in agglutinative languages that is based on #two-level morphology# and a dynamic-programming based search algorithm .	0	used-for	used-for	1
this poster presents an approach to spelling correction in agglutinative languages that is based on #two-level morphology# and a $dynamic-programming based search algorithm$ .	1	conjunction	conjunction	1
this poster presents an approach to $spelling correction$ in agglutinative languages that is based on two-level morphology and a #dynamic-programming based search algorithm# .	0	used-for	used-for	1
after an overview of our approach , we present results from experiments with $spelling correction$ in #turkish# .	0	used-for	used-for	1
in this paper , we present a novel #training method# for a $localized phrase-based prediction model$ for statistical machine translation -lrb- smt -rrb- .	0	used-for	used-for	1
in this paper , we present a novel training method for a #localized phrase-based prediction model# for $statistical machine translation -lrb- smt -rrb-$ .	0	used-for	used-for	1
the #model# predicts blocks with orientation to handle $local phrase re-ordering$ .	0	used-for	used-for	1
we use a #maximum likelihood criterion# to train a $log-linear block bigram model$ which uses real-valued features -lrb- e.g. a language model score -rrb- as well as binary features based on the block identities themselves , e.g. block bigram features .	0	used-for	used-for	1
we use a maximum likelihood criterion to train a $log-linear block bigram model$ which uses #real-valued features# -lrb- e.g. a language model score -rrb- as well as binary features based on the block identities themselves , e.g. block bigram features .	0	used-for	used-for	1
we use a maximum likelihood criterion to train a log-linear block bigram model which uses #real-valued features# -lrb- e.g. a language model score -rrb- as well as $binary features$ based on the block identities themselves , e.g. block bigram features .	1	conjunction	conjunction	1
we use a maximum likelihood criterion to train a log-linear block bigram model which uses $real-valued features$ -lrb- e.g. a #language model score# -rrb- as well as binary features based on the block identities themselves , e.g. block bigram features .	3	hyponym-of	hyponym-of	1
we use a maximum likelihood criterion to train a $log-linear block bigram model$ which uses real-valued features -lrb- e.g. a language model score -rrb- as well as #binary features# based on the block identities themselves , e.g. block bigram features .	0	used-for	used-for	1
our $training algorithm$ can easily handle millions of #features# .	0	used-for	used-for	1
the best #system# obtains a 18.6 % improvement over the $baseline$ on a standard arabic-english translation task .	6	compare	compare	1
the best $system$ obtains a 18.6 % improvement over the baseline on a standard #arabic-english translation task# .	2	evaluate-for	evaluate-for	1
the best system obtains a 18.6 % improvement over the $baseline$ on a standard #arabic-english translation task# .	2	evaluate-for	evaluate-for	1
in this paper we describe a novel #data structure# for $phrase-based statistical machine translation$ which allows for the retrieval of arbitrarily long phrases while simultaneously using less memory than is required by current decoder implementations .	0	used-for	used-for	1
in this paper we describe a novel #data structure# for phrase-based statistical machine translation which allows for the $retrieval of arbitrarily long phrases$ while simultaneously using less memory than is required by current decoder implementations .	0	used-for	used-for	1
we detail the #computational complexity# and $average retrieval times$ for looking up phrase translations in our suffix array-based data structure .	1	conjunction	conjunction	1
we detail the computational complexity and average retrieval times for looking up #phrase translations# in our $suffix array-based data structure$ .	4	part-of	part-of	1
we show how $sampling$ can be used to reduce the #retrieval time# by orders of magnitude with no loss in translation quality .	2	evaluate-for	evaluate-for	1
we show how $sampling$ can be used to reduce the retrieval time by orders of magnitude with no loss in #translation quality# .	2	evaluate-for	evaluate-for	1
the major objective of this program is to develop and demonstrate robust , high performance #continuous speech recognition -lrb- csr -rrb- techniques# focussed on application in $spoken language systems -lrb- sls -rrb-$ which will enhance the effectiveness of military and civilian computer-based systems .	0	used-for	used-for	1
the major objective of this program is to develop and demonstrate robust , high performance continuous speech recognition -lrb- csr -rrb- techniques focussed on application in #spoken language systems -lrb- sls -rrb-# which will enhance the effectiveness of $military and civilian computer-based systems$ .	0	used-for	used-for	1
a key complementary objective is to define and develop applications of robust speech recognition and understanding systems , and to help catalyze the transition of #spoken language technology# into $military and civilian systems$ , with particular focus on application of robust csr to mobile military command and control .	0	used-for	used-for	1
a key complementary objective is to define and develop applications of robust speech recognition and understanding systems , and to help catalyze the transition of spoken language technology into military and civilian systems , with particular focus on application of robust #csr# to $mobile military command and control$ .	0	used-for	used-for	1
the research effort focusses on developing advanced #acoustic modelling# , $rapid search$ , and recognition-time adaptation techniques for robust large-vocabulary csr , and on applying these techniques to the new arpa large-vocabulary csr corpora and to military application tasks .	1	conjunction	conjunction	1
the research effort focusses on developing advanced #acoustic modelling# , rapid search , and recognition-time adaptation techniques for robust $large-vocabulary csr$ , and on applying these techniques to the new arpa large-vocabulary csr corpora and to military application tasks .	0	used-for	used-for	1
the research effort focusses on developing advanced #acoustic modelling# , rapid search , and recognition-time adaptation techniques for robust large-vocabulary csr , and on applying these $techniques$ to the new arpa large-vocabulary csr corpora and to military application tasks .	3	hyponym-of	hyponym-of	1
the research effort focusses on developing advanced #acoustic modelling# , rapid search , and recognition-time adaptation techniques for robust large-vocabulary csr , and on applying these techniques to the new arpa large-vocabulary csr corpora and to $military application tasks$ .	0	used-for	used-for	1
the research effort focusses on developing advanced acoustic modelling , #rapid search# , and $recognition-time adaptation techniques$ for robust large-vocabulary csr , and on applying these techniques to the new arpa large-vocabulary csr corpora and to military application tasks .	1	conjunction	conjunction	1
the research effort focusses on developing advanced acoustic modelling , #rapid search# , and recognition-time adaptation techniques for robust $large-vocabulary csr$ , and on applying these techniques to the new arpa large-vocabulary csr corpora and to military application tasks .	0	used-for	used-for	1
the research effort focusses on developing advanced acoustic modelling , #rapid search# , and recognition-time adaptation techniques for robust large-vocabulary csr , and on applying these $techniques$ to the new arpa large-vocabulary csr corpora and to military application tasks .	3	hyponym-of	hyponym-of	1
the research effort focusses on developing advanced acoustic modelling , #rapid search# , and recognition-time adaptation techniques for robust large-vocabulary csr , and on applying these techniques to the new arpa large-vocabulary csr corpora and to $military application tasks$ .	0	used-for	used-for	1
the research effort focusses on developing advanced acoustic modelling , rapid search , and #recognition-time adaptation techniques# for robust $large-vocabulary csr$ , and on applying these techniques to the new arpa large-vocabulary csr corpora and to military application tasks .	0	used-for	used-for	1
the research effort focusses on developing advanced acoustic modelling , rapid search , and #recognition-time adaptation techniques# for robust large-vocabulary csr , and on applying these $techniques$ to the new arpa large-vocabulary csr corpora and to military application tasks .	3	hyponym-of	hyponym-of	1
the research effort focusses on developing advanced acoustic modelling , rapid search , and #recognition-time adaptation techniques# for robust large-vocabulary csr , and on applying these techniques to the new arpa large-vocabulary csr corpora and to $military application tasks$ .	0	used-for	used-for	1
the research effort focusses on developing advanced acoustic modelling , rapid search , and recognition-time adaptation techniques for robust #large-vocabulary csr# , and on applying these techniques to the new $arpa large-vocabulary csr corpora$ and to military application tasks .	0	used-for	used-for	1
the research effort focusses on developing advanced acoustic modelling , rapid search , and recognition-time adaptation techniques for robust large-vocabulary csr , and on applying these #techniques# to the new arpa large-vocabulary csr corpora and to $military application tasks$ .	0	used-for	used-for	1
the research effort focusses on developing advanced acoustic modelling , rapid search , and recognition-time adaptation techniques for robust large-vocabulary csr , and on applying these $techniques$ to the new #arpa large-vocabulary csr corpora# and to military application tasks .	2	evaluate-for	evaluate-for	1
this paper examines what kind of $similarity between words$ can be represented by what kind of #word vectors# in the vector space model .	0	used-for	used-for	1
this paper examines what kind of similarity between words can be represented by what kind of $word vectors$ in the #vector space model# .	0	used-for	used-for	1
through two experiments , three #methods# for $constructing word vectors$ , i.e. , lsa-based , cooccurrence-based and dictionary-based methods , were compared in terms of the ability to represent two kinds of similarity , i.e. , taxonomic similarity and associative similarity .	0	used-for	used-for	1
through two experiments , three methods for constructing word vectors , i.e. , #lsa-based , cooccurrence-based and dictionary-based methods# , were compared in terms of the ability to represent two kinds of $similarity$ , i.e. , taxonomic similarity and associative similarity .	0	used-for	used-for	1
through two experiments , three methods for constructing word vectors , i.e. , lsa-based , cooccurrence-based and dictionary-based methods , were compared in terms of the ability to represent two kinds of $similarity$ , i.e. , #taxonomic similarity# and associative similarity .	3	hyponym-of	hyponym-of	1
through two experiments , three methods for constructing word vectors , i.e. , lsa-based , cooccurrence-based and dictionary-based methods , were compared in terms of the ability to represent two kinds of similarity , i.e. , #taxonomic similarity# and $associative similarity$ .	1	conjunction	conjunction	1
through two experiments , three methods for constructing word vectors , i.e. , lsa-based , cooccurrence-based and dictionary-based methods , were compared in terms of the ability to represent two kinds of $similarity$ , i.e. , taxonomic similarity and #associative similarity# .	3	hyponym-of	hyponym-of	1
the result of the comparison was that the #dictionary-based word vectors# better reflect $taxonomic similarity$ , while the lsa-based and the cooccurrence-based word vectors better reflect associative similarity .	0	used-for	used-for	1
the result of the comparison was that the dictionary-based word vectors better reflect taxonomic similarity , while the #lsa-based and the cooccurrence-based word vectors# better reflect $associative similarity$ .	0	used-for	used-for	1
this paper presents a $maximum entropy word alignment algorithm$ for #arabic-english# based on supervised training data .	0	used-for	used-for	1
this paper presents a $maximum entropy word alignment algorithm$ for arabic-english based on #supervised training data# .	0	used-for	used-for	1
we demonstrate that it is feasible to create #training material# for problems in $machine translation$ and that a mixture of supervised and unsupervised methods yields superior performance .	0	used-for	used-for	1
the #probabilistic model# used in the $alignment$ directly models the link decisions .	0	used-for	used-for	1
the #probabilistic model# used in the alignment directly models the $link decisions$ .	0	used-for	used-for	1
significant improvement over traditional #word alignment techniques# is shown as well as improvement on several $machine translation tests$ .	0	used-for	used-for	1
performance of the #algorithm# is contrasted with $human annotation$ performance .	6	compare	compare	1
in this paper , we propose a novel #cooperative model# for $natural language understanding$ in a dialogue system .	0	used-for	used-for	1
in this paper , we propose a novel cooperative model for #natural language understanding# in a $dialogue system$ .	0	used-for	used-for	1
we build $this$ based on both #finite state model -lrb- fsm -rrb-# and statistical learning model -lrb- slm -rrb- .	0	used-for	used-for	1
we build this based on both #finite state model -lrb- fsm -rrb-# and $statistical learning model -lrb- slm -rrb-$ .	1	conjunction	conjunction	1
we build $this$ based on both finite state model -lrb- fsm -rrb- and #statistical learning model -lrb- slm -rrb-# .	0	used-for	used-for	1
#fsm# provides two strategies for $language understanding$ and have a high accuracy but little robustness and flexibility .	0	used-for	used-for	1
the #ambiguity resolution of right-side dependencies# is essential for $dependency parsing$ of sentences with two or more verbs .	0	used-for	used-for	1
previous works on shift-reduce dependency parsers may not guarantee the #connectivity# of a $dependency tree$ due to their weakness at resolving the right-side dependencies .	2	evaluate-for	evaluate-for	1
this paper proposes a $two-phase shift-reduce dependency parser$ based on #svm learning# .	0	used-for	used-for	1
the #left-side dependents# and $right-side nominal dependents$ are detected in phase i , and right-side verbal dependents are decided in phase ii .	1	conjunction	conjunction	1
the left-side dependents and $right-side nominal dependents$ are detected in phase i , and #right-side verbal dependents# are decided in phase ii .	1	conjunction	conjunction	1
in experimental evaluation , our proposed #method# outperforms previous $shift-reduce dependency parsers$ for the chine language , showing improvement of dependency accuracy by 10.08 % .	6	compare	compare	1
in experimental evaluation , our proposed $method$ outperforms previous shift-reduce dependency parsers for the #chine language# , showing improvement of dependency accuracy by 10.08 % .	2	evaluate-for	evaluate-for	1
in experimental evaluation , our proposed method outperforms previous $shift-reduce dependency parsers$ for the #chine language# , showing improvement of dependency accuracy by 10.08 % .	2	evaluate-for	evaluate-for	1
in experimental evaluation , our proposed $method$ outperforms previous shift-reduce dependency parsers for the chine language , showing improvement of #dependency accuracy# by 10.08 % .	2	evaluate-for	evaluate-for	1
in experimental evaluation , our proposed method outperforms previous $shift-reduce dependency parsers$ for the chine language , showing improvement of #dependency accuracy# by 10.08 % .	2	evaluate-for	evaluate-for	1
by using #commands# or $rules$ which are defined to facilitate the construction of format expected or some mathematical expressions , elaborate and pretty documents can be successfully obtained .	1	conjunction	conjunction	1
by using #commands# or rules which are defined to facilitate the construction of format expected or some $mathematical expressions$ , elaborate and pretty documents can be successfully obtained .	0	used-for	used-for	1
by using commands or #rules# which are defined to facilitate the construction of format expected or some $mathematical expressions$ , elaborate and pretty documents can be successfully obtained .	0	used-for	used-for	1
this paper presents an #evaluation method# employing a latent variable model for $paraphrases$ with their contexts .	2	evaluate-for	evaluate-for	1
this paper presents an $evaluation method$ employing a #latent variable model# for paraphrases with their contexts .	0	used-for	used-for	1
the results also revealed an upper bound of #accuracy# of 77 % with the $method$ when using only topic information .	2	evaluate-for	evaluate-for	1
the results also revealed an upper bound of accuracy of 77 % with the $method$ when using only #topic information# .	0	used-for	used-for	1
we describe the #methods# and $hardware$ that we are using to produce a real-time demonstration of an integrated spoken language system .	1	conjunction	conjunction	1
we describe the #methods# and hardware that we are using to produce a real-time demonstration of an $integrated spoken language system$ .	0	used-for	used-for	1
we describe the methods and #hardware# that we are using to produce a real-time demonstration of an $integrated spoken language system$ .	0	used-for	used-for	1
we describe #algorithms# that greatly reduce the computation needed to compute the $n-best sentence hypotheses$ .	0	used-for	used-for	1
to avoid $grammar coverage problems$ we use a #fully-connected first-order statistical class grammar# .	0	used-for	used-for	1
the $speech-search algorithm$ is implemented on a #board# with a single intel i860 chip , which provides a factor of 5 speed-up over a sun 4 for straight c code .	0	used-for	used-for	1
the speech-search algorithm is implemented on a $board$ with a single #intel i860 chip# , which provides a factor of 5 speed-up over a sun 4 for straight c code .	4	part-of	part-of	1
the speech-search algorithm is implemented on a board with a single #intel i860 chip# , which provides a factor of 5 speed-up over a $sun 4$ for straight c code .	6	compare	compare	1
the speech-search algorithm is implemented on a board with a single #intel i860 chip# , which provides a factor of 5 speed-up over a sun 4 for $straight c code$ .	0	used-for	used-for	1
the speech-search algorithm is implemented on a board with a single intel i860 chip , which provides a factor of 5 speed-up over a #sun 4# for $straight c code$ .	0	used-for	used-for	1
the #board# plugs directly into the vme bus of the sun4 , which controls the $system$ and contains the natural language system and application back end .	0	used-for	used-for	1
the board plugs directly into the #vme bus# of the $sun4$ , which controls the system and contains the natural language system and application back end .	4	part-of	part-of	1
the $board$ plugs directly into the vme bus of the sun4 , which controls the system and contains the #natural language system# and application back end .	4	part-of	part-of	1
the board plugs directly into the vme bus of the sun4 , which controls the system and contains the #natural language system# and $application back end$ .	1	conjunction	conjunction	1
the $board$ plugs directly into the vme bus of the sun4 , which controls the system and contains the natural language system and #application back end# .	4	part-of	part-of	1
we address the problem of $estimating location information$ of an #image# using principles from automated representation learning .	0	used-for	used-for	1
we address the problem of $estimating location information$ of an image using principles from #automated representation learning# .	0	used-for	used-for	1
we pursue a hierarchical sparse coding approach that learns features useful in discriminating images across locations , by initializing $it$ with a #geometric prior# corresponding to transformations between image appearance space and their corresponding location grouping space using the notion of parallel transport on manifolds .	0	used-for	used-for	1
we pursue a hierarchical sparse coding approach that learns features useful in discriminating images across locations , by initializing it with a $geometric prior$ corresponding to transformations between image appearance space and their corresponding location grouping space using the notion of #parallel transport on manifolds# .	0	used-for	used-for	1
we then extend this #approach# to account for the availability of $heterogeneous data modalities$ such as geo-tags and videos pertaining to different locations , and also study a relatively under-addressed problem of transferring knowledge available from certain locations to infer the grouping of data from novel locations .	0	used-for	used-for	1
we then extend this approach to account for the availability of $heterogeneous data modalities$ such as #geo-tags# and videos pertaining to different locations , and also study a relatively under-addressed problem of transferring knowledge available from certain locations to infer the grouping of data from novel locations .	3	hyponym-of	hyponym-of	1
we then extend this approach to account for the availability of heterogeneous data modalities such as #geo-tags# and $videos$ pertaining to different locations , and also study a relatively under-addressed problem of transferring knowledge available from certain locations to infer the grouping of data from novel locations .	1	conjunction	conjunction	1
we then extend this approach to account for the availability of $heterogeneous data modalities$ such as geo-tags and #videos# pertaining to different locations , and also study a relatively under-addressed problem of transferring knowledge available from certain locations to infer the grouping of data from novel locations .	3	hyponym-of	hyponym-of	1
we then extend this approach to account for the availability of heterogeneous data modalities such as geo-tags and videos pertaining to different locations , and also study a relatively under-addressed problem of #transferring knowledge# available from certain locations to infer the $grouping of data$ from novel locations .	0	used-for	used-for	1
we evaluate our $approach$ on several standard #datasets# such as im2gps , san francisco and mediaeval2010 , and obtain state-of-the-art results .	2	evaluate-for	evaluate-for	1
we evaluate our approach on several standard $datasets$ such as #im2gps# , san francisco and mediaeval2010 , and obtain state-of-the-art results .	3	hyponym-of	hyponym-of	1
we evaluate our approach on several standard datasets such as #im2gps# , $san francisco$ and mediaeval2010 , and obtain state-of-the-art results .	1	conjunction	conjunction	1
we evaluate our approach on several standard $datasets$ such as im2gps , #san francisco# and mediaeval2010 , and obtain state-of-the-art results .	3	hyponym-of	hyponym-of	1
we evaluate our approach on several standard datasets such as im2gps , #san francisco# and $mediaeval2010$ , and obtain state-of-the-art results .	1	conjunction	conjunction	1
we evaluate our approach on several standard $datasets$ such as im2gps , san francisco and #mediaeval2010# , and obtain state-of-the-art results .	3	hyponym-of	hyponym-of	1
conventional $hmms$ have #weak duration constraints# .	5	feature-of	feature-of	1
in noisy conditions , the mismatch between corrupted speech signals and $models$ trained on #clean speech# may cause the decoder to produce word matches with unrealistic durations .	0	used-for	used-for	1
in noisy conditions , the mismatch between corrupted speech signals and models trained on clean speech may cause the #decoder# to produce $word matches$ with unrealistic durations .	0	used-for	used-for	1
in noisy conditions , the mismatch between corrupted speech signals and models trained on clean speech may cause the decoder to produce $word matches$ with #unrealistic durations# .	5	feature-of	feature-of	1
this paper presents a simple way to incorporate $word duration constraints$ by #unrolling hmms# to form a lattice where word duration probabilities can be applied directly to state transitions .	0	used-for	used-for	1
this paper presents a simple way to incorporate word duration constraints by #unrolling hmms# to form a $lattice$ where word duration probabilities can be applied directly to state transitions .	0	used-for	used-for	1
this paper presents a simple way to incorporate word duration constraints by unrolling hmms to form a lattice where #word duration probabilities# can be applied directly to $state transitions$ .	0	used-for	used-for	1
the expanded $hmms$ are compatible with conventional #viterbi decoding# .	1	conjunction	conjunction	1
experiments on #connected-digit recognition# show that when using explicit duration constraints the $decoder$ generates word matches with more reasonable durations , and word error rates are significantly reduced across a broad range of noise conditions .	0	used-for	used-for	1
experiments on connected-digit recognition show that when using explicit #duration constraints# the $decoder$ generates word matches with more reasonable durations , and word error rates are significantly reduced across a broad range of noise conditions .	0	used-for	used-for	1
experiments on connected-digit recognition show that when using explicit duration constraints the #decoder# generates $word matches$ with more reasonable durations , and word error rates are significantly reduced across a broad range of noise conditions .	0	used-for	used-for	1
one of the claimed benefits of #tree adjoining grammars# is that they have an $extended domain of locality -lrb- edol -rrb-$ .	5	feature-of	feature-of	1
we consider how this can be exploited to limit the need for #feature structure unification# during $parsing$ .	0	used-for	used-for	1
we compare two wide-coverage $lexicalized grammars of english$ , #lexsys# and xtag , finding that the two grammars exploit edol in different ways .	3	hyponym-of	hyponym-of	1
we compare two wide-coverage lexicalized grammars of english , #lexsys# and $xtag$ , finding that the two grammars exploit edol in different ways .	6	compare	compare	1
we compare two wide-coverage $lexicalized grammars of english$ , lexsys and #xtag# , finding that the two grammars exploit edol in different ways .	3	hyponym-of	hyponym-of	1
we compare two wide-coverage lexicalized grammars of english , lexsys and xtag , finding that the two #grammars# exploit $edol$ in different ways .	0	used-for	used-for	1
#identity uncertainty# is a pervasive problem in $real-world data analysis$ .	3	hyponym-of	hyponym-of	1
our $approach$ is based on the use of a #relational probability model# to define a generative model for the domain , including models of author and title corruption and a probabilistic citation grammar .	0	used-for	used-for	1
our approach is based on the use of a #relational probability model# to define a $generative model$ for the domain , including models of author and title corruption and a probabilistic citation grammar .	0	used-for	used-for	1
our approach is based on the use of a relational probability model to define a #generative model# for the $domain$ , including models of author and title corruption and a probabilistic citation grammar .	0	used-for	used-for	1
our approach is based on the use of a $relational probability model$ to define a generative model for the domain , including #models of author and title corruption# and a probabilistic citation grammar .	4	part-of	part-of	1
our approach is based on the use of a relational probability model to define a generative model for the domain , including #models of author and title corruption# and a $probabilistic citation grammar$ .	1	conjunction	conjunction	1
our approach is based on the use of a $relational probability model$ to define a generative model for the domain , including models of author and title corruption and a #probabilistic citation grammar# .	4	part-of	part-of	1
$identity uncertainty$ is handled by extending standard #models# to incorporate probabilities over the possible mappings between terms in the language and objects in the domain .	0	used-for	used-for	1
$inference$ is based on #markov chain monte carlo# , augmented with specific methods for generating efficient proposals when the domain contains many objects .	0	used-for	used-for	1
$inference$ is based on markov chain monte carlo , augmented with specific #methods# for generating efficient proposals when the domain contains many objects .	0	used-for	used-for	1
results on several #citation data sets# show that the $method$ outperforms current algorithms for citation matching .	2	evaluate-for	evaluate-for	1
results on several citation data sets show that the #method# outperforms $current algorithms$ for citation matching .	6	compare	compare	1
results on several citation data sets show that the #method# outperforms current algorithms for $citation matching$ .	0	used-for	used-for	1
results on several citation data sets show that the method outperforms #current algorithms# for $citation matching$ .	0	used-for	used-for	1
the declarative , relational nature of the model also means that our #algorithm# can determine $object characteristics$ such as author names by combining multiple citations of multiple papers .	0	used-for	used-for	1
the declarative , relational nature of the model also means that our algorithm can determine $object characteristics$ such as #author names# by combining multiple citations of multiple papers .	3	hyponym-of	hyponym-of	1
the paper proposes and empirically motivates an integration of #supervised learning# with unsupervised learning to deal with $human biases in summarization$ .	0	used-for	used-for	1
the paper proposes and empirically motivates an integration of $supervised learning$ with #unsupervised learning# to deal with human biases in summarization .	1	conjunction	conjunction	1
the paper proposes and empirically motivates an integration of supervised learning with #unsupervised learning# to deal with $human biases in summarization$ .	0	used-for	used-for	1
in particular , we explore the use of $probabilistic decision tree$ within the #clustering framework# to account for the variation as well as regularity in human created summaries .	5	feature-of	feature-of	1
the $corpus of human created extracts$ is created from a #newspaper corpus# and used as a test set .	0	used-for	used-for	1
we build probabilistic decision trees of different flavors and integrate each of $them$ with the #clustering framework# .	1	conjunction	conjunction	1
in this study , we propose a knowledge-independent method for aligning terms and thus extracting translations from a $small , domain-specific corpus$ consisting of #parallel english and chinese court judgments# from hong kong .	4	part-of	part-of	1
with a #sentence-aligned corpus# , $translation equivalences$ are suggested by analysing the frequency profiles of parallel concordances .	0	used-for	used-for	1
with a sentence-aligned corpus , translation equivalences are suggested by analysing the #frequency profiles# of $parallel concordances$ .	4	part-of	part-of	1
the #method# overcomes the limitations of conventional $statistical methods$ which require large corpora to be effective , and lexical approaches which depend on existing bilingual dictionaries .	6	compare	compare	1
the #method# overcomes the limitations of conventional statistical methods which require large corpora to be effective , and $lexical approaches$ which depend on existing bilingual dictionaries .	6	compare	compare	1
the method overcomes the limitations of conventional $statistical methods$ which require #large corpora# to be effective , and lexical approaches which depend on existing bilingual dictionaries .	0	used-for	used-for	1
the method overcomes the limitations of conventional statistical methods which require large corpora to be effective , and $lexical approaches$ which depend on existing #bilingual dictionaries# .	0	used-for	used-for	1
pilot testing on a parallel corpus of about 113k chinese words and 120k english words gives an encouraging 85 % #precision# and 45 % $recall$ .	1	conjunction	conjunction	1
future work includes fine-tuning the algorithm upon the analysis of the errors , and acquiring a #translation lexicon# for $legal terminology$ by filtering out general terms .	0	used-for	used-for	1
traditional #machine learning techniques# have been applied to this $problem$ with reasonable success , but they have been shown to work well only when there is a good match between the training and test data with respect to topic .	0	used-for	used-for	1
this paper demonstrates that match with respect to domain and time is also important , and presents preliminary experiments with $training data$ labeled with #emoticons# , which has the potential of being independent of domain , topic and time .	5	feature-of	feature-of	1
we present a novel #algorithm# for estimating the broad $3d geometric structure of outdoor video scenes$ .	0	used-for	used-for	1
leveraging #spatio-temporal video segmentation# , we decompose a $dynamic scene$ captured by a video into geometric classes , based on predictions made by region-classifiers that are trained on appearance and motion features .	0	used-for	used-for	1
leveraging spatio-temporal video segmentation , we decompose a $dynamic scene$ captured by a video into #geometric classes# , based on predictions made by region-classifiers that are trained on appearance and motion features .	4	part-of	part-of	1
leveraging spatio-temporal video segmentation , we decompose a dynamic scene captured by a video into $geometric classes$ , based on predictions made by #region-classifiers# that are trained on appearance and motion features .	0	used-for	used-for	1
leveraging spatio-temporal video segmentation , we decompose a dynamic scene captured by a video into geometric classes , based on predictions made by $region-classifiers$ that are trained on #appearance and motion features# .	0	used-for	used-for	1
we built a novel , extensive #dataset# on geometric context of video to evaluate our $method$ , consisting of over 100 ground-truth annotated outdoor videos with over 20,000 frames .	2	evaluate-for	evaluate-for	1
we built a novel , extensive $dataset$ on #geometric context of video# to evaluate our method , consisting of over 100 ground-truth annotated outdoor videos with over 20,000 frames .	5	feature-of	feature-of	1
we built a novel , extensive $dataset$ on geometric context of video to evaluate our method , consisting of over 100 ground-truth #annotated outdoor videos# with over 20,000 frames .	4	part-of	part-of	1
to further scale beyond this dataset , we propose a #semi-supervised learning framework# to expand the pool of $labeled data$ with high confidence predictions obtained from unlabeled data .	0	used-for	used-for	1
to further scale beyond this dataset , we propose a $semi-supervised learning framework$ to expand the pool of labeled data with #high confidence predictions# obtained from unlabeled data .	0	used-for	used-for	1
to further scale beyond this dataset , we propose a semi-supervised learning framework to expand the pool of labeled data with $high confidence predictions$ obtained from #unlabeled data# .	0	used-for	used-for	1
our #system# produces an accurate prediction of $geometric context of video$ achieving 96 % accuracy across main geometric classes .	0	used-for	used-for	1
our $system$ produces an accurate prediction of geometric context of video achieving 96 % #accuracy# across main geometric classes .	2	evaluate-for	evaluate-for	1
this paper describes a #system# -lrb- rareas -rrb- which synthesizes $marine weather forecasts$ directly from formatted weather data .	0	used-for	used-for	1
this paper describes a $system$ -lrb- rareas -rrb- which synthesizes marine weather forecasts directly from #formatted weather data# .	0	used-for	used-for	1
such $synthesis$ appears feasible in certain #natural sublanguages with stereotyped text structure# .	0	used-for	used-for	1
$rareas$ draws on several kinds of #linguistic and non-linguistic knowledge# and mirrors a forecaster 's apparent tendency to ascribe less precise temporal adverbs to more remote meteorological events .	0	used-for	used-for	1
the $approach$ can easily be adapted to synthesize #bilingual or multi-lingual texts# .	0	used-for	used-for	1
we go , on to describe #flexp# , a $bottom-up pattern-matching parser$ that we have designed and implemented to provide these flexibilities for restricted natural language input to a limited-domain computer system .	3	hyponym-of	hyponym-of	1
we go , on to describe flexp , a #bottom-up pattern-matching parser# that we have designed and implemented to provide these $flexibilities$ for restricted natural language input to a limited-domain computer system .	0	used-for	used-for	1
we go , on to describe flexp , a bottom-up pattern-matching parser that we have designed and implemented to provide these #flexibilities# for $restricted natural language$ input to a limited-domain computer system .	5	feature-of	feature-of	1
we go , on to describe flexp , a bottom-up pattern-matching parser that we have designed and implemented to provide these #flexibilities# for restricted natural language input to a $limited-domain computer system$ .	4	part-of	part-of	1
we go , on to describe flexp , a $bottom-up pattern-matching parser$ that we have designed and implemented to provide these flexibilities for #restricted natural language# input to a limited-domain computer system .	0	used-for	used-for	1
traditional $information retrieval techniques$ use a #histogram of keywords# as the document representation but oral communication may offer additional indices such as the time and place of the rejoinder and the attendance .	0	used-for	used-for	1
traditional information retrieval techniques use a #histogram of keywords# as the $document representation$ but oral communication may offer additional indices such as the time and place of the rejoinder and the attendance .	0	used-for	used-for	1
an alternative index could be the $activity$ such as #discussing# , planning , informing , story-telling , etc. .	3	hyponym-of	hyponym-of	1
an alternative index could be the activity such as #discussing# , $planning$ , informing , story-telling , etc. .	1	conjunction	conjunction	1
an alternative index could be the $activity$ such as discussing , #planning# , informing , story-telling , etc. .	3	hyponym-of	hyponym-of	1
an alternative index could be the activity such as discussing , #planning# , $informing$ , story-telling , etc. .	1	conjunction	conjunction	1
an alternative index could be the $activity$ such as discussing , planning , #informing# , story-telling , etc. .	3	hyponym-of	hyponym-of	1
an alternative index could be the activity such as discussing , planning , #informing# , $story-telling$ , etc. .	1	conjunction	conjunction	1
an alternative index could be the $activity$ such as discussing , planning , informing , #story-telling# , etc. .	3	hyponym-of	hyponym-of	1
this paper addresses the problem of the $automatic detection$ of those #activities# in meeting situation and everyday rejoinders .	0	used-for	used-for	1
the format of the $corpus$ adopts the #child language data exchange system -lrb- childes -rrb-# .	5	feature-of	feature-of	1
in this paper , we describe #data collection# , $transcription$ , word segmentation , and part-of-speech annotation of this corpus .	1	conjunction	conjunction	1
in this paper , we describe #data collection# , transcription , word segmentation , and part-of-speech annotation of this $corpus$ .	0	used-for	used-for	1
in this paper , we describe data collection , #transcription# , $word segmentation$ , and part-of-speech annotation of this corpus .	1	conjunction	conjunction	1
in this paper , we describe data collection , #transcription# , word segmentation , and part-of-speech annotation of this $corpus$ .	0	used-for	used-for	1
in this paper , we describe data collection , transcription , #word segmentation# , and $part-of-speech annotation$ of this corpus .	1	conjunction	conjunction	1
in this paper , we describe data collection , transcription , #word segmentation# , and part-of-speech annotation of this $corpus$ .	0	used-for	used-for	1
in this paper , we describe data collection , transcription , word segmentation , and #part-of-speech annotation# of this $corpus$ .	0	used-for	used-for	1
this paper shows how $dictionary word sense definitions$ can be analysed by applying a #hierarchy of phrasal patterns# .	0	used-for	used-for	1
an experimental $system$ embodying this #mechanism# has been implemented for processing definitions from the longman dictionary of contemporary english .	4	part-of	part-of	1
a property of this dictionary , exploited by the system , is that $it$ uses a #restricted vocabulary# in its word sense definitions .	0	used-for	used-for	1
a property of this dictionary , exploited by the system , is that it uses a #restricted vocabulary# in its $word sense definitions$ .	0	used-for	used-for	1
the structures generated by the experimental system are intended to be used for the $classification of new word senses$ in terms of the senses of words in the #restricted vocabulary# .	0	used-for	used-for	1
thus the work reported addresses two #robustness problems# faced by current experimental $natural language processing systems$ : coping with an incomplete lexicon and with incomplete knowledge of phrasal constructions .	5	feature-of	feature-of	1
thus the work reported addresses two $robustness problems$ faced by current experimental natural language processing systems : coping with an #incomplete lexicon# and with incomplete knowledge of phrasal constructions .	3	hyponym-of	hyponym-of	1
thus the work reported addresses two $robustness problems$ faced by current experimental natural language processing systems : coping with an incomplete lexicon and with #incomplete knowledge of phrasal constructions# .	3	hyponym-of	hyponym-of	1
this paper presents a $word segmentation system$ in france telecom r&d beijing , which uses a unified #approach# to word breaking and oov identification .	0	used-for	used-for	1
this paper presents a word segmentation system in france telecom r&d beijing , which uses a unified #approach# to $word breaking$ and oov identification .	0	used-for	used-for	1
this paper presents a word segmentation system in france telecom r&d beijing , which uses a unified #approach# to word breaking and $oov identification$ .	0	used-for	used-for	1
this paper presents a word segmentation system in france telecom r&d beijing , which uses a unified approach to #word breaking# and $oov identification$ .	1	conjunction	conjunction	1
the system participated in all the tracks of the $segmentation bakeoff$ -- #pk-open# , pk-closed , as-open , as-closed , hk-open , hk-closed , msr-open and msr - closed -- and achieved the state-of-the-art performance in msr-open , msr-close and pk-open tracks .	3	hyponym-of	hyponym-of	1
the system participated in all the tracks of the segmentation bakeoff -- #pk-open# , $pk-closed$ , as-open , as-closed , hk-open , hk-closed , msr-open and msr - closed -- and achieved the state-of-the-art performance in msr-open , msr-close and pk-open tracks .	1	conjunction	conjunction	1
the system participated in all the tracks of the $segmentation bakeoff$ -- pk-open , #pk-closed# , as-open , as-closed , hk-open , hk-closed , msr-open and msr - closed -- and achieved the state-of-the-art performance in msr-open , msr-close and pk-open tracks .	3	hyponym-of	hyponym-of	1
the system participated in all the tracks of the segmentation bakeoff -- pk-open , #pk-closed# , $as-open$ , as-closed , hk-open , hk-closed , msr-open and msr - closed -- and achieved the state-of-the-art performance in msr-open , msr-close and pk-open tracks .	1	conjunction	conjunction	1
the system participated in all the tracks of the $segmentation bakeoff$ -- pk-open , pk-closed , #as-open# , as-closed , hk-open , hk-closed , msr-open and msr - closed -- and achieved the state-of-the-art performance in msr-open , msr-close and pk-open tracks .	3	hyponym-of	hyponym-of	1
the system participated in all the tracks of the segmentation bakeoff -- pk-open , pk-closed , #as-open# , $as-closed$ , hk-open , hk-closed , msr-open and msr - closed -- and achieved the state-of-the-art performance in msr-open , msr-close and pk-open tracks .	1	conjunction	conjunction	1
the system participated in all the tracks of the $segmentation bakeoff$ -- pk-open , pk-closed , as-open , #as-closed# , hk-open , hk-closed , msr-open and msr - closed -- and achieved the state-of-the-art performance in msr-open , msr-close and pk-open tracks .	3	hyponym-of	hyponym-of	1
the system participated in all the tracks of the segmentation bakeoff -- pk-open , pk-closed , as-open , #as-closed# , $hk-open$ , hk-closed , msr-open and msr - closed -- and achieved the state-of-the-art performance in msr-open , msr-close and pk-open tracks .	1	conjunction	conjunction	1
the system participated in all the tracks of the $segmentation bakeoff$ -- pk-open , pk-closed , as-open , as-closed , #hk-open# , hk-closed , msr-open and msr - closed -- and achieved the state-of-the-art performance in msr-open , msr-close and pk-open tracks .	3	hyponym-of	hyponym-of	1
the system participated in all the tracks of the segmentation bakeoff -- pk-open , pk-closed , as-open , as-closed , #hk-open# , $hk-closed$ , msr-open and msr - closed -- and achieved the state-of-the-art performance in msr-open , msr-close and pk-open tracks .	1	conjunction	conjunction	1
the system participated in all the tracks of the $segmentation bakeoff$ -- pk-open , pk-closed , as-open , as-closed , hk-open , #hk-closed# , msr-open and msr - closed -- and achieved the state-of-the-art performance in msr-open , msr-close and pk-open tracks .	3	hyponym-of	hyponym-of	1
the system participated in all the tracks of the segmentation bakeoff -- pk-open , pk-closed , as-open , as-closed , hk-open , #hk-closed# , $msr-open$ and msr - closed -- and achieved the state-of-the-art performance in msr-open , msr-close and pk-open tracks .	1	conjunction	conjunction	1
the system participated in all the tracks of the $segmentation bakeoff$ -- pk-open , pk-closed , as-open , as-closed , hk-open , hk-closed , #msr-open# and msr - closed -- and achieved the state-of-the-art performance in msr-open , msr-close and pk-open tracks .	3	hyponym-of	hyponym-of	1
the system participated in all the tracks of the segmentation bakeoff -- pk-open , pk-closed , as-open , as-closed , hk-open , hk-closed , #msr-open# and $msr - closed$ -- and achieved the state-of-the-art performance in msr-open , msr-close and pk-open tracks .	1	conjunction	conjunction	1
the system participated in all the tracks of the $segmentation bakeoff$ -- pk-open , pk-closed , as-open , as-closed , hk-open , hk-closed , msr-open and #msr - closed# -- and achieved the state-of-the-art performance in msr-open , msr-close and pk-open tracks .	3	hyponym-of	hyponym-of	1
the system participated in all the tracks of the segmentation bakeoff -- pk-open , pk-closed , as-open , as-closed , hk-open , hk-closed , msr-open and msr - closed -- and achieved the state-of-the-art performance in #msr-open# , $msr-close$ and pk-open tracks .	1	conjunction	conjunction	1
the system participated in all the tracks of the segmentation bakeoff -- pk-open , pk-closed , as-open , as-closed , hk-open , hk-closed , msr-open and msr - closed -- and achieved the state-of-the-art performance in msr-open , #msr-close# and $pk-open$ tracks .	1	conjunction	conjunction	1
this paper describes a #method# of $interactively visualizing and directing the process of translating$ a sentence .	0	used-for	used-for	1
the #method# allows a user to explore a $model$ of syntax-based statistical machine translation -lrb- mt -rrb- , to understand the model 's strengths and weaknesses , and to compare it to other mt systems .	0	used-for	used-for	1
the method allows a user to explore a #model# of $syntax-based statistical machine translation -lrb- mt -rrb-$ , to understand the model 's strengths and weaknesses , and to compare it to other mt systems .	0	used-for	used-for	1
the method allows a user to explore a model of syntax-based statistical machine translation -lrb- mt -rrb- , to understand the model 's strengths and weaknesses , and to compare #it# to other $mt systems$ .	6	compare	compare	1
using this #visualization method# , we can find and address conceptual and practical problems in an $mt system$ .	0	used-for	used-for	1
a #method# of $sense resolution$ is proposed that is based on wordnet , an on-line lexical database that incorporates semantic relations -lrb- synonymy , antonymy , hyponymy , meronymy , causal and troponymic entailment -rrb- as labeled pointers between word senses .	0	used-for	used-for	1
a $method$ of sense resolution is proposed that is based on #wordnet# , an on-line lexical database that incorporates semantic relations -lrb- synonymy , antonymy , hyponymy , meronymy , causal and troponymic entailment -rrb- as labeled pointers between word senses .	0	used-for	used-for	1
a method of sense resolution is proposed that is based on #wordnet# , an $on-line lexical database$ that incorporates semantic relations -lrb- synonymy , antonymy , hyponymy , meronymy , causal and troponymic entailment -rrb- as labeled pointers between word senses .	3	hyponym-of	hyponym-of	1
a method of sense resolution is proposed that is based on $wordnet$ , an on-line lexical database that incorporates #semantic relations# -lrb- synonymy , antonymy , hyponymy , meronymy , causal and troponymic entailment -rrb- as labeled pointers between word senses .	4	part-of	part-of	1
a method of sense resolution is proposed that is based on wordnet , an on-line lexical database that incorporates $semantic relations$ -lrb- #synonymy# , antonymy , hyponymy , meronymy , causal and troponymic entailment -rrb- as labeled pointers between word senses .	3	hyponym-of	hyponym-of	1
a method of sense resolution is proposed that is based on wordnet , an on-line lexical database that incorporates semantic relations -lrb- #synonymy# , $antonymy$ , hyponymy , meronymy , causal and troponymic entailment -rrb- as labeled pointers between word senses .	1	conjunction	conjunction	1
a method of sense resolution is proposed that is based on wordnet , an on-line lexical database that incorporates $semantic relations$ -lrb- synonymy , #antonymy# , hyponymy , meronymy , causal and troponymic entailment -rrb- as labeled pointers between word senses .	3	hyponym-of	hyponym-of	1
a method of sense resolution is proposed that is based on wordnet , an on-line lexical database that incorporates semantic relations -lrb- synonymy , #antonymy# , $hyponymy$ , meronymy , causal and troponymic entailment -rrb- as labeled pointers between word senses .	1	conjunction	conjunction	1
a method of sense resolution is proposed that is based on wordnet , an on-line lexical database that incorporates $semantic relations$ -lrb- synonymy , antonymy , #hyponymy# , meronymy , causal and troponymic entailment -rrb- as labeled pointers between word senses .	3	hyponym-of	hyponym-of	1
a method of sense resolution is proposed that is based on wordnet , an on-line lexical database that incorporates semantic relations -lrb- synonymy , antonymy , #hyponymy# , $meronymy$ , causal and troponymic entailment -rrb- as labeled pointers between word senses .	1	conjunction	conjunction	1
a method of sense resolution is proposed that is based on wordnet , an on-line lexical database that incorporates $semantic relations$ -lrb- synonymy , antonymy , hyponymy , #meronymy# , causal and troponymic entailment -rrb- as labeled pointers between word senses .	3	hyponym-of	hyponym-of	1
a method of sense resolution is proposed that is based on wordnet , an on-line lexical database that incorporates semantic relations -lrb- synonymy , antonymy , hyponymy , #meronymy# , $causal and troponymic entailment$ -rrb- as labeled pointers between word senses .	1	conjunction	conjunction	1
a method of sense resolution is proposed that is based on wordnet , an on-line lexical database that incorporates $semantic relations$ -lrb- synonymy , antonymy , hyponymy , meronymy , #causal and troponymic entailment# -rrb- as labeled pointers between word senses .	3	hyponym-of	hyponym-of	1
with #wordnet# , it is easy to retrieve sets of $semantically related words$ , a facility that will be used for sense resolution during text processing , as follows .	0	used-for	used-for	1
with wordnet , it is easy to retrieve sets of #semantically related words# , a facility that will be used for $sense resolution$ during text processing , as follows .	0	used-for	used-for	1
with wordnet , it is easy to retrieve sets of semantically related words , a facility that will be used for #sense resolution# during $text processing$ , as follows .	0	used-for	used-for	1
or , -lrb- 2 -rrb- the context of the polysemous word will be used as a key to search a large corpus ; all words found to occur in that context will be noted ; #wordnet# will then be used to estimate the $semantic distance$ from those words to the alternative senses of the polysemous word ; and that sense will be chosen that is closest in meaning to other words occurring in the same context if successful , this procedure could have practical applications to problems of information retrieval , mechanical translation , intelligent tutoring systems , and elsewhere .	0	used-for	used-for	1
or , -lrb- 2 -rrb- the context of the polysemous word will be used as a key to search a large corpus ; all words found to occur in that context will be noted ; wordnet will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word ; and that sense will be chosen that is closest in meaning to other words occurring in the same context if successful , this #procedure# could have practical applications to problems of $information retrieval$ , mechanical translation , intelligent tutoring systems , and elsewhere .	0	used-for	used-for	1
or , -lrb- 2 -rrb- the context of the polysemous word will be used as a key to search a large corpus ; all words found to occur in that context will be noted ; wordnet will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word ; and that sense will be chosen that is closest in meaning to other words occurring in the same context if successful , this #procedure# could have practical applications to problems of information retrieval , $mechanical translation$ , intelligent tutoring systems , and elsewhere .	0	used-for	used-for	1
or , -lrb- 2 -rrb- the context of the polysemous word will be used as a key to search a large corpus ; all words found to occur in that context will be noted ; wordnet will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word ; and that sense will be chosen that is closest in meaning to other words occurring in the same context if successful , this #procedure# could have practical applications to problems of information retrieval , mechanical translation , $intelligent tutoring systems$ , and elsewhere .	0	used-for	used-for	1
or , -lrb- 2 -rrb- the context of the polysemous word will be used as a key to search a large corpus ; all words found to occur in that context will be noted ; wordnet will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word ; and that sense will be chosen that is closest in meaning to other words occurring in the same context if successful , this procedure could have practical applications to problems of #information retrieval# , $mechanical translation$ , intelligent tutoring systems , and elsewhere .	1	conjunction	conjunction	1
or , -lrb- 2 -rrb- the context of the polysemous word will be used as a key to search a large corpus ; all words found to occur in that context will be noted ; wordnet will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word ; and that sense will be chosen that is closest in meaning to other words occurring in the same context if successful , this procedure could have practical applications to problems of information retrieval , #mechanical translation# , $intelligent tutoring systems$ , and elsewhere .	1	conjunction	conjunction	1
the #interlingual approach# to $mt$ has been repeatedly advocated by researchers originally interested in natural language understanding who take machine translation to be one possible application .	0	used-for	used-for	1
in contrast , our project , the #mu-project# , adopts the transfer approach as the basic framework of $mt$ .	0	used-for	used-for	1
in contrast , our project , the $mu-project$ , adopts the #transfer approach# as the basic framework of mt .	0	used-for	used-for	1
this paper describes the detailed construction of the #transfer phase# of our $system$ from japanese to english , and gives some examples of problems which seem difficult to treat in the interlingual approach .	4	part-of	part-of	1
the basic design principles of the #transfer phase# of our $system$ have already been mentioned in -lrb- 1 -rrb- -lrb- 2 -rrb- .	4	part-of	part-of	1
some of the $principles$ which are relevant to the topic of this paper are : -lrb- a -rrb- #multiple layer of grammars# -lrb- b -rrb- multiple layer presentation -lrb- c -rrb- lexicon driven processing -lrb- d -rrb- form-oriented dictionary description .	4	part-of	part-of	1
some of the principles which are relevant to the topic of this paper are : -lrb- a -rrb- #multiple layer of grammars# -lrb- b -rrb- $multiple layer presentation$ -lrb- c -rrb- lexicon driven processing -lrb- d -rrb- form-oriented dictionary description .	1	conjunction	conjunction	1
some of the $principles$ which are relevant to the topic of this paper are : -lrb- a -rrb- multiple layer of grammars -lrb- b -rrb- #multiple layer presentation# -lrb- c -rrb- lexicon driven processing -lrb- d -rrb- form-oriented dictionary description .	4	part-of	part-of	1
some of the principles which are relevant to the topic of this paper are : -lrb- a -rrb- multiple layer of grammars -lrb- b -rrb- #multiple layer presentation# -lrb- c -rrb- $lexicon driven processing$ -lrb- d -rrb- form-oriented dictionary description .	1	conjunction	conjunction	1
some of the $principles$ which are relevant to the topic of this paper are : -lrb- a -rrb- multiple layer of grammars -lrb- b -rrb- multiple layer presentation -lrb- c -rrb- #lexicon driven processing# -lrb- d -rrb- form-oriented dictionary description .	4	part-of	part-of	1
some of the principles which are relevant to the topic of this paper are : -lrb- a -rrb- multiple layer of grammars -lrb- b -rrb- multiple layer presentation -lrb- c -rrb- #lexicon driven processing# -lrb- d -rrb- $form-oriented dictionary description$ .	1	conjunction	conjunction	1
some of the $principles$ which are relevant to the topic of this paper are : -lrb- a -rrb- multiple layer of grammars -lrb- b -rrb- multiple layer presentation -lrb- c -rrb- lexicon driven processing -lrb- d -rrb- #form-oriented dictionary description# .	4	part-of	part-of	1
this paper also shows how these #principles# are realized in the current $system$ .	4	part-of	part-of	1
in this paper discourse segments are defined and a #method# for $discourse segmentation$ primarily based on abduction of temporal relations between segments is proposed .	0	used-for	used-for	1
in this paper discourse segments are defined and a method for $discourse segmentation$ primarily based on #abduction of temporal relations# between segments is proposed .	0	used-for	used-for	1
this $method$ is precise and computationally feasible and is supported by previous work in the area of #temporal anaphora resolution# .	0	used-for	used-for	1
this paper describes to what extent $deep processing$ may benefit from #shallow techniques# and it presents a nlp system which integrates a linguistic pos tagger and chunker as a preprocessing module of a broad coverage unification based grammar of spanish .	0	used-for	used-for	1
this paper describes to what extent deep processing may benefit from shallow techniques and it presents a nlp system which integrates a #linguistic pos tagger and chunker# as a preprocessing module of a $broad coverage unification based grammar of spanish$ .	4	part-of	part-of	1
this paper describes to what extent deep processing may benefit from shallow techniques and it presents a $nlp system$ which integrates a linguistic pos tagger and chunker as a preprocessing module of a #broad coverage unification based grammar of spanish# .	0	used-for	used-for	1
experiments show that the efficiency of the overall analysis improves significantly and that our #system# also provides robustness to the $linguistic processing$ while maintaining both the accuracy and the precision of the grammar .	0	used-for	used-for	1
experiments show that the efficiency of the overall analysis improves significantly and that our $system$ also provides #robustness# to the linguistic processing while maintaining both the accuracy and the precision of the grammar .	2	evaluate-for	evaluate-for	1
experiments show that the efficiency of the overall analysis improves significantly and that our $system$ also provides robustness to the linguistic processing while maintaining both the #accuracy# and the precision of the grammar .	2	evaluate-for	evaluate-for	1
experiments show that the efficiency of the overall analysis improves significantly and that our system also provides robustness to the linguistic processing while maintaining both the #accuracy# and the $precision$ of the grammar .	1	conjunction	conjunction	1
experiments show that the efficiency of the overall analysis improves significantly and that our $system$ also provides robustness to the linguistic processing while maintaining both the accuracy and the #precision# of the grammar .	2	evaluate-for	evaluate-for	1
#joint image filters# can leverage the guidance image as a prior and transfer the structural details from the guidance image to the target image for $suppressing noise$ or enhancing spatial resolution .	0	used-for	used-for	1
#joint image filters# can leverage the guidance image as a prior and transfer the structural details from the guidance image to the target image for suppressing noise or $enhancing spatial resolution$ .	0	used-for	used-for	1
$joint image filters$ can leverage the #guidance image# as a prior and transfer the structural details from the guidance image to the target image for suppressing noise or enhancing spatial resolution .	0	used-for	used-for	1
joint image filters can leverage the guidance image as a prior and transfer the structural details from the guidance image to the target image for #suppressing noise# or $enhancing spatial resolution$ .	1	conjunction	conjunction	1
$existing methods$ rely on various kinds of #explicit filter construction# or hand-designed objective functions .	0	used-for	used-for	1
existing methods rely on various kinds of #explicit filter construction# or $hand-designed objective functions$ .	1	conjunction	conjunction	1
$existing methods$ rely on various kinds of explicit filter construction or #hand-designed objective functions# .	0	used-for	used-for	1
it is thus difficult to understand , improve , and accelerate $them$ in a #coherent framework# .	0	used-for	used-for	1
in this paper , we propose a #learning-based approach# to construct a $joint filter$ based on convolution-al neural networks .	0	used-for	used-for	1
in this paper , we propose a $learning-based approach$ to construct a joint filter based on #convolution-al neural networks# .	0	used-for	used-for	1
in contrast to existing #methods# that consider only the guidance image , our $method$ can selectively transfer salient structures that are consistent in both guidance and target images .	6	compare	compare	1
in contrast to existing $methods$ that consider only the #guidance image# , our method can selectively transfer salient structures that are consistent in both guidance and target images .	0	used-for	used-for	1
in contrast to existing methods that consider only the guidance image , our #method# can selectively $transfer salient structures$ that are consistent in both guidance and target images .	0	used-for	used-for	1
we show that the #model# trained on a certain type of data , e.g. , rgb and depth images , generalizes well for other $modalities$ , e.g. , flash/non-flash and rgb/nir images .	0	used-for	used-for	1
we show that the $model$ trained on a certain type of #data# , e.g. , rgb and depth images , generalizes well for other modalities , e.g. , flash/non-flash and rgb/nir images .	0	used-for	used-for	1
we show that the model trained on a certain type of $data$ , e.g. , #rgb and depth images# , generalizes well for other modalities , e.g. , flash/non-flash and rgb/nir images .	3	hyponym-of	hyponym-of	1
we show that the model trained on a certain type of data , e.g. , rgb and depth images , generalizes well for other $modalities$ , e.g. , #flash/non-flash and rgb/nir images# .	3	hyponym-of	hyponym-of	1
we validate the effectiveness of the proposed #joint filter# through extensive comparisons with $state-of-the-art methods$ .	6	compare	compare	1
in our current research into the design of $cognitively well-motivated interfaces$ relying primarily on the #display of graphical information# , we have observed that graphical information alone does not provide sufficient support to users - particularly when situations arise that do not simply conform to the users ' expectations .	0	used-for	used-for	1
to solve this problem , we are working towards the integration of #natural language generation# to augment the $interaction$	0	used-for	used-for	1
a central problem of word sense disambiguation -lrb- wsd -rrb- is the lack of #manually sense-tagged data# required for $supervised learning$ .	0	used-for	used-for	1
in this paper , we evaluate an #approach# to automatically acquire $sense-tagged training data$ from english-chinese parallel corpora , which are then used for disambiguating the nouns in the senseval-2 english lexical sample task .	0	used-for	used-for	1
in this paper , we evaluate an #approach# to automatically acquire sense-tagged training data from english-chinese parallel corpora , which are then used for disambiguating the $nouns$ in the senseval-2 english lexical sample task .	0	used-for	used-for	1
in this paper , we evaluate an approach to automatically acquire $sense-tagged training data$ from #english-chinese parallel corpora# , which are then used for disambiguating the nouns in the senseval-2 english lexical sample task .	0	used-for	used-for	1
in this paper , we evaluate an approach to automatically acquire sense-tagged training data from english-chinese parallel corpora , which are then used for disambiguating the #nouns# in the $senseval-2 english lexical sample task$ .	4	part-of	part-of	1
our investigation reveals that this #method# of $acquiring sense-tagged data$ is promising .	0	used-for	used-for	1
on a subset of the most difficult senseval-2 nouns , the accuracy difference between the two approaches is only 14.0 % , and the difference could narrow further to 6.5 % if we disregard the advantage that $manually sense-tagged data$ have in their #sense coverage# .	5	feature-of	feature-of	1
our analysis also highlights the importance of the issue of #domain dependence# in $evaluating wsd programs$ .	5	feature-of	feature-of	1
this paper presents an analysis of $temporal anaphora$ in sentences which contain #quantification over events# , within the framework of discourse representation theory .	4	part-of	part-of	1
this paper presents an analysis of $temporal anaphora$ in sentences which contain quantification over events , within the framework of #discourse representation theory# .	0	used-for	used-for	1
the analysis in -lrb- partee , 1984 -rrb- of quantified sentences , introduced by a temporal connective , gives the wrong truth-conditions when the #temporal connective# in the $subordinate clause$ is before or after .	4	part-of	part-of	1
this $problem$ has been previously analyzed in -lrb- de swart , 1991 -rrb- as an instance of the proportion problem and given a solution from a #generalized quantifier approach# .	0	used-for	used-for	1
by using a careful distinction between the different notions of reference time based on -lrb- kamp and reyle , 1993 -rrb- , we propose a #solution# to this $problem$ , within the framework of drt .	0	used-for	used-for	1
by using a careful distinction between the different notions of reference time based on -lrb- kamp and reyle , 1993 -rrb- , we propose a solution to this $problem$ , within the framework of #drt# .	0	used-for	used-for	1
we show some applications of this #solution# to additional $temporal anaphora phenomena$ in quantified sentences .	0	used-for	used-for	1
we show some applications of this $solution$ to additional temporal anaphora phenomena in #quantified sentences# .	0	used-for	used-for	1
in this paper , we explore #correlation of dependency relation paths# to rank candidate answers in $answer extraction$ .	0	used-for	used-for	1
using the #correlation measure# , we compare $dependency relations$ of a candidate answer and mapped question phrases in sentence with the corresponding relations in question .	0	used-for	used-for	1
different from previous studies , we propose an approximate phrase mapping algorithm and incorporate the #mapping score# into the $correlation measure$ .	4	part-of	part-of	1
the #correlations# are further incorporated into a $maximum entropy-based ranking model$ which estimates path weights from training .	4	part-of	part-of	1
experimental results show that our #method# significantly outperforms state-of-the-art $syntactic relation-based methods$ by up to 20 % in mrr .	6	compare	compare	1
experimental results show that our $method$ significantly outperforms state-of-the-art syntactic relation-based methods by up to 20 % in #mrr# .	2	evaluate-for	evaluate-for	1
experimental results show that our method significantly outperforms state-of-the-art $syntactic relation-based methods$ by up to 20 % in #mrr# .	2	evaluate-for	evaluate-for	1
#evaluation# is also crucial to assessing competing claims and identifying promising technical $approaches$ .	0	used-for	used-for	1
recently considerable progress has been made by a number of groups involved in the darpa spoken language systems -lrb- sls -rrb- program to agree on a #methodology# for comparative $evaluation of sls systems$ , and that methodology has been put into practice several times in comparative tests of several sls systems .	2	evaluate-for	evaluate-for	1
recently considerable progress has been made by a number of groups involved in the darpa spoken language systems -lrb- sls -rrb- program to agree on a methodology for comparative evaluation of sls systems , and that #methodology# has been put into practice several times in comparative tests of several $sls systems$ .	2	evaluate-for	evaluate-for	1
these #evaluations# are probably the only $nl evaluations$ other than the series of message understanding conferences -lrb- sundheim , 1989 ; sundheim , 1991 -rrb- to have been developed and used by a group of researchers at different sites , although several excellent workshops have been held to study some of these problems -lrb- palmer et al. , 1989 ; neal et al. , 1991 -rrb- .	3	hyponym-of	hyponym-of	1
these #evaluations# are probably the only nl evaluations other than the series of $message understanding conferences$ -lrb- sundheim , 1989 ; sundheim , 1991 -rrb- to have been developed and used by a group of researchers at different sites , although several excellent workshops have been held to study some of these problems -lrb- palmer et al. , 1989 ; neal et al. , 1991 -rrb- .	1	conjunction	conjunction	1
this paper describes a practical #`` black-box '' methodology# for $automatic evaluation of question-answering nl systems$ .	2	evaluate-for	evaluate-for	1
while each new application domain will require some development of special resources , the heart of the methodology is domain-independent , and $it$ can be used with either #speech or text input# .	0	used-for	used-for	1
in this paper we present a novel #autonomous pipeline# to build a $personalized parametric model -lrb- pose-driven avatar -rrb-$ using a single depth sensor .	0	used-for	used-for	1
in this paper we present a novel $autonomous pipeline$ to build a personalized parametric model -lrb- pose-driven avatar -rrb- using a #single depth sensor# .	0	used-for	used-for	1
we fit each incomplete scan using $template fitting techniques$ with a generic #human template# , and register all scans to every pose using global consistency constraints .	0	used-for	used-for	1
after registration , these #watertight models# with different poses are used to train a $parametric model$ in a fashion similar to the scape method .	0	used-for	used-for	1
after registration , these watertight models with different poses are used to train a $parametric model$ in a fashion similar to the #scape method# .	0	used-for	used-for	1
once the parametric model is built , #it# can be used as an $anim-itable avatar$ or more interestingly synthesizing dynamic 3d models from single-view depth videos .	0	used-for	used-for	1
once the parametric model is built , #it# can be used as an anim-itable avatar or more interestingly synthesizing $dynamic 3d models$ from single-view depth videos .	0	used-for	used-for	1
once the parametric model is built , it can be used as an anim-itable avatar or more interestingly synthesizing $dynamic 3d models$ from #single-view depth videos# .	0	used-for	used-for	1
experimental results demonstrate the effectiveness of our #system# to produce $dynamic models$ .	0	used-for	used-for	1
in this paper , we propose a novel #algorithm# to detect/compensate $on-line interference effects$ when integrating global navigation satellite system -lrb- gnss -rrb- and inertial navigation system -lrb- ins -rrb- .	0	used-for	used-for	1
in this paper , we propose a novel algorithm to detect/compensate on-line interference effects when integrating #global navigation satellite system -lrb- gnss -rrb-# and $inertial navigation system -lrb- ins -rrb-$ .	1	conjunction	conjunction	1
the $gnss/ins coupling$ is usually performed by an #extended kalman filter -lrb- ekf -rrb-# which yields an accurate and robust localization .	0	used-for	used-for	1
the gnss/ins coupling is usually performed by an #extended kalman filter -lrb- ekf -rrb-# which yields an $accurate and robust localization$ .	0	used-for	used-for	1
we first study the impact of the gnss noise inflation on the $covariance$ of the #ekf outputs# so as to compute a least square estimate of the potential variance jumps .	5	feature-of	feature-of	1
we first study the impact of the gnss noise inflation on the covariance of the ekf outputs so as to compute a #least square estimate# of the potential $variance jumps$ .	0	used-for	used-for	1
then , this #estimation# is used in a $bayesian test$ which decides whether interference are corrupting the gnss signal or not .	0	used-for	used-for	1
the results show the performance of the proposed $approach$ on #simulated data# .	2	evaluate-for	evaluate-for	1
we propose a #unified variational formulation# for $joint motion estimation and segmentation$ with explicit occlusion handling .	0	used-for	used-for	1
we propose a $unified variational formulation$ for joint motion estimation and segmentation with #explicit occlusion handling# .	0	used-for	used-for	1
we use a #convex formulation# of the $multi-label potts model$ with label costs and show that the asymmetric map-uniqueness criterion can be integrated into our formulation by means of convex constraints .	0	used-for	used-for	1
we use a convex formulation of the multi-label potts model with label costs and show that the #asymmetric map-uniqueness criterion# can be integrated into our $formulation$ by means of convex constraints .	4	part-of	part-of	1
we use a convex formulation of the multi-label potts model with label costs and show that the asymmetric map-uniqueness criterion can be integrated into our $formulation$ by means of #convex constraints# .	0	used-for	used-for	1
by using a fast #primal-dual algorithm# we are able to handle several hundred $motion segments$ .	0	used-for	used-for	1
two main classes of #approaches# have been studied to perform $monocular nonrigid 3d reconstruction$ : template-based methods and non-rigid structure from motion techniques .	0	used-for	used-for	1
two main classes of $approaches$ have been studied to perform monocular nonrigid 3d reconstruction : #template-based methods# and non-rigid structure from motion techniques .	3	hyponym-of	hyponym-of	1
two main classes of approaches have been studied to perform monocular nonrigid 3d reconstruction : #template-based methods# and $non-rigid structure from motion techniques$ .	1	conjunction	conjunction	1
two main classes of $approaches$ have been studied to perform monocular nonrigid 3d reconstruction : template-based methods and #non-rigid structure from motion techniques# .	3	hyponym-of	hyponym-of	1
while the first #ones# have been applied to reconstruct $poorly-textured surfaces$ , they assume the availability of a 3d shape model prior to reconstruction .	0	used-for	used-for	1
while the first ones have been applied to reconstruct poorly-textured surfaces , $they$ assume the availability of a #3d shape model# prior to reconstruction .	0	used-for	used-for	1
in this paper , we introduce a #template-free approach# to reconstructing a $poorly-textured , deformable surface$ .	0	used-for	used-for	1
to this end , we leverage #surface isometry# and formulate $3d reconstruction$ as the joint problem of non-rigid image registration and depth estimation .	0	used-for	used-for	1
to this end , we leverage surface isometry and formulate $3d reconstruction$ as the #joint problem of non-rigid image registration and depth estimation# .	0	used-for	used-for	1
our experiments demonstrate that our #approach# yields much more accurate 3d reconstructions than $state-of-the-art techniques$ .	6	compare	compare	1
our experiments demonstrate that our $approach$ yields much more accurate #3d reconstructions# than state-of-the-art techniques .	2	evaluate-for	evaluate-for	1
our experiments demonstrate that our approach yields much more accurate #3d reconstructions# than $state-of-the-art techniques$ .	2	evaluate-for	evaluate-for	1
many $computer vision applications$ , such as #image classification# and video indexing , are usually multi-label classification problems in which an instance can be assigned to more than one category .	3	hyponym-of	hyponym-of	1
many computer vision applications , such as #image classification# and $video indexing$ , are usually multi-label classification problems in which an instance can be assigned to more than one category .	1	conjunction	conjunction	1
many $computer vision applications$ , such as image classification and #video indexing# , are usually multi-label classification problems in which an instance can be assigned to more than one category .	3	hyponym-of	hyponym-of	1
many $computer vision applications$ , such as image classification and video indexing , are usually #multi-label classification problems# in which an instance can be assigned to more than one category .	0	used-for	used-for	1
in this paper , we present a novel $multi-label classification approach$ with #hypergraph regu-larization# that addresses the correlations among different categories .	5	feature-of	feature-of	1
then , an improved #svm like learning system# incorporating the hypergraph regularization , called rank-hlapsvm , is proposed to handle the $multi-label classification problems$ .	0	used-for	used-for	1
then , an improved $svm like learning system$ incorporating the #hypergraph regularization# , called rank-hlapsvm , is proposed to handle the multi-label classification problems .	4	part-of	part-of	1
then , an improved $svm like learning system$ incorporating the hypergraph regularization , called #rank-hlapsvm# , is proposed to handle the multi-label classification problems .	3	hyponym-of	hyponym-of	1
we find that the corresponding $optimization problem$ can be efficiently solved by the #dual coordinate descent method# .	0	used-for	used-for	1
many promising experimental results on the #real datasets# including imageclef and me-diamill demonstrate the effectiveness and efficiency of the proposed $algorithm$ .	2	evaluate-for	evaluate-for	1
many promising experimental results on the $real datasets$ including #imageclef# and me-diamill demonstrate the effectiveness and efficiency of the proposed algorithm .	3	hyponym-of	hyponym-of	1
many promising experimental results on the real datasets including #imageclef# and $me-diamill$ demonstrate the effectiveness and efficiency of the proposed algorithm .	1	conjunction	conjunction	1
many promising experimental results on the $real datasets$ including imageclef and #me-diamill# demonstrate the effectiveness and efficiency of the proposed algorithm .	3	hyponym-of	hyponym-of	1
we derive a #convex optimization problem# for the task of $segmenting sequential data$ , which explicitly treats presence of outliers .	0	used-for	used-for	1
we derive a convex optimization problem for the task of #segmenting sequential data# , which explicitly treats presence of $outliers$ .	0	used-for	used-for	1
we describe two #algorithms# for solving this $problem$ , one exact and one a top-down novel approach , and we derive a consistency results for the case of two segments and no outliers .	0	used-for	used-for	1
$robustness$ to #outliers# is evaluated on two real-world tasks related to speech segmentation .	5	feature-of	feature-of	1
$robustness$ to outliers is evaluated on two #real-world tasks# related to speech segmentation .	2	evaluate-for	evaluate-for	1
$robustness$ to outliers is evaluated on two real-world tasks related to #speech segmentation# .	2	evaluate-for	evaluate-for	1
robustness to outliers is evaluated on two $real-world tasks$ related to #speech segmentation# .	5	feature-of	feature-of	1
our #algorithms# outperform $baseline seg-mentation algorithms$ .	6	compare	compare	1
this paper examines the properties of $feature-based partial descriptions$ built on top of #halliday 's systemic networks# .	0	used-for	used-for	1
we show that the crucial operation of #consistency checking# for such $descriptions$ is np-complete , and therefore probably intractable , but proceed to develop algorithms which can sometimes alleviate the unpleasant consequences of this intractability .	0	used-for	used-for	1
we describe #yoopick# , a $combinatorial sports prediction market$ that implements a flexible betting language , and in turn facilitates fine-grained probabilistic estimation of outcomes .	3	hyponym-of	hyponym-of	1
we describe #yoopick# , a combinatorial sports prediction market that implements a flexible betting language , and in turn facilitates $fine-grained probabilistic estimation of outcomes$ .	0	used-for	used-for	1
we describe $yoopick$ , a combinatorial sports prediction market that implements a #flexible betting language# , and in turn facilitates fine-grained probabilistic estimation of outcomes .	0	used-for	used-for	1
the goal of this paper is to discover a set of #discriminative patches# which can serve as a fully $unsupervised mid-level visual representation$ .	0	used-for	used-for	1
we pose this as an $unsupervised discriminative clustering problem$ on a huge dataset of #image patches# .	0	used-for	used-for	1
we use an iterative procedure which alternates between clustering and training discriminative classifiers , while applying careful #cross-validation# at each step to prevent $overfitting$ .	0	used-for	used-for	1
the paper experimentally demonstrates the effectiveness of #discriminative patches# as an $unsupervised mid-level visual representation$ , suggesting that it could be used in place of visual words for many tasks .	0	used-for	used-for	1
the paper experimentally demonstrates the effectiveness of $discriminative patches$ as an unsupervised mid-level visual representation , suggesting that #it# could be used in place of visual words for many tasks .	0	used-for	used-for	1
furthermore , #discrim-inative patches# can also be used in a $supervised regime$ , such as scene classification , where they demonstrate state-of-the-art performance on the mit indoor-67 dataset .	0	used-for	used-for	1
furthermore , discrim-inative patches can also be used in a $supervised regime$ , such as #scene classification# , where they demonstrate state-of-the-art performance on the mit indoor-67 dataset .	3	hyponym-of	hyponym-of	1
furthermore , discrim-inative patches can also be used in a supervised regime , such as scene classification , where $they$ demonstrate state-of-the-art performance on the #mit indoor-67 dataset# .	2	evaluate-for	evaluate-for	1
we investigate the utility of an #algorithm# for $translation lexicon acquisition -lrb- sable -rrb-$ , used previously on a very large corpus to acquire general translation lexicons , when that algorithm is applied to a much smaller corpus to produce candidates for domain-specific translation lexicons .	0	used-for	used-for	1
we investigate the utility of an #algorithm# for translation lexicon acquisition -lrb- sable -rrb- , used previously on a very large corpus to acquire $general translation lexicons$ , when that algorithm is applied to a much smaller corpus to produce candidates for domain-specific translation lexicons .	0	used-for	used-for	1
we investigate the utility of an algorithm for translation lexicon acquisition -lrb- sable -rrb- , used previously on a very large corpus to acquire general translation lexicons , when that #algorithm# is applied to a much smaller corpus to produce candidates for $domain-specific translation lexicons$ .	0	used-for	used-for	1
this paper describes a #computational model# of $word segmentation$ and presents simulation results on realistic acquisition .	0	used-for	used-for	1
this paper describes a #computational model# of word segmentation and presents simulation results on $realistic acquisition$ .	0	used-for	used-for	1
in particular , we explore the capacity and limitations of $statistical learning mechanisms$ that have recently gained prominence in #cognitive psychology# and linguistics .	0	used-for	used-for	1
in particular , we explore the capacity and limitations of statistical learning mechanisms that have recently gained prominence in #cognitive psychology# and $linguistics$ .	1	conjunction	conjunction	1
in particular , we explore the capacity and limitations of $statistical learning mechanisms$ that have recently gained prominence in cognitive psychology and #linguistics# .	0	used-for	used-for	1
in the #model-based policy search approach# to $reinforcement learning -lrb- rl -rrb-$ , policies are found using a model -lrb- or `` simulator '' -rrb- of the markov decision process .	0	used-for	used-for	1
in the model-based policy search approach to reinforcement learning -lrb- rl -rrb- , $policies$ are found using a model -lrb- or `` simulator '' -rrb- of the #markov decision process# .	0	used-for	used-for	1
however , for $high-dimensional continuous-state tasks$ , it can be extremely difficult to build an accurate #model# , and thus often the algorithm returns a policy that works in simulation but not in real-life .	0	used-for	used-for	1
however , for high-dimensional continuous-state tasks , it can be extremely difficult to build an accurate model , and thus often the #algorithm# returns a $policy$ that works in simulation but not in real-life .	0	used-for	used-for	1
the other extreme , $model-free rl$ , tends to require infeasibly large numbers of #real-life trials# .	0	used-for	used-for	1
in this paper , we present a $hybrid algorithm$ that requires only an #approximate model# , and only a small number of real-life trials .	0	used-for	used-for	1
in this paper , we present a hybrid algorithm that requires only an $approximate model$ , and only a small number of #real-life trials# .	0	used-for	used-for	1
the key idea is to successively `` ground '' the $policy evaluations$ using #real-life trials# , but to rely on the approximate model to suggest local changes .	0	used-for	used-for	1
empirical results also demonstrate that -- when given only a #crude model# and a small number of $real-life trials$ -- our algorithm can obtain near-optimal performance in the real system .	1	conjunction	conjunction	1
empirical results also demonstrate that -- when given only a #crude model# and a small number of real-life trials -- our $algorithm$ can obtain near-optimal performance in the real system .	0	used-for	used-for	1
empirical results also demonstrate that -- when given only a crude model and a small number of #real-life trials# -- our $algorithm$ can obtain near-optimal performance in the real system .	0	used-for	used-for	1
although every $natural language system$ needs a #computational lexicon# , each system puts different amounts and types of information into its lexicon according to its individual needs .	0	used-for	used-for	1
this paper presents our experience in planning and building #complex# , a $computational lexicon$ designed to be a repository of shared lexical information for use by natural language processing -lrb- nlp -rrb- systems .	3	hyponym-of	hyponym-of	1
this paper presents our experience in planning and building #complex# , a computational lexicon designed to be a repository of shared lexical information for use by $natural language processing -lrb- nlp -rrb- systems$ .	0	used-for	used-for	1
sentence planning is a set of inter-related but distinct $tasks$ , one of which is #sentence scoping# , i.e. the choice of syntactic structure for elementary speech acts and the decision of how to combine them into one or more sentences .	4	part-of	part-of	1
sentence planning is a set of inter-related but distinct tasks , one of which is sentence scoping , i.e. the choice of #syntactic structure# for elementary $speech acts$ and the decision of how to combine them into one or more sentences .	0	used-for	used-for	1
in this paper , we present #spot# , a $sentence planner$ , and a new methodology for automatically training spot on the basis of feedback provided by human judges .	3	hyponym-of	hyponym-of	1
in this paper , we present spot , a sentence planner , and a new #methodology# for automatically training $spot$ on the basis of feedback provided by human judges .	0	used-for	used-for	1
first , a very simple , #randomized sentence-plan-generator -lrb- spg -rrb-# generates a potentially large list of possible $sentence plans$ for a given text-plan input .	0	used-for	used-for	1
first , a very simple , $randomized sentence-plan-generator -lrb- spg -rrb-$ generates a potentially large list of possible sentence plans for a given #text-plan input# .	0	used-for	used-for	1
second , the #sentence-plan-ranker -lrb- spr -rrb-# ranks the list of output $sentence plans$ , and then selects the top-ranked plan .	0	used-for	used-for	1
the $spr$ uses #ranking rules# automatically learned from training data .	0	used-for	used-for	1
we show that the trained #spr# learns to select a $sentence plan$ whose rating on average is only 5 % worse than the top human-ranked sentence plan .	0	used-for	used-for	1
we show that the trained spr learns to select a #sentence plan# whose rating on average is only 5 % worse than the $top human-ranked sentence plan$ .	6	compare	compare	1
we discuss #maximum a posteriori estimation# of $continuous density hidden markov models -lrb- cdhmm -rrb-$ .	0	used-for	used-for	1
the classical $mle reestimation algorithms$ , namely the #forward-backward algorithm# and the segmental k-means algorithm , are expanded and reestimation formulas are given for hmm with gaussian mixture observation densities .	3	hyponym-of	hyponym-of	1
the classical $mle reestimation algorithms$ , namely the forward-backward algorithm and the #segmental k-means algorithm# , are expanded and reestimation formulas are given for hmm with gaussian mixture observation densities .	3	hyponym-of	hyponym-of	1
the classical mle reestimation algorithms , namely the $forward-backward algorithm$ and the #segmental k-means algorithm# , are expanded and reestimation formulas are given for hmm with gaussian mixture observation densities .	1	conjunction	conjunction	1
the classical mle reestimation algorithms , namely the forward-backward algorithm and the segmental k-means algorithm , are expanded and #reestimation formulas# are given for $hmm with gaussian mixture observation densities$ .	0	used-for	used-for	1
because of its adaptive nature , #bayesian learning# serves as a unified approach for the following four $speech recognition applications$ , namely parameter smoothing , speaker adaptation , speaker group modeling and corrective training .	0	used-for	used-for	1
because of its adaptive nature , bayesian learning serves as a unified approach for the following four $speech recognition applications$ , namely #parameter smoothing# , speaker adaptation , speaker group modeling and corrective training .	3	hyponym-of	hyponym-of	1
because of its adaptive nature , bayesian learning serves as a unified approach for the following four speech recognition applications , namely #parameter smoothing# , $speaker adaptation$ , speaker group modeling and corrective training .	1	conjunction	conjunction	1
because of its adaptive nature , bayesian learning serves as a unified approach for the following four $speech recognition applications$ , namely parameter smoothing , #speaker adaptation# , speaker group modeling and corrective training .	3	hyponym-of	hyponym-of	1
because of its adaptive nature , bayesian learning serves as a unified approach for the following four speech recognition applications , namely parameter smoothing , #speaker adaptation# , $speaker group modeling$ and corrective training .	1	conjunction	conjunction	1
because of its adaptive nature , bayesian learning serves as a unified approach for the following four $speech recognition applications$ , namely parameter smoothing , speaker adaptation , #speaker group modeling# and corrective training .	3	hyponym-of	hyponym-of	1
because of its adaptive nature , bayesian learning serves as a unified approach for the following four speech recognition applications , namely parameter smoothing , speaker adaptation , #speaker group modeling# and $corrective training$ .	1	conjunction	conjunction	1
because of its adaptive nature , bayesian learning serves as a unified approach for the following four $speech recognition applications$ , namely parameter smoothing , speaker adaptation , speaker group modeling and #corrective training# .	3	hyponym-of	hyponym-of	1
new experimental results on all four #applications# are provided to show the effectiveness of the $map estimation approach$ .	2	evaluate-for	evaluate-for	1
this paper describes a characters-based chinese collocation system and discusses the advantages of #it# over a traditional $word-based system$ .	6	compare	compare	1
since wordbreaks are not conventionally marked in chinese text corpora , a $character-based collocation system$ has the dual advantages of #avoiding pre-processing distortion# and directly accessing sub-lexical information .	5	feature-of	feature-of	1
since wordbreaks are not conventionally marked in chinese text corpora , a character-based collocation system has the dual advantages of #avoiding pre-processing distortion# and directly $accessing sub-lexical information$ .	1	conjunction	conjunction	1
since wordbreaks are not conventionally marked in chinese text corpora , a $character-based collocation system$ has the dual advantages of avoiding pre-processing distortion and directly #accessing sub-lexical information# .	5	feature-of	feature-of	1
furthermore , $word-based collocational properties$ can be obtained through an #auxiliary module of automatic segmentation# .	0	used-for	used-for	1
this paper describes a #method# for $utterance classification$ that does not require manual transcription of training data .	0	used-for	used-for	1
the #method# combines domain independent acoustic models with off-the-shelf classifiers to give $utterance classification$ performance that is surprisingly close to what can be achieved using conventional word-trigram recognition requiring manual transcription .	0	used-for	used-for	1
the $method$ combines #domain independent acoustic models# with off-the-shelf classifiers to give utterance classification performance that is surprisingly close to what can be achieved using conventional word-trigram recognition requiring manual transcription .	4	part-of	part-of	1
the $method$ combines domain independent acoustic models with off-the-shelf #classifiers# to give utterance classification performance that is surprisingly close to what can be achieved using conventional word-trigram recognition requiring manual transcription .	4	part-of	part-of	1
the method combines $domain independent acoustic models$ with off-the-shelf #classifiers# to give utterance classification performance that is surprisingly close to what can be achieved using conventional word-trigram recognition requiring manual transcription .	1	conjunction	conjunction	1
the $method$ combines domain independent acoustic models with off-the-shelf classifiers to give utterance classification performance that is surprisingly close to what can be achieved using conventional #word-trigram recognition# requiring manual transcription .	0	used-for	used-for	1
the method combines domain independent acoustic models with off-the-shelf classifiers to give utterance classification performance that is surprisingly close to what can be achieved using conventional $word-trigram recognition$ requiring #manual transcription# .	0	used-for	used-for	1
in our $method$ , #unsupervised training# is first used to train a phone n-gram model for a particular domain ; the output of recognition with this model is then passed to a phone-string classifier .	4	part-of	part-of	1
in our method , #unsupervised training# is first used to train a $phone n-gram model$ for a particular domain ; the output of recognition with this model is then passed to a phone-string classifier .	0	used-for	used-for	1
in our method , #unsupervised training# is first used to train a phone n-gram model for a particular $domain$ ; the output of recognition with this model is then passed to a phone-string classifier .	0	used-for	used-for	1
the #classification accuracy# of the $method$ is evaluated on three different spoken language system domains .	2	evaluate-for	evaluate-for	1
the classification accuracy of the $method$ is evaluated on three different #spoken language system domains# .	2	evaluate-for	evaluate-for	1
the #interval algebra -lrb- ia -rrb-# and a subset of the $region connection calculus -lrb- rcc -rrb-$ , namely rcc-8 , are the dominant artificial intelligence approaches for representing and reasoning about qualitative temporal and topological relations respectively .	1	conjunction	conjunction	1
the #interval algebra -lrb- ia -rrb-# and a subset of the region connection calculus -lrb- rcc -rrb- , namely rcc-8 , are the dominant $artificial intelligence approaches$ for representing and reasoning about qualitative temporal and topological relations respectively .	3	hyponym-of	hyponym-of	1
the interval algebra -lrb- ia -rrb- and a subset of the #region connection calculus -lrb- rcc -rrb-# , namely rcc-8 , are the dominant $artificial intelligence approaches$ for representing and reasoning about qualitative temporal and topological relations respectively .	3	hyponym-of	hyponym-of	1
the interval algebra -lrb- ia -rrb- and a subset of the $region connection calculus -lrb- rcc -rrb-$ , namely #rcc-8# , are the dominant artificial intelligence approaches for representing and reasoning about qualitative temporal and topological relations respectively .	3	hyponym-of	hyponym-of	1
the interval algebra -lrb- ia -rrb- and a subset of the region connection calculus -lrb- rcc -rrb- , namely rcc-8 , are the dominant #artificial intelligence approaches# for $representing and reasoning about qualitative temporal and topological relations$ respectively .	0	used-for	used-for	1
such $qualitative information$ can be formulated as a #qualitative constraint network -lrb- qcn -rrb-# .	0	used-for	used-for	1
in this paper , we focus on the minimal labeling problem -lrb- mlp -rrb- and we propose an #algorithm# to efficiently derive all the feasible base relations of a $qcn$ .	0	used-for	used-for	1
our $algorithm$ considers #chordal qcns# and a new form of partial consistency which we define as ◆ g-consistency .	4	part-of	part-of	1
our algorithm considers #chordal qcns# and a new form of $partial consistency$ which we define as ◆ g-consistency .	1	conjunction	conjunction	1
our $algorithm$ considers chordal qcns and a new form of #partial consistency# which we define as ◆ g-consistency .	4	part-of	part-of	1
our algorithm considers chordal qcns and a new form of $partial consistency$ which we define as #◆ g-consistency# .	3	hyponym-of	hyponym-of	1
experi-mentations with #qcns of ia and rcc-8# show the importance and efficiency of this new $approach$ .	2	evaluate-for	evaluate-for	1
in this paper a #morphological component# with a limited capability to automatically interpret -lrb- and generate -rrb- $derived words$ is presented .	0	used-for	used-for	1
the $system$ combines an extended #two-level morphology# -lsb- trost , 1991a ; trost , 1991b -rsb- with a feature-based word grammar building on a hierarchical lexicon .	0	used-for	used-for	1
the system combines an extended #two-level morphology# -lsb- trost , 1991a ; trost , 1991b -rsb- with a $feature-based word grammar$ building on a hierarchical lexicon .	1	conjunction	conjunction	1
the system combines an extended two-level morphology -lsb- trost , 1991a ; trost , 1991b -rsb- with a $feature-based word grammar$ building on a #hierarchical lexicon# .	0	used-for	used-for	1
$polymorphemic stems$ not explicitly stored in the lexicon are given a #compositional interpretation# .	5	feature-of	feature-of	1
the $system$ is implemented in #commonlisp# and has been tested on examples from german derivation .	0	used-for	used-for	1
the $system$ is implemented in commonlisp and has been tested on examples from #german derivation# .	2	evaluate-for	evaluate-for	1
four problems render vector space model -lrb- vsm -rrb- - based text classification approach ineffective : 1 -rrb- many words within song lyrics actually contribute little to sentiment ; 2 -rrb- nouns and verbs used to express sentiment are ambiguous ; 3 -rrb- #negations# and $modifiers$ around the sentiment keywords make particular contributions to sentiment ; 4 -rrb- song lyric is usually very short .	1	conjunction	conjunction	1
four problems render vector space model -lrb- vsm -rrb- - based text classification approach ineffective : 1 -rrb- many words within song lyrics actually contribute little to sentiment ; 2 -rrb- nouns and verbs used to express sentiment are ambiguous ; 3 -rrb- #negations# and modifiers around the sentiment keywords make particular contributions to $sentiment$ ; 4 -rrb- song lyric is usually very short .	0	used-for	used-for	1
four problems render vector space model -lrb- vsm -rrb- - based text classification approach ineffective : 1 -rrb- many words within song lyrics actually contribute little to sentiment ; 2 -rrb- nouns and verbs used to express sentiment are ambiguous ; 3 -rrb- negations and #modifiers# around the sentiment keywords make particular contributions to $sentiment$ ; 4 -rrb- song lyric is usually very short .	0	used-for	used-for	1
to address these problems , the #sentiment vector space model -lrb- s-vsm -rrb-# is proposed to represent $song lyric document$ .	0	used-for	used-for	1
the preliminary experiments prove that the #s-vsm model# outperforms the $vsm model$ in the lyric-based song sentiment classification task .	6	compare	compare	1
the preliminary experiments prove that the $s-vsm model$ outperforms the vsm model in the #lyric-based song sentiment classification task# .	2	evaluate-for	evaluate-for	1
the preliminary experiments prove that the s-vsm model outperforms the $vsm model$ in the #lyric-based song sentiment classification task# .	2	evaluate-for	evaluate-for	1
we present an efficient #algorithm# for the $redundancy elimination problem$ : given an underspecified semantic representation -lrb- usr -rrb- of a scope ambiguity , compute an usr with fewer mutually equivalent readings .	0	used-for	used-for	1
we present an efficient algorithm for the redundancy elimination problem : given an #underspecified semantic representation -lrb- usr -rrb-# of a $scope ambiguity$ , compute an usr with fewer mutually equivalent readings .	0	used-for	used-for	1
we present an efficient algorithm for the redundancy elimination problem : given an underspecified semantic representation -lrb- usr -rrb- of a scope ambiguity , compute an $usr$ with fewer mutually #equivalent readings# .	0	used-for	used-for	1
the #algorithm# operates on $underspecified chart representations$ which are derived from dominance graphs ; it can be applied to the usrs computed by large-scale grammars .	0	used-for	used-for	1
the algorithm operates on $underspecified chart representations$ which are derived from #dominance graphs# ; it can be applied to the usrs computed by large-scale grammars .	0	used-for	used-for	1
the algorithm operates on underspecified chart representations which are derived from dominance graphs ; #it# can be applied to the $usrs$ computed by large-scale grammars .	0	used-for	used-for	1
the algorithm operates on underspecified chart representations which are derived from dominance graphs ; it can be applied to the $usrs$ computed by #large-scale grammars# .	0	used-for	used-for	1
we evaluate the algorithm on a corpus , and show that #it# reduces the $degree of ambiguity$ significantly while taking negligible runtime .	0	used-for	used-for	1
currently several $grammatical formalisms$ converge towards being declarative and towards utilizing #context-free phrase-structure grammar# as a backbone , e.g. lfg and patr-ii .	0	used-for	used-for	1
currently several $grammatical formalisms$ converge towards being declarative and towards utilizing context-free phrase-structure grammar as a backbone , e.g. #lfg# and patr-ii .	3	hyponym-of	hyponym-of	1
currently several grammatical formalisms converge towards being declarative and towards utilizing context-free phrase-structure grammar as a backbone , e.g. #lfg# and $patr-ii$ .	1	conjunction	conjunction	1
currently several $grammatical formalisms$ converge towards being declarative and towards utilizing context-free phrase-structure grammar as a backbone , e.g. lfg and #patr-ii# .	3	hyponym-of	hyponym-of	1
typically the processing of these $formalisms$ is organized within a #chart-parsing framework# .	5	feature-of	feature-of	1
the aim of this paper is to provide a survey and a practical comparison of fundamental #rule-invocation strategies# within $context-free chart parsing$ .	4	part-of	part-of	1
the present paper focusses on $terminology structuring$ by #lexical methods# , which match terms on the basis on their content words , taking morphological variants into account .	0	used-for	used-for	1
experiments are done on a ` flat ' list of terms obtained from an originally $hierarchically-structured terminology$ : the french version of the #us national library of medicine mesh thesaurus# .	3	hyponym-of	hyponym-of	1
we compare the #lexically-induced relations# with the original $mesh relations$ : after a quantitative evaluation of their congruence through recall and precision metrics , we perform a qualitative , human analysis ofthe ` new ' relations not present in the mesh .	6	compare	compare	1
we compare the lexically-induced relations with the original $mesh relations$ : after a quantitative evaluation of their congruence through #recall and precision metrics# , we perform a qualitative , human analysis ofthe ` new ' relations not present in the mesh .	2	evaluate-for	evaluate-for	1
in order to boost the translation quality of $ebmt$ based on a #small-sized bilingual corpus# , we use an out-of-domain bilingual corpus and , in addition , the language model of an in-domain monolingual corpus .	0	used-for	used-for	1
in order to boost the translation quality of $ebmt$ based on a small-sized bilingual corpus , we use an #out-of-domain bilingual corpus# and , in addition , the language model of an in-domain monolingual corpus .	0	used-for	used-for	1
in order to boost the translation quality of $ebmt$ based on a small-sized bilingual corpus , we use an out-of-domain bilingual corpus and , in addition , the #language model# of an in-domain monolingual corpus .	0	used-for	used-for	1
in order to boost the translation quality of ebmt based on a small-sized bilingual corpus , we use an out-of-domain bilingual corpus and , in addition , the $language model$ of an #in-domain monolingual corpus# .	0	used-for	used-for	1
the two #evaluation measures# of the bleu score and the nist score demonstrated the effect of using an $out-of-domain bilingual corpus$ and the possibility of using the language model .	0	used-for	used-for	1
the two #evaluation measures# of the bleu score and the nist score demonstrated the effect of using an out-of-domain bilingual corpus and the possibility of using the $language model$ .	2	evaluate-for	evaluate-for	1
the two $evaluation measures$ of the #bleu score# and the nist score demonstrated the effect of using an out-of-domain bilingual corpus and the possibility of using the language model .	3	hyponym-of	hyponym-of	1
the two evaluation measures of the #bleu score# and the $nist score$ demonstrated the effect of using an out-of-domain bilingual corpus and the possibility of using the language model .	1	conjunction	conjunction	1
the two $evaluation measures$ of the bleu score and the #nist score# demonstrated the effect of using an out-of-domain bilingual corpus and the possibility of using the language model .	3	hyponym-of	hyponym-of	1
#diagrams# are common tools for representing $complex concepts$ , relationships and events , often when it would be difficult to portray the same information with natural images .	0	used-for	used-for	1
#diagrams# are common tools for representing complex concepts , $relationships$ and events , often when it would be difficult to portray the same information with natural images .	0	used-for	used-for	1
#diagrams# are common tools for representing complex concepts , relationships and $events$ , often when it would be difficult to portray the same information with natural images .	0	used-for	used-for	1
diagrams are common tools for representing #complex concepts# , $relationships$ and events , often when it would be difficult to portray the same information with natural images .	1	conjunction	conjunction	1
diagrams are common tools for representing complex concepts , #relationships# and $events$ , often when it would be difficult to portray the same information with natural images .	1	conjunction	conjunction	1
#understanding natural images# has been extensively studied in $computer vision$ , while diagram understanding has received little attention .	4	part-of	part-of	1
#understanding natural images# has been extensively studied in computer vision , while $diagram understanding$ has received little attention .	6	compare	compare	1
in this paper , we study the problem of diagram interpretation and reasoning , the challenging #task# of identifying the $structure of a diagram$ and the semantics of its constituents and their relationships .	0	used-for	used-for	1
we introduce #diagram parse graphs -lrb- dpg -rrb-# as our representation to model the $structure of diagrams$ .	0	used-for	used-for	1
we define #syntactic parsing of diagrams# as learning to infer $dpgs$ for diagrams and study semantic interpretation and reasoning of diagrams in the context of diagram question answering .	0	used-for	used-for	1
we define syntactic parsing of diagrams as learning to infer dpgs for diagrams and study #semantic interpretation and reasoning of diagrams# in the context of $diagram question answering$ .	0	used-for	used-for	1
we devise an #lstm-based method# for $syntactic parsing of diagrams$ and introduce a dpg-based attention model for diagram question answering .	0	used-for	used-for	1
we devise an lstm-based method for syntactic parsing of diagrams and introduce a #dpg-based attention model# for $diagram question answering$ .	0	used-for	used-for	1
we compile a new $dataset$ of #diagrams# with exhaustive annotations of constituents and relationships for over 5,000 diagrams and 15,000 questions and answers .	5	feature-of	feature-of	1
our results show the significance of our #models# for $syntactic parsing and question answering in diagrams$ using dpgs .	0	used-for	used-for	1
our results show the significance of our $models$ for syntactic parsing and question answering in diagrams using #dpgs# .	0	used-for	used-for	1
previous #change detection methods# , focusing on $detecting large-scale significant changes$ , can not do this well .	0	used-for	used-for	1
this paper proposes a feasible #end-to-end approach# to this challenging $problem$ .	0	used-for	used-for	1
given two times observations , we formulate $fine-grained change detection$ as a #joint optimization problem# of three related factors , i.e. , normal-aware lighting difference , camera geometry correction flow , and real scene change mask .	0	used-for	used-for	1
given two times observations , we formulate fine-grained change detection as a $joint optimization problem$ of three related #factors# , i.e. , normal-aware lighting difference , camera geometry correction flow , and real scene change mask .	5	feature-of	feature-of	1
given two times observations , we formulate fine-grained change detection as a joint optimization problem of three related $factors$ , i.e. , #normal-aware lighting difference# , camera geometry correction flow , and real scene change mask .	3	hyponym-of	hyponym-of	1
given two times observations , we formulate fine-grained change detection as a joint optimization problem of three related factors , i.e. , #normal-aware lighting difference# , $camera geometry correction flow$ , and real scene change mask .	1	conjunction	conjunction	1
given two times observations , we formulate fine-grained change detection as a joint optimization problem of three related $factors$ , i.e. , normal-aware lighting difference , #camera geometry correction flow# , and real scene change mask .	3	hyponym-of	hyponym-of	1
given two times observations , we formulate fine-grained change detection as a joint optimization problem of three related factors , i.e. , normal-aware lighting difference , #camera geometry correction flow# , and $real scene change mask$ .	1	conjunction	conjunction	1
given two times observations , we formulate fine-grained change detection as a joint optimization problem of three related $factors$ , i.e. , normal-aware lighting difference , camera geometry correction flow , and #real scene change mask# .	3	hyponym-of	hyponym-of	1
we solve the three $factors$ in a #coarse-to-fine manner# and achieve reliable change decision by rank minimization .	0	used-for	used-for	1
we solve the three factors in a coarse-to-fine manner and achieve reliable $change decision$ by #rank minimization# .	0	used-for	used-for	1
we build three #real-world datasets# to benchmark $fine-grained change detection of misaligned scenes$ under varied multiple lighting conditions .	2	evaluate-for	evaluate-for	1
we build three real-world datasets to benchmark $fine-grained change detection of misaligned scenes$ under #varied multiple lighting conditions# .	5	feature-of	feature-of	1
extensive experiments show the superior performance of our #approach# over state-of-the-art $change detection methods$ and its ability to distinguish real scene changes from false ones caused by lighting variations .	6	compare	compare	1
extensive experiments show the superior performance of our #approach# over state-of-the-art change detection methods and its ability to distinguish $real scene changes$ from false ones caused by lighting variations .	0	used-for	used-for	1
#automatic evaluation metrics# for $machine translation -lrb- mt -rrb- systems$ , such as bleu or nist , are now well established .	2	evaluate-for	evaluate-for	1
$automatic evaluation metrics$ for machine translation -lrb- mt -rrb- systems , such as #bleu# or nist , are now well established .	3	hyponym-of	hyponym-of	1
automatic evaluation metrics for machine translation -lrb- mt -rrb- systems , such as #bleu# or $nist$ , are now well established .	1	conjunction	conjunction	1
$automatic evaluation metrics$ for machine translation -lrb- mt -rrb- systems , such as bleu or #nist# , are now well established .	3	hyponym-of	hyponym-of	1
yet , #they# are scarcely used for the $assessment of language pairs$ like english-chinese or english-japanese , because of the word segmentation problem .	0	used-for	used-for	1
yet , they are scarcely used for the assessment of $language pairs$ like #english-chinese# or english-japanese , because of the word segmentation problem .	3	hyponym-of	hyponym-of	1
yet , they are scarcely used for the assessment of language pairs like #english-chinese# or $english-japanese$ , because of the word segmentation problem .	1	conjunction	conjunction	1
yet , they are scarcely used for the assessment of $language pairs$ like english-chinese or #english-japanese# , because of the word segmentation problem .	3	hyponym-of	hyponym-of	1
this study establishes the equivalence between the standard use of #bleu# in $word n-grams$ and its application at the character level .	0	used-for	used-for	1
this study establishes the equivalence between the standard use of #bleu# in word n-grams and its application at the $character level$ .	0	used-for	used-for	1
this study establishes the equivalence between the standard use of bleu in #word n-grams# and its application at the $character level$ .	1	conjunction	conjunction	1
the use of #bleu# at the $character level$ eliminates the word segmentation problem : it makes it possible to directly compare commercial systems outputting unsegmented texts with , for instance , statistical mt systems which usually segment their outputs .	0	used-for	used-for	1
the use of #bleu# at the character level eliminates the $word segmentation problem$ : it makes it possible to directly compare commercial systems outputting unsegmented texts with , for instance , statistical mt systems which usually segment their outputs .	0	used-for	used-for	1
the use of bleu at the character level eliminates the word segmentation problem : #it# makes it possible to directly compare $commercial systems$ outputting unsegmented texts with , for instance , statistical mt systems which usually segment their outputs .	2	evaluate-for	evaluate-for	1
the use of bleu at the character level eliminates the word segmentation problem : #it# makes it possible to directly compare commercial systems outputting unsegmented texts with , for instance , $statistical mt systems$ which usually segment their outputs .	2	evaluate-for	evaluate-for	1
the use of bleu at the character level eliminates the word segmentation problem : it makes it possible to directly compare #commercial systems# outputting unsegmented texts with , for instance , $statistical mt systems$ which usually segment their outputs .	6	compare	compare	1
this paper proposes a series of modifications to the #left corner parsing algorithm# for $context-free grammars$ .	0	used-for	used-for	1
it is argued that the resulting #algorithm# is both efficient and flexible and is , therefore , a good choice for the $parser$ used in a natural language interface .	0	used-for	used-for	1
it is argued that the resulting algorithm is both efficient and flexible and is , therefore , a good choice for the #parser# used in a $natural language interface$ .	0	used-for	used-for	1
this paper presents a novel $statistical singing voice conversion -lrb- svc -rrb- technique$ with #direct waveform modification# based on the spectrum differential that can convert voice timbre of a source singer into that of a target singer without using a vocoder to generate converted singing voice waveforms .	0	used-for	used-for	1
this paper presents a novel statistical singing voice conversion -lrb- svc -rrb- technique with $direct waveform modification$ based on the #spectrum differential# that can convert voice timbre of a source singer into that of a target singer without using a vocoder to generate converted singing voice waveforms .	0	used-for	used-for	1
this paper presents a novel statistical singing voice conversion -lrb- svc -rrb- technique with direct waveform modification based on the #spectrum differential# that can convert $voice timbre$ of a source singer into that of a target singer without using a vocoder to generate converted singing voice waveforms .	0	used-for	used-for	1
this paper presents a novel statistical singing voice conversion -lrb- svc -rrb- technique with direct waveform modification based on the spectrum differential that can convert voice timbre of a source singer into that of a target singer without using a #vocoder# to generate $converted singing voice waveforms$ .	0	used-for	used-for	1
#svc# makes it possible to convert $singing voice characteristics$ of an arbitrary source singer into those of an arbitrary target singer .	0	used-for	used-for	1
however , #speech quality# of the $converted singing voice$ is significantly degraded compared to that of a natural singing voice due to various factors , such as analysis and modeling errors in the vocoder-based framework .	2	evaluate-for	evaluate-for	1
however , #speech quality# of the converted singing voice is significantly degraded compared to that of a $natural singing voice$ due to various factors , such as analysis and modeling errors in the vocoder-based framework .	2	evaluate-for	evaluate-for	1
however , speech quality of the #converted singing voice# is significantly degraded compared to that of a $natural singing voice$ due to various factors , such as analysis and modeling errors in the vocoder-based framework .	6	compare	compare	1
the $differential spectral feature$ is directly estimated using a #differential gaussian mixture model -lrb- gmm -rrb-# that is analytically derived from the traditional gmm used as a conversion model in the conventional svc .	0	used-for	used-for	1
the differential spectral feature is directly estimated using a $differential gaussian mixture model -lrb- gmm -rrb-$ that is analytically derived from the traditional #gmm# used as a conversion model in the conventional svc .	0	used-for	used-for	1
the differential spectral feature is directly estimated using a differential gaussian mixture model -lrb- gmm -rrb- that is analytically derived from the traditional #gmm# used as a $conversion model$ in the conventional svc .	0	used-for	used-for	1
the differential spectral feature is directly estimated using a differential gaussian mixture model -lrb- gmm -rrb- that is analytically derived from the traditional gmm used as a #conversion model# in the conventional $svc$ .	0	used-for	used-for	1
the experimental results demonstrate that the proposed #method# makes it possible to significantly improve speech quality in the converted singing voice while preserving the conversion accuracy of singer identity compared to the conventional $svc$ .	6	compare	compare	1
the experimental results demonstrate that the proposed $method$ makes it possible to significantly improve #speech quality# in the converted singing voice while preserving the conversion accuracy of singer identity compared to the conventional svc .	2	evaluate-for	evaluate-for	1
the experimental results demonstrate that the proposed method makes it possible to significantly improve #speech quality# in the converted singing voice while preserving the conversion accuracy of singer identity compared to the conventional $svc$ .	2	evaluate-for	evaluate-for	1
the experimental results demonstrate that the proposed $method$ makes it possible to significantly improve speech quality in the converted singing voice while preserving the #conversion accuracy of singer identity# compared to the conventional svc .	2	evaluate-for	evaluate-for	1
the experimental results demonstrate that the proposed method makes it possible to significantly improve speech quality in the converted singing voice while preserving the #conversion accuracy of singer identity# compared to the conventional $svc$ .	0	used-for	used-for	1
during late-2013 through early-2014 nist coordinated a special $i-vector challenge$ based on data used in previous #nist speaker recognition evaluations -lrb- sres -rrb-# .	0	used-for	used-for	1
unlike evaluations in the sre series , the i-vector challenge was run entirely online and used #fixed-length feature vectors# projected into a $low-dimensional space -lrb- i-vectors -rrb-$ rather than audio recordings .	0	used-for	used-for	1
unlike evaluations in the sre series , the $i-vector challenge$ was run entirely online and used fixed-length feature vectors projected into a #low-dimensional space -lrb- i-vectors -rrb-# rather than audio recordings .	0	used-for	used-for	1
unlike evaluations in the sre series , the i-vector challenge was run entirely online and used fixed-length feature vectors projected into a $low-dimensional space -lrb- i-vectors -rrb-$ rather than #audio recordings# .	6	compare	compare	1
compared to the 2012 #sre# , the $i-vector challenge$ saw an increase in the number of participants by nearly a factor of two , and a two orders of magnitude increase in the number of systems submitted for evaluation .	6	compare	compare	1
initial results indicate the #leading system# achieved an approximate 37 % improvement relative to the $baseline system$ .	6	compare	compare	1
theoretical research in the area of $machine translation$ usually involves the search for and creation of an appropriate #formalism# .	0	used-for	used-for	1
in this paper , we will introduce the #anaphoric component# of the $mimo formalism$ .	4	part-of	part-of	1
in #mimo# , the $translation of anaphoric relations$ is compositional .	0	used-for	used-for	1
the #anaphoric component# is used to define $linguistic phenomena$ such as wh-movement , the passive and the binding of reflexives and pronouns mono-lingually .	0	used-for	used-for	1
the anaphoric component is used to define $linguistic phenomena$ such as #wh-movement# , the passive and the binding of reflexives and pronouns mono-lingually .	3	hyponym-of	hyponym-of	1
the anaphoric component is used to define linguistic phenomena such as #wh-movement# , $the passive and the binding of reflexives and pronouns$ mono-lingually .	1	conjunction	conjunction	1
the anaphoric component is used to define $linguistic phenomena$ such as wh-movement , #the passive and the binding of reflexives and pronouns# mono-lingually .	3	hyponym-of	hyponym-of	1
the #efficiency# and $quality$ is exhibited in a live demonstration that recognizes cd-covers from a database of 40000 images of popular music cd 's .	1	conjunction	conjunction	1
the efficiency and quality is exhibited in a live demonstration that recognizes cd-covers from a $database$ of 40000 #images of popular music cd 's# .	5	feature-of	feature-of	1
the #scheme# builds upon popular techniques of indexing descriptors extracted from local regions , and is robust to $background clutter$ and occlusion .	0	used-for	used-for	1
the #scheme# builds upon popular techniques of indexing descriptors extracted from local regions , and is robust to background clutter and $occlusion$ .	0	used-for	used-for	1
the $scheme$ builds upon popular techniques of #indexing descriptors# extracted from local regions , and is robust to background clutter and occlusion .	0	used-for	used-for	1
the scheme builds upon popular techniques of $indexing descriptors$ extracted from #local regions# , and is robust to background clutter and occlusion .	0	used-for	used-for	1
the scheme builds upon popular techniques of indexing descriptors extracted from local regions , and is robust to #background clutter# and $occlusion$ .	1	conjunction	conjunction	1
the $local region descriptors$ are hierarchically quantized in a #vocabulary tree# .	0	used-for	used-for	1
the #quantization# and the $indexing$ are therefore fully integrated , essentially being one and the same .	1	conjunction	conjunction	1
the #recognition quality# is evaluated through retrieval on a database with ground truth , showing the power of the $vocabulary tree approach$ , going as high as 1 million images .	2	evaluate-for	evaluate-for	1
the $recognition quality$ is evaluated through #retrieval# on a database with ground truth , showing the power of the vocabulary tree approach , going as high as 1 million images .	2	evaluate-for	evaluate-for	1
the recognition quality is evaluated through $retrieval$ on a #database with ground truth# , showing the power of the vocabulary tree approach , going as high as 1 million images .	0	used-for	used-for	1
this paper presents a #method# for $blind estimation of reverberation times$ in reverberant enclosures .	0	used-for	used-for	1
this paper presents a method for $blind estimation of reverberation times$ in #reverberant enclosures# .	5	feature-of	feature-of	1
the proposed $algorithm$ is based on a #statistical model of short-term log-energy sequences# for echo-free speech .	0	used-for	used-for	1
the proposed algorithm is based on a #statistical model of short-term log-energy sequences# for $echo-free speech$ .	0	used-for	used-for	1
the #method# has been successfully applied to $robust automatic speech recognition$ in reverberant environments by model selection .	0	used-for	used-for	1
the method has been successfully applied to $robust automatic speech recognition$ in #reverberant environments# by model selection .	5	feature-of	feature-of	1
the $method$ has been successfully applied to robust automatic speech recognition in reverberant environments by #model selection# .	0	used-for	used-for	1
for this application , the $reverberation time$ is first estimated from the #reverberated speech utterance# to be recognized .	0	used-for	used-for	1
the #estimation# is then used to select the best $acoustic model$ out of a library of models trained in various artificial re-verberant conditions .	0	used-for	used-for	1
the estimation is then used to select the best #acoustic model# out of a library of $models$ trained in various artificial re-verberant conditions .	4	part-of	part-of	1
the estimation is then used to select the best acoustic model out of a library of $models$ trained in various #artificial re-verberant conditions# .	5	feature-of	feature-of	1
#speech recognition# experiments in simulated and real reverberant environments show the efficiency of our $approach$ which outperforms standard channel normaliza-tion techniques .	2	evaluate-for	evaluate-for	1
#speech recognition# experiments in simulated and real reverberant environments show the efficiency of our approach which outperforms standard $channel normaliza-tion techniques$ .	2	evaluate-for	evaluate-for	1
$speech recognition$ experiments in #simulated and real reverberant environments# show the efficiency of our approach which outperforms standard channel normaliza-tion techniques .	5	feature-of	feature-of	1
speech recognition experiments in simulated and real reverberant environments show the efficiency of our $approach$ which outperforms standard #channel normaliza-tion techniques# .	6	compare	compare	1
for one thing , #learning methodology# applicable in $general domains$ does not readily lend itself in the linguistic domain .	0	used-for	used-for	1
for one thing , learning methodology applicable in #general domains# does not readily lend itself in the $linguistic domain$ .	6	compare	compare	1
for another , #linguistic representation# used by $language processing systems$ is not geared to learning .	0	used-for	used-for	1
we introduced a new #linguistic representation# , the dynamic hierarchical phrasal lexicon -lrb- dhpl -rrb- -lsb- zernik88 -rsb- , to facilitate $language acquisition$ .	0	used-for	used-for	1
we introduced a new $linguistic representation$ , the #dynamic hierarchical phrasal lexicon -lrb- dhpl -rrb-# -lsb- zernik88 -rsb- , to facilitate language acquisition .	3	hyponym-of	hyponym-of	1
we introduced a new linguistic representation , the #dynamic hierarchical phrasal lexicon -lrb- dhpl -rrb-# -lsb- zernik88 -rsb- , to facilitate $language acquisition$ .	0	used-for	used-for	1
from this , a #language learning model# was implemented in the program $rina$ , which enhances its own lexical hierarchy by processing examples in context .	4	part-of	part-of	1
we identified two tasks : first , how #linguistic concepts# are acquired from training examples and organized in a $hierarchy$ ; this task was discussed in previous papers -lsb- zernik87 -rsb- .	4	part-of	part-of	1
second , we show in this paper how a #lexical hierarchy# is used in predicting new $linguistic concepts$ .	0	used-for	used-for	1
this paper presents a novel #ensemble learning approach# to resolving $german pronouns$ .	0	used-for	used-for	1
experiments show that this #approach# is superior to a single $decision-tree classifier$ .	6	compare	compare	1
furthermore , we present a #standalone system# that resolves $pronouns$ in unannotated text by using a fully automatic sequence of preprocessing modules that mimics the manual annotation process .	0	used-for	used-for	1
furthermore , we present a #standalone system# that resolves pronouns in $unannotated text$ by using a fully automatic sequence of preprocessing modules that mimics the manual annotation process .	0	used-for	used-for	1
furthermore , we present a standalone system that resolves #pronouns# in $unannotated text$ by using a fully automatic sequence of preprocessing modules that mimics the manual annotation process .	4	part-of	part-of	1
furthermore , we present a $standalone system$ that resolves pronouns in unannotated text by using a fully automatic sequence of #preprocessing modules# that mimics the manual annotation process .	0	used-for	used-for	1
furthermore , we present a standalone system that resolves pronouns in unannotated text by using a fully automatic sequence of #preprocessing modules# that mimics the $manual annotation process$ .	0	used-for	used-for	1
although the $system$ performs well within a limited #textual domain# , further research is needed to make it effective for open-domain question answering and text summarisation .	2	evaluate-for	evaluate-for	1
although the system performs well within a limited #textual domain# , further research is needed to make it effective for $open-domain question answering$ and text summarisation .	6	compare	compare	1
although the system performs well within a limited textual domain , further research is needed to make #it# effective for $open-domain question answering$ and text summarisation .	0	used-for	used-for	1
although the system performs well within a limited textual domain , further research is needed to make #it# effective for open-domain question answering and $text summarisation$ .	0	used-for	used-for	1
although the system performs well within a limited textual domain , further research is needed to make it effective for #open-domain question answering# and $text summarisation$ .	1	conjunction	conjunction	1
in this paper , we compare the performance of a state-of-the-art #statistical parser# -lrb- bikel , 2004 -rrb- in $parsing written and spoken language$ and in generating sub-categorization cues from written and spoken language .	0	used-for	used-for	1
in this paper , we compare the performance of a state-of-the-art #statistical parser# -lrb- bikel , 2004 -rrb- in parsing written and spoken language and in $generating sub-categorization cues$ from written and spoken language .	0	used-for	used-for	1
in this paper , we compare the performance of a state-of-the-art statistical parser -lrb- bikel , 2004 -rrb- in #parsing written and spoken language# and in $generating sub-categorization cues$ from written and spoken language .	1	conjunction	conjunction	1
in this paper , we compare the performance of a state-of-the-art statistical parser -lrb- bikel , 2004 -rrb- in parsing written and spoken language and in $generating sub-categorization cues$ from #written and spoken language# .	0	used-for	used-for	1
although #bikel 's parser# achieves a higher accuracy for $parsing written language$ , it achieves a higher accuracy when extracting subcategorization cues from spoken language .	0	used-for	used-for	1
although $bikel 's parser$ achieves a higher #accuracy# for parsing written language , it achieves a higher accuracy when extracting subcategorization cues from spoken language .	2	evaluate-for	evaluate-for	1
although bikel 's parser achieves a higher accuracy for parsing written language , #it# achieves a higher accuracy when extracting $subcategorization cues$ from spoken language .	0	used-for	used-for	1
although bikel 's parser achieves a higher accuracy for parsing written language , $it$ achieves a higher #accuracy# when extracting subcategorization cues from spoken language .	2	evaluate-for	evaluate-for	1
although bikel 's parser achieves a higher accuracy for parsing written language , it achieves a higher accuracy when extracting #subcategorization cues# from $spoken language$ .	4	part-of	part-of	1
our experiments also show that current #technology# for $extracting subcategorization frames$ initially designed for written texts works equally well for spoken language .	0	used-for	used-for	1
our experiments also show that current #technology# for extracting subcategorization frames initially designed for written texts works equally well for $spoken language$ .	0	used-for	used-for	1
our experiments also show that current technology for #extracting subcategorization frames# initially designed for $written texts$ works equally well for spoken language .	0	used-for	used-for	1
our experiments also show that current technology for extracting subcategorization frames initially designed for #written texts# works equally well for $spoken language$ .	6	compare	compare	1
additionally , we explore the utility of #punctuation# in helping $parsing$ and extraction of subcategorization cues .	0	used-for	used-for	1
additionally , we explore the utility of #punctuation# in helping parsing and $extraction of subcategorization cues$ .	0	used-for	used-for	1
our experiments show that punctuation is of little help in #parsing spoken language# and $extracting subcategorization cues$ from spoken language .	1	conjunction	conjunction	1
our experiments show that punctuation is of little help in parsing spoken language and extracting #subcategorization cues# from $spoken language$ .	4	part-of	part-of	1
our experiments show that punctuation is of little help in parsing spoken language and $extracting subcategorization cues$ from #spoken language# .	0	used-for	used-for	1
this paper proposes an #alignment adaptation approach# to improve $domain-specific -lrb- in-domain -rrb- word alignment$ .	0	used-for	used-for	1
the basic idea of #alignment adaptation# is to use out-of-domain corpus to improve $in-domain word alignment$ results .	0	used-for	used-for	1
the basic idea of $alignment adaptation$ is to use #out-of-domain corpus# to improve in-domain word alignment results .	0	used-for	used-for	1
in this paper , we first train two $statistical word alignment models$ with the #large-scale out-of-domain corpus# and the small-scale in-domain corpus respectively , and then interpolate these two models to improve the domain-specific word alignment .	0	used-for	used-for	1
in this paper , we first train two statistical word alignment models with the #large-scale out-of-domain corpus# and the $small-scale in-domain corpus$ respectively , and then interpolate these two models to improve the domain-specific word alignment .	1	conjunction	conjunction	1
in this paper , we first train two $statistical word alignment models$ with the large-scale out-of-domain corpus and the #small-scale in-domain corpus# respectively , and then interpolate these two models to improve the domain-specific word alignment .	0	used-for	used-for	1
in this paper , we first train two statistical word alignment models with the large-scale out-of-domain corpus and the small-scale in-domain corpus respectively , and then interpolate these two #models# to improve the $domain-specific word alignment$ .	0	used-for	used-for	1
experimental results show that our #approach# improves $domain-specific word alignment$ in terms of both precision and recall , achieving a relative error rate reduction of 6.56 % as compared with the state-of-the-art technologies .	0	used-for	used-for	1
experimental results show that our #approach# improves domain-specific word alignment in terms of both precision and recall , achieving a relative error rate reduction of 6.56 % as compared with the $state-of-the-art technologies$ .	6	compare	compare	1
experimental results show that our $approach$ improves domain-specific word alignment in terms of both #precision# and recall , achieving a relative error rate reduction of 6.56 % as compared with the state-of-the-art technologies .	2	evaluate-for	evaluate-for	1
experimental results show that our approach improves domain-specific word alignment in terms of both #precision# and $recall$ , achieving a relative error rate reduction of 6.56 % as compared with the state-of-the-art technologies .	1	conjunction	conjunction	1
experimental results show that our $approach$ improves domain-specific word alignment in terms of both precision and #recall# , achieving a relative error rate reduction of 6.56 % as compared with the state-of-the-art technologies .	2	evaluate-for	evaluate-for	1
experimental results show that our $approach$ improves domain-specific word alignment in terms of both precision and recall , achieving a #relative error rate reduction# of 6.56 % as compared with the state-of-the-art technologies .	2	evaluate-for	evaluate-for	1
experimental results show that our approach improves domain-specific word alignment in terms of both precision and recall , achieving a #relative error rate reduction# of 6.56 % as compared with the $state-of-the-art technologies$ .	2	evaluate-for	evaluate-for	1
with performance above 97 % #accuracy# for newspaper text , $part of speech -lrb- pos -rrb- tagging$ might be considered a solved problem .	2	evaluate-for	evaluate-for	1
with performance above 97 % accuracy for #newspaper text# , $part of speech -lrb- pos -rrb- tagging$ might be considered a solved problem .	2	evaluate-for	evaluate-for	1
previous studies have shown that allowing the #parser# to resolve $pos tag ambiguity$ does not improve performance .	0	used-for	used-for	1
however , for $grammar formalisms$ which use more #fine-grained grammatical categories# , for example tag and ccg , tagging accuracy is much lower .	0	used-for	used-for	1
however , for grammar formalisms which use more $fine-grained grammatical categories$ , for example #tag# and ccg , tagging accuracy is much lower .	3	hyponym-of	hyponym-of	1
however , for grammar formalisms which use more $fine-grained grammatical categories$ , for example tag and #ccg# , tagging accuracy is much lower .	3	hyponym-of	hyponym-of	1
however , for $grammar formalisms$ which use more fine-grained grammatical categories , for example tag and ccg , #tagging accuracy# is much lower .	2	evaluate-for	evaluate-for	1
in fact , for these $formalisms$ , premature ambiguity resolution makes #parsing# infeasible .	0	used-for	used-for	1
we describe a #multi-tagging approach# which maintains a suitable level of lexical category ambiguity for accurate and efficient $ccg parsing$ .	0	used-for	used-for	1
we describe a $multi-tagging approach$ which maintains a suitable level of #lexical category ambiguity# for accurate and efficient ccg parsing .	5	feature-of	feature-of	1
we extend this #multi-tagging approach# to the $pos level$ to overcome errors introduced by automatically assigned pos tags .	0	used-for	used-for	1
although pos tagging accuracy seems high , maintaining some #pos tag ambiguity# in the $language processing pipeline$ results in more accurate ccg supertagging .	5	feature-of	feature-of	1
although pos tagging accuracy seems high , maintaining some #pos tag ambiguity# in the language processing pipeline results in more accurate $ccg supertagging$ .	0	used-for	used-for	1
we previously presented a #framework# for $segmentation of complex scenes$ using multiple physical hypotheses for simple image regions .	0	used-for	used-for	1
we previously presented a $framework$ for segmentation of complex scenes using multiple #physical hypotheses# for simple image regions .	0	used-for	used-for	1
we previously presented a framework for segmentation of complex scenes using multiple #physical hypotheses# for $simple image regions$ .	0	used-for	used-for	1
a consequence of that #framework# was a proposal for a new $approach$ to the segmentation of complex scenes into regions corresponding to coherent surfaces rather than merely regions of similar color .	0	used-for	used-for	1
a consequence of that framework was a proposal for a new #approach# to the $segmentation of complex scenes$ into regions corresponding to coherent surfaces rather than merely regions of similar color .	0	used-for	used-for	1
a consequence of that framework was a proposal for a new approach to the segmentation of complex scenes into regions corresponding to #coherent surfaces# rather than merely $regions of similar color$ .	6	compare	compare	1
herein we present an implementation of this new approach and show example #segmentations# for $scenes$ containing multi-colored piece-wise uniform objects .	0	used-for	used-for	1
herein we present an implementation of this new approach and show example segmentations for $scenes$ containing #multi-colored piece-wise uniform objects# .	5	feature-of	feature-of	1
using our #approach# we are able to intelligently segment scenes with objects of greater complexity than previous $physics-based segmentation algorithms$ .	6	compare	compare	1
#smartkom# is a $multimodal dialog system$ that combines speech , gesture , and mimics input and output .	3	hyponym-of	hyponym-of	1
smartkom is a $multimodal dialog system$ that combines #speech# , gesture , and mimics input and output .	0	used-for	used-for	1
smartkom is a multimodal dialog system that combines #speech# , $gesture$ , and mimics input and output .	1	conjunction	conjunction	1
smartkom is a $multimodal dialog system$ that combines speech , #gesture# , and mimics input and output .	0	used-for	used-for	1
#spontaneous speech understanding# is combined with the $video-based recognition of natural gestures$ .	1	conjunction	conjunction	1
one of the major scientific goals of #smartkom# is to design new $computational methods$ for the seamless integration and mutual disambiguation of multimodal input and output on a semantic and pragmatic level .	0	used-for	used-for	1
one of the major scientific goals of smartkom is to design new #computational methods# for the seamless $integration and mutual disambiguation of multimodal input and output$ on a semantic and pragmatic level .	0	used-for	used-for	1
one of the major scientific goals of smartkom is to design new computational methods for the seamless $integration and mutual disambiguation of multimodal input and output$ on a #semantic and pragmatic level# .	5	feature-of	feature-of	1
$smartkom$ is based on the #situated delegation-oriented dialog paradigm# , in which the user delegates a task to a virtual communication assistant , visualized as a lifelike character on a graphical display .	0	used-for	used-for	1
we describe the smartkom architecture , the use of an #xml-based markup language# for $multimodal content$ , and some of the distinguishing features of the first fully operational smartkom demonstrator .	0	used-for	used-for	1
we present a #single-image highlight removal method# that incorporates illumination-based constraints into $image in-painting$ .	0	used-for	used-for	1
we present a single-image highlight removal method that incorporates #illumination-based constraints# into $image in-painting$ .	4	part-of	part-of	1
#constraints# provided by observed pixel colors , highlight color analysis and illumination color uniformity are employed in our $method$ to improve estimation of the underlying diffuse color .	0	used-for	used-for	1
constraints provided by observed #pixel colors# , $highlight color analysis$ and illumination color uniformity are employed in our method to improve estimation of the underlying diffuse color .	1	conjunction	conjunction	1
constraints provided by observed pixel colors , #highlight color analysis# and $illumination color uniformity$ are employed in our method to improve estimation of the underlying diffuse color .	1	conjunction	conjunction	1
constraints provided by observed pixel colors , highlight color analysis and illumination color uniformity are employed in our #method# to improve $estimation of the underlying diffuse color$ .	0	used-for	used-for	1
the inclusion of these #illumination constraints# allows for better $recovery of shading and textures$ by inpainting .	0	used-for	used-for	1
the inclusion of these illumination constraints allows for better $recovery of shading and textures$ by #inpainting# .	0	used-for	used-for	1
in this paper , we propose a novel #method# , called local non-negative matrix factorization -lrb- lnmf -rrb- , for learning $spatially localized , parts-based subspace representation of visual patterns$ .	0	used-for	used-for	1
an #objective function# is defined to impose $lo-calization constraint$ , in addition to the non-negativity constraint in the standard nmf -lsb- 1 -rsb- .	0	used-for	used-for	1
an objective function is defined to impose lo-calization constraint , in addition to the #non-negativity constraint# in the standard $nmf$ -lsb- 1 -rsb- .	4	part-of	part-of	1
an #algorithm# is presented for the $learning$ of such basis components .	0	used-for	used-for	1
experimental results are presented to compare #lnmf# with the $nmf and pca methods$ for face representation and recognition , which demonstrates advantages of lnmf .	6	compare	compare	1
experimental results are presented to compare #lnmf# with the nmf and pca methods for $face representation and recognition$ , which demonstrates advantages of lnmf .	0	used-for	used-for	1
experimental results are presented to compare lnmf with the #nmf and pca methods# for $face representation and recognition$ , which demonstrates advantages of lnmf .	0	used-for	used-for	1
experimental results are presented to compare lnmf with the nmf and pca methods for #face representation and recognition# , which demonstrates advantages of $lnmf$ .	2	evaluate-for	evaluate-for	1
many ai researchers have investigated useful ways of verifying and validating knowledge bases for #ontologies# and $rules$ , but it is not easy to directly apply them to checking process models .	1	conjunction	conjunction	1
other techniques developed for #checking and refining planning knowledge# tend to focus on $automated plan generation$ rather than helping users author process information .	0	used-for	used-for	1
in this paper , we propose a #complementary approach# which helps users author and check $process models$ .	0	used-for	used-for	1
$it$ builds #interdepen-dency models# from this analysis and uses them to find errors and propose fixes .	0	used-for	used-for	1
it builds interdepen-dency models from this analysis and uses #them# to find $errors$ and propose fixes .	0	used-for	used-for	1
it builds interdepen-dency models from this analysis and uses #them# to find errors and propose $fixes$ .	0	used-for	used-for	1
in this paper , we describe the research using #machine learning techniques# to build a $comma checker$ to be integrated in a grammar checker for basque .	0	used-for	used-for	1
in this paper , we describe the research using machine learning techniques to build a #comma checker# to be integrated in a $grammar checker$ for basque .	4	part-of	part-of	1
in this paper , we describe the research using machine learning techniques to build a comma checker to be integrated in a #grammar checker# for $basque$ .	0	used-for	used-for	1
after several experiments , and trained with a little corpus of 100,000 words , the $system$ guesses correctly not placing commas with a #precision# of 96 % and a recall of 98 % .	2	evaluate-for	evaluate-for	1
after several experiments , and trained with a little corpus of 100,000 words , the $system$ guesses correctly not placing commas with a precision of 96 % and a #recall# of 98 % .	2	evaluate-for	evaluate-for	1
#it# also gets a precision of 70 % and a recall of 49 % in the task of $placing commas$ .	0	used-for	used-for	1
$it$ also gets a #precision# of 70 % and a recall of 49 % in the task of placing commas .	2	evaluate-for	evaluate-for	1
$it$ also gets a precision of 70 % and a #recall# of 49 % in the task of placing commas .	2	evaluate-for	evaluate-for	1
the present paper reports on a preparatory research for building a #language corpus annotation scenario# capturing the $discourse relations$ in czech .	0	used-for	used-for	1
the present paper reports on a preparatory research for building a language corpus annotation scenario capturing the $discourse relations$ in #czech# .	5	feature-of	feature-of	1
we primarily focus on the description of the $syntactically motivated relations in discourse$ , basing our findings on the theoretical background of the #prague dependency treebank 2.0# and the penn discourse treebank 2 .	0	used-for	used-for	1
we primarily focus on the description of the syntactically motivated relations in discourse , basing our findings on the theoretical background of the #prague dependency treebank 2.0# and the $penn discourse treebank 2$ .	1	conjunction	conjunction	1
we primarily focus on the description of the $syntactically motivated relations in discourse$ , basing our findings on the theoretical background of the prague dependency treebank 2.0 and the #penn discourse treebank 2# .	0	used-for	used-for	1
our aim is to revisit the present-day #syntactico-semantic -lrb- tectogrammatical -rrb- annotation# in the $prague dependency treebank$ , extend it for the purposes of a sentence-boundary-crossing representation and eventually to design a new , discourse level of annotation .	4	part-of	part-of	1
our aim is to revisit the present-day syntactico-semantic -lrb- tectogrammatical -rrb- annotation in the prague dependency treebank , extend #it# for the purposes of a $sentence-boundary-crossing representation$ and eventually to design a new , discourse level of annotation .	0	used-for	used-for	1
our aim is to revisit the present-day syntactico-semantic -lrb- tectogrammatical -rrb- annotation in the prague dependency treebank , extend #it# for the purposes of a sentence-boundary-crossing representation and eventually to design a new , $discourse level of annotation$ .	0	used-for	used-for	1
in this paper , we propose a feasible process of such a transfer , comparing the possibilities the $praguian dependency-based approach$ offers with the #penn discourse annotation# based primarily on the analysis and classification of discourse connectives .	6	compare	compare	1
in this paper , we propose a feasible process of such a transfer , comparing the possibilities the $praguian dependency-based approach$ offers with the penn discourse annotation based primarily on the #analysis and classification of discourse connectives# .	2	evaluate-for	evaluate-for	1
in this paper , we propose a feasible process of such a transfer , comparing the possibilities the praguian dependency-based approach offers with the $penn discourse annotation$ based primarily on the #analysis and classification of discourse connectives# .	2	evaluate-for	evaluate-for	1
#regression-based techniques# have shown promising results for $people counting in crowded scenes$ .	0	used-for	used-for	1
however , most existing $techniques$ require expensive and laborious #data annotation# for model training .	0	used-for	used-for	1
however , most existing techniques require expensive and laborious #data annotation# for $model training$ .	0	used-for	used-for	1
-lrb- 2 -rrb- rather than learning from only #labelled data# , the $abundant unlabelled data$ are exploited .	6	compare	compare	1
all three ideas are implemented in a #unified active and semi-supervised regression framework# with ability to perform $transfer learning$ , by exploiting the underlying geometric structure of crowd patterns via manifold analysis .	0	used-for	used-for	1
all three ideas are implemented in a $unified active and semi-supervised regression framework$ with ability to perform transfer learning , by exploiting the underlying #geometric structure of crowd patterns# via manifold analysis .	0	used-for	used-for	1
all three ideas are implemented in a unified active and semi-supervised regression framework with ability to perform transfer learning , by exploiting the underlying $geometric structure of crowd patterns$ via #manifold analysis# .	0	used-for	used-for	1
#representing images with layers# has many important $applications$ , such as video compression , motion analysis , and 3d scene analysis .	0	used-for	used-for	1
representing images with layers has many important $applications$ , such as #video compression# , motion analysis , and 3d scene analysis .	3	hyponym-of	hyponym-of	1
representing images with layers has many important applications , such as #video compression# , $motion analysis$ , and 3d scene analysis .	1	conjunction	conjunction	1
representing images with layers has many important $applications$ , such as video compression , #motion analysis# , and 3d scene analysis .	3	hyponym-of	hyponym-of	1
representing images with layers has many important applications , such as video compression , #motion analysis# , and $3d scene analysis$ .	1	conjunction	conjunction	1
representing images with layers has many important $applications$ , such as video compression , motion analysis , and #3d scene analysis# .	3	hyponym-of	hyponym-of	1
this paper presents an #approach# to reliably extracting $layers$ from images by taking advantages of the fact that homographies induced by planar patches in the scene form a low dimensional linear subspace .	0	used-for	used-for	1
this paper presents an approach to reliably extracting #layers# from $images$ by taking advantages of the fact that homographies induced by planar patches in the scene form a low dimensional linear subspace .	4	part-of	part-of	1
this paper presents an approach to reliably extracting layers from images by taking advantages of the fact that homographies induced by #planar patches# in the $scene$ form a low dimensional linear subspace .	4	part-of	part-of	1
#layers# in the input $images$ will be mapped in the subspace , where it is proven that they form well-defined clusters and can be reliably identified by a simple mean-shift based clustering algorithm .	4	part-of	part-of	1
layers in the input #images# will be mapped in the subspace , where it is proven that they form well-defined $clusters$ and can be reliably identified by a simple mean-shift based clustering algorithm .	0	used-for	used-for	1
layers in the input images will be mapped in the subspace , where it is proven that they form well-defined $clusters$ and can be reliably identified by a simple #mean-shift based clustering algorithm# .	0	used-for	used-for	1
global optimality is achieved since all valid regions are simultaneously taken into account , and $noise$ can be effectively reduced by enforcing the #subspace constraint# .	0	used-for	used-for	1
the $construction of causal graphs$ from #non-experimental data# rests on a set of constraints that the graph structure imposes on all probability distributions compatible with the graph .	0	used-for	used-for	1
the construction of causal graphs from non-experimental data rests on a set of constraints that the graph structure imposes on all #probability distributions# compatible with the $graph$ .	5	feature-of	feature-of	1
these $constraints$ are of two types : #conditional inde-pendencies# and algebraic constraints , first noted by verma .	3	hyponym-of	hyponym-of	1
these $constraints$ are of two types : conditional inde-pendencies and #algebraic constraints# , first noted by verma .	3	hyponym-of	hyponym-of	1
while #conditional independencies# are well studied and frequently used in $causal induction algorithms$ , verma constraints are still poorly understood , and rarely applied .	0	used-for	used-for	1
while $conditional independencies$ are well studied and frequently used in causal induction algorithms , #verma constraints# are still poorly understood , and rarely applied .	6	compare	compare	1
in this paper we examine a special subset of verma constraints which are easy to understand , easy to identify and easy to apply ; they arise from '' #dormant independencies# , '' namely , $conditional independencies$ that hold in interventional distributions .	1	conjunction	conjunction	1
in this paper we examine a special subset of verma constraints which are easy to understand , easy to identify and easy to apply ; they arise from '' dormant independencies , '' namely , #conditional independencies# that hold in $interventional distributions$ .	5	feature-of	feature-of	1
we give a complete #algorithm# for determining if a $dormant independence$ between two sets of variables is entailed by the causal graph , such that this independence is identifiable , in other words if it resides in an interventional distribution that can be predicted without resorting to interventions .	0	used-for	used-for	1
we give a complete algorithm for determining if a dormant independence between two sets of variables is entailed by the causal graph , such that this independence is identifiable , in other words if $it$ resides in an #interventional distribution# that can be predicted without resorting to interventions .	5	feature-of	feature-of	1
we further show the usefulness of #dormant independencies# in $model testing$ and induction by giving an algorithm that uses constraints entailed by dormant independencies to prune extraneous edges from a given causal graph .	0	used-for	used-for	1
we further show the usefulness of #dormant independencies# in model testing and $induction$ by giving an algorithm that uses constraints entailed by dormant independencies to prune extraneous edges from a given causal graph .	0	used-for	used-for	1
we further show the usefulness of dormant independencies in #model testing# and $induction$ by giving an algorithm that uses constraints entailed by dormant independencies to prune extraneous edges from a given causal graph .	1	conjunction	conjunction	1
we further show the usefulness of dormant independencies in model testing and induction by giving an #algorithm# that uses constraints entailed by dormant independencies to prune $extraneous edges$ from a given causal graph .	0	used-for	used-for	1
we further show the usefulness of dormant independencies in model testing and induction by giving an $algorithm$ that uses #constraints# entailed by dormant independencies to prune extraneous edges from a given causal graph .	0	used-for	used-for	1
we further show the usefulness of dormant independencies in model testing and induction by giving an algorithm that uses constraints entailed by dormant independencies to prune #extraneous edges# from a given $causal graph$ .	4	part-of	part-of	1
with the recent popularity of $animated gifs$ on #social media# , there is need for ways to index them with rich meta-data .	5	feature-of	feature-of	1
to advance research on $animated gif understanding$ , we collected a new #dataset# , tumblr gif -lrb- tgif -rrb- , with 100k animated gifs from tumblr and 120k natural language descriptions obtained via crowdsourcing .	0	used-for	used-for	1
to advance research on animated gif understanding , we collected a new dataset , tumblr gif -lrb- tgif -rrb- , with 100k $animated gifs$ from tumblr and 120k #natural language descriptions# obtained via crowdsourcing .	1	conjunction	conjunction	1
to advance research on animated gif understanding , we collected a new dataset , tumblr gif -lrb- tgif -rrb- , with 100k animated gifs from tumblr and 120k $natural language descriptions$ obtained via #crowdsourcing# .	0	used-for	used-for	1
the motivation for this work is to develop a testbed for image sequence description systems , where the task is to generate #natural language descriptions# for $animated gifs$ or video clips .	0	used-for	used-for	1
the motivation for this work is to develop a testbed for image sequence description systems , where the task is to generate #natural language descriptions# for animated gifs or $video clips$ .	0	used-for	used-for	1
the motivation for this work is to develop a testbed for image sequence description systems , where the task is to generate natural language descriptions for #animated gifs# or $video clips$ .	1	conjunction	conjunction	1
to ensure a high quality dataset , we developed a series of novel #quality controls# to validate $free-form text input$ from crowd-workers .	0	used-for	used-for	1
we show that there is unambiguous association between #visual content# and $natural language descriptions$ in our dataset , making it an ideal benchmark for the visual content captioning task .	1	conjunction	conjunction	1
we show that there is unambiguous association between #visual content# and natural language descriptions in our $dataset$ , making it an ideal benchmark for the visual content captioning task .	4	part-of	part-of	1
we show that there is unambiguous association between visual content and #natural language descriptions# in our $dataset$ , making it an ideal benchmark for the visual content captioning task .	4	part-of	part-of	1
we show that there is unambiguous association between visual content and natural language descriptions in our dataset , making #it# an ideal benchmark for the $visual content captioning task$ .	2	evaluate-for	evaluate-for	1
we perform extensive statistical analyses to compare our #dataset# to existing $image and video description datasets$ .	6	compare	compare	1
next , we provide baseline results on the $animated gif description task$ , using three #representative techniques# : nearest neighbor , statistical machine translation , and recurrent neural networks .	0	used-for	used-for	1
next , we provide baseline results on the animated gif description task , using three $representative techniques$ : #nearest neighbor# , statistical machine translation , and recurrent neural networks .	3	hyponym-of	hyponym-of	1
next , we provide baseline results on the animated gif description task , using three representative techniques : #nearest neighbor# , $statistical machine translation$ , and recurrent neural networks .	1	conjunction	conjunction	1
next , we provide baseline results on the animated gif description task , using three $representative techniques$ : nearest neighbor , #statistical machine translation# , and recurrent neural networks .	3	hyponym-of	hyponym-of	1
next , we provide baseline results on the animated gif description task , using three representative techniques : nearest neighbor , #statistical machine translation# , and $recurrent neural networks$ .	1	conjunction	conjunction	1
next , we provide baseline results on the animated gif description task , using three $representative techniques$ : nearest neighbor , statistical machine translation , and #recurrent neural networks# .	3	hyponym-of	hyponym-of	1
finally , we show that models fine-tuned from our #animated gif description dataset# can be helpful for $automatic movie description$ .	0	used-for	used-for	1
#systemic grammar# has been used for $ai text generation$ work in the past , but the implementations have tended be ad hoc or inefficient .	0	used-for	used-for	1
this paper presents an #approach# to systemic $text generation$ where ai problem solving techniques are applied directly to an unadulterated systemic grammar .	0	used-for	used-for	1
this paper presents an approach to systemic text generation where #ai problem solving techniques# are applied directly to an unadulterated $systemic grammar$ .	0	used-for	used-for	1
this approach is made possible by a special relationship between #systemic grammar# and $problem solving$ : both are organized primarily as choosing from alternatives .	1	conjunction	conjunction	1
the result is simple , efficient $text generation$ firmly based in a #linguistic theory# .	0	used-for	used-for	1
in this paper a novel #solution# to $automatic and unsupervised word sense induction -lrb- wsi -rrb-$ is introduced .	0	used-for	used-for	1
#it# represents an instantiation of the $one sense per collocation observation$ -lrb- gale et al. , 1992 -rrb- .	3	hyponym-of	hyponym-of	1
like most existing approaches $it$ utilizes #clustering of word co-occurrences# .	0	used-for	used-for	1
this #approach# differs from other $approaches$ to wsi in that it enhances the effect of the one sense per collocation observation by using triplets of words instead of pairs .	6	compare	compare	1
this #approach# differs from other approaches to $wsi$ in that it enhances the effect of the one sense per collocation observation by using triplets of words instead of pairs .	0	used-for	used-for	1
this approach differs from other #approaches# to $wsi$ in that it enhances the effect of the one sense per collocation observation by using triplets of words instead of pairs .	0	used-for	used-for	1
this approach differs from other approaches to wsi in that #it# enhances the effect of the $one sense per collocation observation$ by using triplets of words instead of pairs .	0	used-for	used-for	1
this approach differs from other approaches to wsi in that $it$ enhances the effect of the one sense per collocation observation by using #triplets of words# instead of pairs .	0	used-for	used-for	1
the combination with a $two-step clustering process$ using #sentence co-occurrences# as features allows for accurate results .	0	used-for	used-for	1
additionally , a novel and likewise #automatic and unsupervised evaluation method# inspired by schutze 's -lrb- 1992 -rrb- idea of evaluation of $word sense disambiguation algorithms$ is employed .	2	evaluate-for	evaluate-for	1
offering advantages like reproducability and independency of a given biased gold standard it also enables #automatic parameter optimization# of the $wsi algorithm$ .	0	used-for	used-for	1
this abstract describes a #natural language system# which deals usefully with $ungrammatical input$ and describes some actual and potential applications of it in computer aided second language learning .	0	used-for	used-for	1
this abstract describes a natural language system which deals usefully with ungrammatical input and describes some actual and potential applications of #it# in $computer aided second language learning$ .	0	used-for	used-for	1
however , $this$ is not the only area in which the principles of the #system# might be used , and the aim in building it was simply to demonstrate the workability of the general mechanism , and provide a framework for assessing developments of it .	0	used-for	used-for	1
in a motorized vehicle a number of easily $measurable signals$ with #frequency components# related to the rotational speed of the engine can be found , e.g. , vibrations , electrical system voltage level , and ambient sound .	0	used-for	used-for	1
in a motorized vehicle a number of easily measurable signals with #frequency components# related to the $rotational speed of the engine$ can be found , e.g. , vibrations , electrical system voltage level , and ambient sound .	5	feature-of	feature-of	1
in a motorized vehicle a number of easily $measurable signals$ with frequency components related to the rotational speed of the engine can be found , e.g. , #vibrations# , electrical system voltage level , and ambient sound .	3	hyponym-of	hyponym-of	1
in a motorized vehicle a number of easily measurable signals with frequency components related to the rotational speed of the engine can be found , e.g. , #vibrations# , $electrical system voltage level$ , and ambient sound .	1	conjunction	conjunction	1
in a motorized vehicle a number of easily $measurable signals$ with frequency components related to the rotational speed of the engine can be found , e.g. , vibrations , #electrical system voltage level# , and ambient sound .	3	hyponym-of	hyponym-of	1
in a motorized vehicle a number of easily measurable signals with frequency components related to the rotational speed of the engine can be found , e.g. , vibrations , #electrical system voltage level# , and $ambient sound$ .	1	conjunction	conjunction	1
in a motorized vehicle a number of easily $measurable signals$ with frequency components related to the rotational speed of the engine can be found , e.g. , vibrations , electrical system voltage level , and #ambient sound# .	3	hyponym-of	hyponym-of	1
these #signals# could potentially be used to estimate the $speed and related states of the vehicle$ .	0	used-for	used-for	1
unfortunately , such estimates would typically require the relations -lrb- scale factors -rrb- between the #frequency components# and the $speed$ for different gears to be known .	1	conjunction	conjunction	1
unfortunately , such estimates would typically require the relations -lrb- scale factors -rrb- between the frequency components and the #speed# for different $gears$ to be known .	5	feature-of	feature-of	1
consequently , in this article we look at the problem of estimating these $gear scale factors$ from #training data# consisting only of speed measurements and measurements of the signal in question .	0	used-for	used-for	1
the $estimation problem$ is formulated as a #maximum likelihood estimation problem# and heuristics is used to find initial values for a numerical evaluation of the estimator .	0	used-for	used-for	1
the estimation problem is formulated as a maximum likelihood estimation problem and #heuristics# is used to find initial values for a $numerical evaluation of the estimator$ .	0	used-for	used-for	1
finally , a measurement campaign is conducted and the functionality of the $estimation method$ is verified on #real data# .	2	evaluate-for	evaluate-for	1
$lpc based speech coders$ operating at #bit rates# below 3.0 kbits/sec are usually associated with buzzy or metallic artefacts in the synthetic speech .	5	feature-of	feature-of	1
lpc based speech coders operating at bit rates below 3.0 kbits/sec are usually associated with #buzzy or metallic artefacts# in the $synthetic speech$ .	5	feature-of	feature-of	1
in this paper a new lpc vocoder is presented which splits the $lpc excitation$ into two frequency bands using a #variable cutoff frequency# .	0	used-for	used-for	1
in this paper a new lpc vocoder is presented which splits the lpc excitation into two $frequency bands$ using a #variable cutoff frequency# .	0	used-for	used-for	1
in doing so the #coder# 's performance during both mixed voicing speech and speech containing acoustic noise is greatly improved , producing $soft natural sounding speech$ .	0	used-for	used-for	1
in doing so the $coder$ 's performance during both #mixed voicing speech# and speech containing acoustic noise is greatly improved , producing soft natural sounding speech .	0	used-for	used-for	1
in doing so the $coder$ 's performance during both mixed voicing speech and #speech containing acoustic noise# is greatly improved , producing soft natural sounding speech .	0	used-for	used-for	1
the paper also describes new #parameter determination# and $quantisation techniques$ vital to the operation of this coder at such low bit rates .	1	conjunction	conjunction	1
the paper also describes new #parameter determination# and quantisation techniques vital to the operation of this $coder$ at such low bit rates .	0	used-for	used-for	1
the paper also describes new parameter determination and #quantisation techniques# vital to the operation of this $coder$ at such low bit rates .	0	used-for	used-for	1
the paper also describes new parameter determination and quantisation techniques vital to the operation of this $coder$ at such #low bit rates# .	5	feature-of	feature-of	1
we consider a problem of $blind source separation$ from a set of #instantaneous linear mixtures# , where the mixing matrix is unknown .	0	used-for	used-for	1
it was discovered recently , that exploiting the $sparsity of sources$ in an appropriate representation according to some #signal dictionary# , dramatically improves the quality of separation .	0	used-for	used-for	1
it was discovered recently , that exploiting the $sparsity of sources$ in an appropriate representation according to some signal dictionary , dramatically improves the #quality of separation# .	2	evaluate-for	evaluate-for	1
in this work we use the property of $multi scale transforms$ , such as #wavelet or wavelet packets# , to decompose signals into sets of local features with various degrees of sparsity .	3	hyponym-of	hyponym-of	1
the performance of the $algorithm$ is verified on #noise-free and noisy data# .	2	evaluate-for	evaluate-for	1
experiments with #simulated signals# , $musical sounds$ and images demonstrate significant improvement of separation quality over previously reported results .	1	conjunction	conjunction	1
experiments with #simulated signals# , musical sounds and images demonstrate significant improvement of $separation quality$ over previously reported results .	2	evaluate-for	evaluate-for	1
experiments with simulated signals , #musical sounds# and $images$ demonstrate significant improvement of separation quality over previously reported results .	1	conjunction	conjunction	1
experiments with simulated signals , #musical sounds# and images demonstrate significant improvement of $separation quality$ over previously reported results .	2	evaluate-for	evaluate-for	1
experiments with simulated signals , musical sounds and #images# demonstrate significant improvement of $separation quality$ over previously reported results .	2	evaluate-for	evaluate-for	1
in this paper , we explore $multilingual feature-level data sharing$ via #deep neural network -lrb- dnn -rrb- stacked bottleneck features# .	0	used-for	used-for	1
given a set of available source languages , we apply #language identification# to pick the language most similar to the target language , for more efficient use of $multilingual resources$ .	0	used-for	used-for	1
our experiments with iarpa-babel languages show that $bottleneck features$ trained on the most similar source language perform better than #those# trained on all available source languages .	6	compare	compare	1
further analysis suggests that only #data# similar to the target language is useful for $multilingual training$ .	0	used-for	used-for	1
this article introduces a #bidirectional grammar generation system# called feature structure-directed generation , developed for a $dialogue translation system$ .	0	used-for	used-for	1
this article introduces a $bidirectional grammar generation system$ called #feature structure-directed generation# , developed for a dialogue translation system .	3	hyponym-of	hyponym-of	1
this article introduces a bidirectional grammar generation system called #feature structure-directed generation# , developed for a $dialogue translation system$ .	0	used-for	used-for	1
the $system$ utilizes #typed feature structures# to control the top-down derivation in a declarative way .	0	used-for	used-for	1
the system utilizes #typed feature structures# to control the $top-down derivation$ in a declarative way .	0	used-for	used-for	1
this $generation system$ also uses #disjunctive feature structures# to reduce the number of copies of the derivation tree .	0	used-for	used-for	1
this generation system also uses #disjunctive feature structures# to reduce the number of copies of the $derivation tree$ .	0	used-for	used-for	1
the #grammar# for this $generator$ is designed to properly generate the speaker 's intention in a telephone dialogue .	0	used-for	used-for	1
the #grammar# for this generator is designed to properly generate the $speaker 's intention$ in a telephone dialogue .	0	used-for	used-for	1
the grammar for this generator is designed to properly generate the $speaker 's intention$ in a #telephone dialogue# .	5	feature-of	feature-of	1
#automatic image annotation# is a newly developed and promising technique to provide $semantic image retrieval$ via text descriptions .	0	used-for	used-for	1
automatic image annotation is a newly developed and promising technique to provide $semantic image retrieval$ via #text descriptions# .	0	used-for	used-for	1
it concerns a process of $automatically labeling the image contents$ with a pre-defined set of #keywords# which are exploited to represent the image semantics .	0	used-for	used-for	1
it concerns a process of automatically labeling the image contents with a pre-defined set of #keywords# which are exploited to represent the $image semantics$ .	0	used-for	used-for	1
a #maximum entropy model-based approach# to the task of $automatic image annotation$ is proposed in this paper .	0	used-for	used-for	1
in the phase of training , a basic #visual vocabulary# consisting of blob-tokens to describe the $image content$ is generated at first ; then the statistical relationship is modeled between the blob-tokens and keywords by a maximum entropy model constructed from the training set of labeled images .	0	used-for	used-for	1
in the phase of training , a basic $visual vocabulary$ consisting of #blob-tokens# to describe the image content is generated at first ; then the statistical relationship is modeled between the blob-tokens and keywords by a maximum entropy model constructed from the training set of labeled images .	4	part-of	part-of	1
in the phase of training , a basic visual vocabulary consisting of blob-tokens to describe the image content is generated at first ; then the $statistical relationship$ is modeled between the blob-tokens and keywords by a #maximum entropy model# constructed from the training set of labeled images .	0	used-for	used-for	1
in the phase of annotation , for an unlabeled image , the most likely associated $keywords$ are predicted in terms of the #blob-token set# extracted from the given image .	0	used-for	used-for	1
we carried out experiments on a $medium-sized image collection$ with about 5000 images from #corel photo cds# .	0	used-for	used-for	1
the experimental results demonstrated that the $annotation$ performance of this #method# outperforms some traditional annotation methods by about 8 % in mean precision , showing a potential of the maximum entropy model in the task of automatic image annotation .	0	used-for	used-for	1
the experimental results demonstrated that the annotation performance of this #method# outperforms some traditional $annotation methods$ by about 8 % in mean precision , showing a potential of the maximum entropy model in the task of automatic image annotation .	6	compare	compare	1
the experimental results demonstrated that the $annotation$ performance of this method outperforms some traditional #annotation methods# by about 8 % in mean precision , showing a potential of the maximum entropy model in the task of automatic image annotation .	0	used-for	used-for	1
the experimental results demonstrated that the annotation performance of this method outperforms some traditional $annotation methods$ by about 8 % in #mean precision# , showing a potential of the maximum entropy model in the task of automatic image annotation .	2	evaluate-for	evaluate-for	1
the experimental results demonstrated that the annotation performance of this method outperforms some traditional annotation methods by about 8 % in mean precision , showing a potential of the #maximum entropy model# in the task of $automatic image annotation$ .	0	used-for	used-for	1
however most of the works found in the literature have focused on identifying and understanding $temporal expressions$ in #newswire texts# .	5	feature-of	feature-of	1
in this paper we report our work on anchoring $temporal expressions$ in a novel genre , #emails# .	5	feature-of	feature-of	1
the highly under-specified nature of these expressions fits well with our $constraint-based representation of time$ , #time calculus for natural language -lrb- tcnl -rrb-# .	3	hyponym-of	hyponym-of	1
we have developed and evaluated a temporal expression anchoror -lrb- tea -rrb- , and the result shows that #it# performs significantly better than the $baseline$ , and compares favorably with some of the closely related work .	6	compare	compare	1
we address the problem of populating #object category detection datasets# with dense , $per-object 3d reconstructions$ , bootstrapped from class labels , ground truth figure-ground segmentations and a small set of keypoint annotations .	0	used-for	used-for	1
we address the problem of populating object category detection datasets with dense , $per-object 3d reconstructions$ , bootstrapped from class labels , #ground truth figure-ground segmentations# and a small set of keypoint annotations .	0	used-for	used-for	1
we address the problem of populating object category detection datasets with dense , per-object 3d reconstructions , bootstrapped from class labels , #ground truth figure-ground segmentations# and a small set of $keypoint annotations$ .	1	conjunction	conjunction	1
we address the problem of populating object category detection datasets with dense , $per-object 3d reconstructions$ , bootstrapped from class labels , ground truth figure-ground segmentations and a small set of #keypoint annotations# .	0	used-for	used-for	1
our proposed #algorithm# first estimates $camera viewpoint$ using rigid structure-from-motion , then reconstructs object shapes by optimizing over visual hull proposals guided by loose within-class shape similarity assumptions .	0	used-for	used-for	1
our proposed #algorithm# first estimates camera viewpoint using rigid structure-from-motion , then reconstructs $object shapes$ by optimizing over visual hull proposals guided by loose within-class shape similarity assumptions .	0	used-for	used-for	1
our proposed $algorithm$ first estimates camera viewpoint using #rigid structure-from-motion# , then reconstructs object shapes by optimizing over visual hull proposals guided by loose within-class shape similarity assumptions .	0	used-for	used-for	1
our proposed algorithm first estimates camera viewpoint using rigid structure-from-motion , then reconstructs $object shapes$ by optimizing over #visual hull proposals# guided by loose within-class shape similarity assumptions .	0	used-for	used-for	1
our proposed algorithm first estimates camera viewpoint using rigid structure-from-motion , then reconstructs object shapes by optimizing over $visual hull proposals$ guided by #loose within-class shape similarity assumptions# .	0	used-for	used-for	1
we show that our #method# is able to produce convincing $per-object 3d reconstructions$ on one of the most challenging existing object-category detection datasets , pascal voc .	0	used-for	used-for	1
we show that our $method$ is able to produce convincing per-object 3d reconstructions on one of the most challenging existing #object-category detection datasets# , pascal voc .	0	used-for	used-for	1
we show that our method is able to produce convincing per-object 3d reconstructions on one of the most challenging existing $object-category detection datasets$ , #pascal voc# .	3	hyponym-of	hyponym-of	1
#probabilistic models# have been previously shown to be efficient and effective for $modeling and recognition of human motion$ .	0	used-for	used-for	1
in particular we focus on methods which represent the $human motion model$ as a #triangulated graph# .	0	used-for	used-for	1
previous approaches learned $models$ based just on #positions# and velocities of the body parts while ignoring their appearance .	0	used-for	used-for	1
previous approaches learned models based just on #positions# and $velocities$ of the body parts while ignoring their appearance .	1	conjunction	conjunction	1
previous approaches learned $models$ based just on positions and #velocities# of the body parts while ignoring their appearance .	0	used-for	used-for	1
moreover , a #heuristic approach# was commonly used to obtain $translation invariance$ .	0	used-for	used-for	1
in this paper we suggest an improved #approach# for learning such $models$ and using them for human motion recognition .	0	used-for	used-for	1
in this paper we suggest an improved approach for learning such models and using #them# for $human motion recognition$ .	0	used-for	used-for	1
the suggested #approach# combines multiple cues , i.e. , positions , velocities and appearance into both the $learning and detection phases$ .	0	used-for	used-for	1
the suggested approach combines multiple $cues$ , i.e. , #positions# , velocities and appearance into both the learning and detection phases .	3	hyponym-of	hyponym-of	1
the suggested approach combines multiple cues , i.e. , #positions# , $velocities$ and appearance into both the learning and detection phases .	1	conjunction	conjunction	1
the suggested approach combines multiple $cues$ , i.e. , positions , #velocities# and appearance into both the learning and detection phases .	3	hyponym-of	hyponym-of	1
the suggested approach combines multiple cues , i.e. , positions , #velocities# and $appearance$ into both the learning and detection phases .	1	conjunction	conjunction	1
the suggested approach combines multiple $cues$ , i.e. , positions , velocities and #appearance# into both the learning and detection phases .	3	hyponym-of	hyponym-of	1
furthermore , we introduce #global variables# in the $model$ , which can represent global properties such as translation , scale or viewpoint .	0	used-for	used-for	1
furthermore , we introduce #global variables# in the model , which can represent $global properties$ such as translation , scale or viewpoint .	0	used-for	used-for	1
furthermore , we introduce global variables in the model , which can represent $global properties$ such as #translation# , scale or viewpoint .	3	hyponym-of	hyponym-of	1
furthermore , we introduce global variables in the model , which can represent global properties such as #translation# , $scale$ or viewpoint .	1	conjunction	conjunction	1
furthermore , we introduce global variables in the model , which can represent $global properties$ such as translation , #scale# or viewpoint .	3	hyponym-of	hyponym-of	1
furthermore , we introduce global variables in the model , which can represent global properties such as translation , #scale# or $viewpoint$ .	1	conjunction	conjunction	1
furthermore , we introduce global variables in the model , which can represent $global properties$ such as translation , scale or #viewpoint# .	3	hyponym-of	hyponym-of	1
the $model$ is learned in an #unsupervised manner# from un-labelled data .	0	used-for	used-for	1
the model is learned in an $unsupervised manner$ from #un-labelled data# .	0	used-for	used-for	1
we show that the suggested $hybrid proba-bilistic model$ -lrb- which combines #global variables# , like translation , with local variables , like relative positions and appearances of body parts -rrb- , leads to : -lrb- i -rrb- faster convergence of learning phase , -lrb- ii -rrb- robustness to occlusions , and , -lrb- iii -rrb- higher recognition rate .	0	used-for	used-for	1
we show that the suggested hybrid proba-bilistic model -lrb- which combines $global variables$ , like #translation# , with local variables , like relative positions and appearances of body parts -rrb- , leads to : -lrb- i -rrb- faster convergence of learning phase , -lrb- ii -rrb- robustness to occlusions , and , -lrb- iii -rrb- higher recognition rate .	3	hyponym-of	hyponym-of	1
we show that the suggested hybrid proba-bilistic model -lrb- which combines global variables , like translation , with $local variables$ , like #relative positions# and appearances of body parts -rrb- , leads to : -lrb- i -rrb- faster convergence of learning phase , -lrb- ii -rrb- robustness to occlusions , and , -lrb- iii -rrb- higher recognition rate .	3	hyponym-of	hyponym-of	1
we show that the suggested hybrid proba-bilistic model -lrb- which combines global variables , like translation , with local variables , like #relative positions# and $appearances of body parts$ -rrb- , leads to : -lrb- i -rrb- faster convergence of learning phase , -lrb- ii -rrb- robustness to occlusions , and , -lrb- iii -rrb- higher recognition rate .	1	conjunction	conjunction	1
we show that the suggested hybrid proba-bilistic model -lrb- which combines global variables , like translation , with $local variables$ , like relative positions and #appearances of body parts# -rrb- , leads to : -lrb- i -rrb- faster convergence of learning phase , -lrb- ii -rrb- robustness to occlusions , and , -lrb- iii -rrb- higher recognition rate .	3	hyponym-of	hyponym-of	1
we show that the suggested hybrid proba-bilistic model -lrb- which combines global variables , like translation , with local variables , like relative positions and appearances of body parts -rrb- , leads to : -lrb- i -rrb- #faster convergence# of $learning phase$ , -lrb- ii -rrb- robustness to occlusions , and , -lrb- iii -rrb- higher recognition rate .	5	feature-of	feature-of	1
we show that the suggested hybrid proba-bilistic model -lrb- which combines global variables , like translation , with local variables , like relative positions and appearances of body parts -rrb- , leads to : -lrb- i -rrb- #faster convergence# of learning phase , -lrb- ii -rrb- $robustness$ to occlusions , and , -lrb- iii -rrb- higher recognition rate .	1	conjunction	conjunction	1
we show that the suggested hybrid proba-bilistic model -lrb- which combines global variables , like translation , with local variables , like relative positions and appearances of body parts -rrb- , leads to : -lrb- i -rrb- faster convergence of learning phase , -lrb- ii -rrb- #robustness# to occlusions , and , -lrb- iii -rrb- higher $recognition rate$ .	1	conjunction	conjunction	1
#factor analysis# and $principal components analysis$ can be used to model linear relationships between observed variables and linearly map high-dimensional data to a lower-dimensional hidden space .	1	conjunction	conjunction	1
#factor analysis# and principal components analysis can be used to model $linear relationships between observed variables$ and linearly map high-dimensional data to a lower-dimensional hidden space .	0	used-for	used-for	1
factor analysis and #principal components analysis# can be used to model $linear relationships between observed variables$ and linearly map high-dimensional data to a lower-dimensional hidden space .	0	used-for	used-for	1
we describe a #nonlinear generalization of factor analysis# , called `` product analy-sis '' , that models the $observed variables$ as a linear combination of products of normally distributed hidden variables .	0	used-for	used-for	1
we describe a $nonlinear generalization of factor analysis$ , called #`` product analy-sis ''# , that models the observed variables as a linear combination of products of normally distributed hidden variables .	3	hyponym-of	hyponym-of	1
we describe a $nonlinear generalization of factor analysis$ , called `` product analy-sis '' , that models the observed variables as a #linear combination of products of normally distributed hidden variables# .	0	used-for	used-for	1
just as $factor analysis$ can be viewed as #unsupervised linear regression# on unobserved , normally distributed hidden variables , product analysis can be viewed as unsupervised linear regression on products of unobserved , normally distributed hidden variables .	0	used-for	used-for	1
just as factor analysis can be viewed as unsupervised linear regression on unobserved , normally distributed hidden variables , $product analysis$ can be viewed as #unsupervised linear regression# on products of unobserved , normally distributed hidden variables .	0	used-for	used-for	1
the mapping between the data and the hidden space is nonlinear , so we use an #approximate variational technique# for $inference$ and learning .	0	used-for	used-for	1
the mapping between the data and the hidden space is nonlinear , so we use an #approximate variational technique# for inference and $learning$ .	0	used-for	used-for	1
the mapping between the data and the hidden space is nonlinear , so we use an approximate variational technique for #inference# and $learning$ .	1	conjunction	conjunction	1
since #product analysis# is a $generalization of factor analysis$ , product analysis always finds a higher data likelihood than factor analysis .	3	hyponym-of	hyponym-of	1
since product analysis is a generalization of factor analysis , #product analysis# always finds a higher data likelihood than $factor analysis$ .	6	compare	compare	1
we give results on #pattern recognition# and $illumination-invariant image clustering$ .	1	conjunction	conjunction	1
this paper describes a #domain independent strategy# for the $multimedia articulation of answers$ elicited by a natural language interface to database query applications .	0	used-for	used-for	1
this paper describes a domain independent strategy for the #multimedia articulation of answers# elicited by a $natural language interface$ to database query applications .	0	used-for	used-for	1
this paper describes a domain independent strategy for the multimedia articulation of answers elicited by a #natural language interface# to $database query applications$ .	0	used-for	used-for	1
$multimedia answers$ include #videodisc images# and heuristically-produced complete sentences in text or text-to-speech form .	4	part-of	part-of	1
#deictic reference# and $feedback$ about the discourse are enabled .	1	conjunction	conjunction	1
#deictic reference# and feedback about the $discourse$ are enabled .	5	feature-of	feature-of	1
deictic reference and #feedback# about the $discourse$ are enabled .	5	feature-of	feature-of	1
the #logon mt demonstrator# assembles independently valuable $general-purpose nlp components$ into a machine translation pipeline that capitalizes on output quality .	0	used-for	used-for	1
the logon mt demonstrator assembles independently valuable #general-purpose nlp components# into a $machine translation pipeline$ that capitalizes on output quality .	4	part-of	part-of	1
the $demonstrator$ embodies an interesting combination of #hand-built , symbolic resources# and stochastic processes .	4	part-of	part-of	1
the demonstrator embodies an interesting combination of #hand-built , symbolic resources# and $stochastic processes$ .	1	conjunction	conjunction	1
the $demonstrator$ embodies an interesting combination of hand-built , symbolic resources and #stochastic processes# .	4	part-of	part-of	1
we describe both the #syntax# and $semantics$ of a general propositional language of context , and give a hilbert style proof system for this language .	1	conjunction	conjunction	1
we describe both the #syntax# and semantics of a general $propositional language of context$ , and give a hilbert style proof system for this language .	5	feature-of	feature-of	1
we describe both the syntax and #semantics# of a general $propositional language of context$ , and give a hilbert style proof system for this language .	5	feature-of	feature-of	1
we describe both the syntax and semantics of a general propositional language of context , and give a #hilbert style proof system# for this $language$ .	0	used-for	used-for	1
a $propositional logic of context$ extends #classical propositional logic# in two ways .	0	used-for	used-for	1
#image matching# is a fundamental problem in $computer vision$ .	3	hyponym-of	hyponym-of	1
in the context of $feature-based matching$ , #sift# and its variants have long excelled in a wide array of applications .	0	used-for	used-for	1
however , for ultra-wide baselines , as in the case of $aerial images$ captured under #large camera rotations# , the appearance variation goes beyond the reach of sift and ransac .	5	feature-of	feature-of	1
however , for ultra-wide baselines , as in the case of aerial images captured under large camera rotations , the appearance variation goes beyond the reach of #sift# and $ransac$ .	1	conjunction	conjunction	1
in this paper we propose a data-driven , deep learning-based approach that sidesteps local correspondence by framing the $problem$ as a #classification task# .	0	used-for	used-for	1
we train our $models$ on a #dataset of urban aerial imagery# consisting of ` same ' and ` different ' pairs , collected for this purpose , and characterize the problem via a human study with annotations from amazon mechanical turk .	0	used-for	used-for	1
we train our models on a dataset of urban aerial imagery consisting of ` same ' and ` different ' pairs , collected for this purpose , and characterize the $problem$ via a #human study# with annotations from amazon mechanical turk .	0	used-for	used-for	1
we train our models on a dataset of urban aerial imagery consisting of ` same ' and ` different ' pairs , collected for this purpose , and characterize the problem via a $human study$ with #annotations from amazon mechanical turk# .	0	used-for	used-for	1
we demonstrate that our #models# outperform the $state-of-the-art$ on ultra-wide baseline matching and approach human accuracy .	6	compare	compare	1
we demonstrate that our #models# outperform the state-of-the-art on ultra-wide baseline matching and approach $human accuracy$ .	6	compare	compare	1
we demonstrate that our $models$ outperform the state-of-the-art on #ultra-wide baseline matching# and approach human accuracy .	2	evaluate-for	evaluate-for	1
we demonstrate that our models outperform the $state-of-the-art$ on #ultra-wide baseline matching# and approach human accuracy .	2	evaluate-for	evaluate-for	1
we argue that a more sophisticated and #fine-grained annotation# in the tree-bank would have very positve effects on $stochastic parsers$ trained on the tree-bank and on grammars induced from the treebank , and it would make the treebank more valuable as a source of data for theoretical linguistic investigations .	0	used-for	used-for	1
we argue that a more sophisticated and fine-grained annotation in the tree-bank would have very positve effects on $stochastic parsers$ trained on the #tree-bank# and on grammars induced from the treebank , and it would make the treebank more valuable as a source of data for theoretical linguistic investigations .	0	used-for	used-for	1
we argue that a more sophisticated and fine-grained annotation in the tree-bank would have very positve effects on stochastic parsers trained on the tree-bank and on $grammars$ induced from the #treebank# , and it would make the treebank more valuable as a source of data for theoretical linguistic investigations .	0	used-for	used-for	1
we argue that a more sophisticated and fine-grained annotation in the tree-bank would have very positve effects on stochastic parsers trained on the tree-bank and on grammars induced from the treebank , and it would make the #treebank# more valuable as a source of data for $theoretical linguistic investigations$ .	0	used-for	used-for	1
the information gained from corpus research and the analyses that are proposed are realized in the framework of #silva# , a $parsing and extraction tool$ for german text corpora .	3	hyponym-of	hyponym-of	1
the information gained from corpus research and the analyses that are proposed are realized in the framework of $silva$ , a parsing and extraction tool for #german text corpora# .	0	used-for	used-for	1
while #paraphrasing# is critical both for $interpretation and generation of natural language$ , current systems use manual or semi-automatic methods to collect paraphrases .	0	used-for	used-for	1
while paraphrasing is critical both for interpretation and generation of natural language , current #systems# use manual or semi-automatic methods to collect $paraphrases$ .	0	used-for	used-for	1
while paraphrasing is critical both for interpretation and generation of natural language , current $systems$ use #manual or semi-automatic methods# to collect paraphrases .	0	used-for	used-for	1
we present an #unsupervised learning algorithm# for $identification of paraphrases$ from a corpus of multiple english translations of the same source text .	0	used-for	used-for	1
we present an unsupervised learning algorithm for $identification of paraphrases$ from a #corpus of multiple english translations# of the same source text .	0	used-for	used-for	1
our #approach# yields $phrasal and single word lexical paraphrases$ as well as syntactic paraphrases .	0	used-for	used-for	1
our #approach# yields phrasal and single word lexical paraphrases as well as $syntactic paraphrases$ .	0	used-for	used-for	1
our approach yields #phrasal and single word lexical paraphrases# as well as $syntactic paraphrases$ .	1	conjunction	conjunction	1
an efficient #bit-vector-based cky-style parser# for $context-free parsing$ is presented .	0	used-for	used-for	1
the #parser# computes a compact $parse forest representation$ of the complete set of possible analyses for large treebank grammars and long input sentences .	0	used-for	used-for	1
the parser computes a compact #parse forest representation# of the complete set of possible analyses for $large treebank grammars$ and long input sentences .	0	used-for	used-for	1
the $parser$ uses #bit-vector operations# to parallelise the basic parsing operations .	0	used-for	used-for	1
in this paper , we propose a #partially-blurred-image classification and analysis framework# for $automatically detecting images$ containing blurred regions and recognizing the blur types for those regions without needing to perform blur kernel estimation and image deblurring .	0	used-for	used-for	1
in this paper , we propose a partially-blurred-image classification and analysis framework for automatically detecting $images$ containing #blurred regions# and recognizing the blur types for those regions without needing to perform blur kernel estimation and image deblurring .	4	part-of	part-of	1
in this paper , we propose a partially-blurred-image classification and analysis framework for automatically detecting images containing blurred regions and recognizing the blur types for those regions without needing to perform #blur kernel estimation# and $image deblurring$ .	1	conjunction	conjunction	1
we develop several $blur features$ modeled by #image color# , gradient , and spectrum information , and use feature parameter training to robustly classify blurred images .	0	used-for	used-for	1
we develop several blur features modeled by #image color# , $gradient$ , and spectrum information , and use feature parameter training to robustly classify blurred images .	1	conjunction	conjunction	1
we develop several $blur features$ modeled by image color , #gradient# , and spectrum information , and use feature parameter training to robustly classify blurred images .	0	used-for	used-for	1
we develop several blur features modeled by image color , #gradient# , and $spectrum information$ , and use feature parameter training to robustly classify blurred images .	1	conjunction	conjunction	1
we develop several $blur features$ modeled by image color , gradient , and #spectrum information# , and use feature parameter training to robustly classify blurred images .	0	used-for	used-for	1
we develop several blur features modeled by image color , gradient , and spectrum information , and use #feature parameter training# to robustly classify $blurred images$ .	0	used-for	used-for	1
our $blur detection$ is based on #image patches# , making region-wise training and classification in one image efficient .	0	used-for	used-for	1
our $blur detection$ is based on image patches , making #region-wise training and classification# in one image efficient .	0	used-for	used-for	1
extensive experiments show that our #method# works satisfactorily on challenging image data , which establishes a technical foundation for solving several $computer vision problems$ , such as motion analysis and image restoration , using the blur information .	0	used-for	used-for	1
extensive experiments show that our $method$ works satisfactorily on challenging #image data# , which establishes a technical foundation for solving several computer vision problems , such as motion analysis and image restoration , using the blur information .	2	evaluate-for	evaluate-for	1
extensive experiments show that our method works satisfactorily on challenging image data , which establishes a technical foundation for solving several $computer vision problems$ , such as #motion analysis# and image restoration , using the blur information .	3	hyponym-of	hyponym-of	1
extensive experiments show that our method works satisfactorily on challenging image data , which establishes a technical foundation for solving several computer vision problems , such as #motion analysis# and $image restoration$ , using the blur information .	1	conjunction	conjunction	1
extensive experiments show that our method works satisfactorily on challenging image data , which establishes a technical foundation for solving several $computer vision problems$ , such as motion analysis and #image restoration# , using the blur information .	3	hyponym-of	hyponym-of	1
extensive experiments show that our $method$ works satisfactorily on challenging image data , which establishes a technical foundation for solving several computer vision problems , such as motion analysis and image restoration , using the #blur information# .	0	used-for	used-for	1
we have recently reported on two new $word-sense disambiguation systems$ , #one# trained on bilingual material -lrb- the canadian hansards -rrb- and the other trained on monolingual material -lrb- roget 's thesaurus and grolier 's encyclopedia -rrb- .	3	hyponym-of	hyponym-of	1
we have recently reported on two new word-sense disambiguation systems , #one# trained on bilingual material -lrb- the canadian hansards -rrb- and the $other$ trained on monolingual material -lrb- roget 's thesaurus and grolier 's encyclopedia -rrb- .	1	conjunction	conjunction	1
we have recently reported on two new word-sense disambiguation systems , $one$ trained on #bilingual material# -lrb- the canadian hansards -rrb- and the other trained on monolingual material -lrb- roget 's thesaurus and grolier 's encyclopedia -rrb- .	2	evaluate-for	evaluate-for	1
we have recently reported on two new $word-sense disambiguation systems$ , one trained on bilingual material -lrb- the canadian hansards -rrb- and the #other# trained on monolingual material -lrb- roget 's thesaurus and grolier 's encyclopedia -rrb- .	3	hyponym-of	hyponym-of	1
we have recently reported on two new word-sense disambiguation systems , one trained on bilingual material -lrb- the canadian hansards -rrb- and the $other$ trained on #monolingual material# -lrb- roget 's thesaurus and grolier 's encyclopedia -rrb- .	0	used-for	used-for	1
we have recently reported on two new word-sense disambiguation systems , one trained on bilingual material -lrb- the canadian hansards -rrb- and the other trained on $monolingual material$ -lrb- #roget 's thesaurus# and grolier 's encyclopedia -rrb- .	3	hyponym-of	hyponym-of	1
we have recently reported on two new word-sense disambiguation systems , one trained on bilingual material -lrb- the canadian hansards -rrb- and the other trained on monolingual material -lrb- #roget 's thesaurus# and $grolier 's encyclopedia$ -rrb- .	1	conjunction	conjunction	1
we have recently reported on two new word-sense disambiguation systems , one trained on bilingual material -lrb- the canadian hansards -rrb- and the other trained on $monolingual material$ -lrb- roget 's thesaurus and #grolier 's encyclopedia# -rrb- .	3	hyponym-of	hyponym-of	1
in addition , #it# could also be used to help evaluate $disambiguation algorithms$ that did not make use of the discourse constraint .	2	evaluate-for	evaluate-for	1
we study and compare two novel #embedding methods# for $segmenting feature points of piece-wise planar structures$ from two -lrb- uncalibrated -rrb- perspective images .	0	used-for	used-for	1
we show that a set of different $homographies$ can be embedded in different ways to a #higher-dimensional real or complex space# , so that each homography corresponds to either a complex bilinear form or a real quadratic form .	5	feature-of	feature-of	1
we show that a set of different homographies can be embedded in different ways to a higher-dimensional real or complex space , so that each $homography$ corresponds to either a #complex bilinear form# or a real quadratic form .	5	feature-of	feature-of	1
we show that a set of different homographies can be embedded in different ways to a higher-dimensional real or complex space , so that each homography corresponds to either a #complex bilinear form# or a $real quadratic form$ .	1	conjunction	conjunction	1
we show that a set of different homographies can be embedded in different ways to a higher-dimensional real or complex space , so that each $homography$ corresponds to either a complex bilinear form or a #real quadratic form# .	5	feature-of	feature-of	1
we give a $closed-form segmentation solution$ for each case by utilizing these properties based on #subspace-segmentation methods# .	0	used-for	used-for	1
these theoretical results show that one can intrinsically segment a $piece-wise planar scene$ from #2-d images# without explicitly performing any 3-d reconstruction .	5	feature-of	feature-of	1
#background maintenance# is a frequent element of $video surveillance systems$ .	4	part-of	part-of	1
we develop wallflower , a #three-component system# for $background maintenance$ : the pixel-level component performs wiener filtering to make probabilistic predictions of the expected background ; the region-level component fills in homogeneous regions of foreground objects ; and the frame-level component detects sudden , global changes in the image and swaps in better approximations of the background .	0	used-for	used-for	1
we develop wallflower , a $three-component system$ for background maintenance : the #pixel-level component# performs wiener filtering to make probabilistic predictions of the expected background ; the region-level component fills in homogeneous regions of foreground objects ; and the frame-level component detects sudden , global changes in the image and swaps in better approximations of the background .	4	part-of	part-of	1
we develop wallflower , a three-component system for background maintenance : the #pixel-level component# performs wiener filtering to make probabilistic predictions of the expected background ; the $region-level component$ fills in homogeneous regions of foreground objects ; and the frame-level component detects sudden , global changes in the image and swaps in better approximations of the background .	1	conjunction	conjunction	1
we develop wallflower , a three-component system for background maintenance : the $pixel-level component$ performs #wiener filtering# to make probabilistic predictions of the expected background ; the region-level component fills in homogeneous regions of foreground objects ; and the frame-level component detects sudden , global changes in the image and swaps in better approximations of the background .	0	used-for	used-for	1
we develop wallflower , a three-component system for background maintenance : the pixel-level component performs #wiener filtering# to make $probabilistic predictions of the expected background$ ; the region-level component fills in homogeneous regions of foreground objects ; and the frame-level component detects sudden , global changes in the image and swaps in better approximations of the background .	0	used-for	used-for	1
we develop wallflower , a $three-component system$ for background maintenance : the pixel-level component performs wiener filtering to make probabilistic predictions of the expected background ; the #region-level component# fills in homogeneous regions of foreground objects ; and the frame-level component detects sudden , global changes in the image and swaps in better approximations of the background .	4	part-of	part-of	1
we develop wallflower , a three-component system for background maintenance : the pixel-level component performs wiener filtering to make probabilistic predictions of the expected background ; the #region-level component# fills in $homogeneous regions of foreground objects$ ; and the frame-level component detects sudden , global changes in the image and swaps in better approximations of the background .	0	used-for	used-for	1
we develop wallflower , a three-component system for background maintenance : the pixel-level component performs wiener filtering to make probabilistic predictions of the expected background ; the #region-level component# fills in homogeneous regions of foreground objects ; and the $frame-level component$ detects sudden , global changes in the image and swaps in better approximations of the background .	1	conjunction	conjunction	1
we develop wallflower , a $three-component system$ for background maintenance : the pixel-level component performs wiener filtering to make probabilistic predictions of the expected background ; the region-level component fills in homogeneous regions of foreground objects ; and the #frame-level component# detects sudden , global changes in the image and swaps in better approximations of the background .	4	part-of	part-of	1
we compare our #system# with 8 other $background subtraction algorithms$ .	6	compare	compare	1
#wallflower# is shown to outperform previous $algorithms$ by handling a greater set of the difficult situations that can occur .	6	compare	compare	1
finally , we analyze the experimental results and propose #normative principles# for $background maintenance$ .	0	used-for	used-for	1
is it possible to use #out-of-domain acoustic training data# to improve a $speech recognizer$ 's performance on a speciic , independent application ?	0	used-for	used-for	1
in our experiments , we use #wallstreet journal -lrb- wsj -rrb- data# to train a $recognizer$ , which is adapted and evaluated in the phonebook domain .	0	used-for	used-for	1
in our experiments , we use wallstreet journal -lrb- wsj -rrb- data to train a $recognizer$ , which is adapted and evaluated in the #phonebook domain# .	2	evaluate-for	evaluate-for	1
first , starting from the #wsj-trained recognizer# , how much adaptation data -lrb- taken from the phonebook training corpus -rrb- is necessary to achieve a reasonable $recognition$ performance in spite of the high degree of mismatch ?	0	used-for	used-for	1
first , starting from the $wsj-trained recognizer$ , how much #adaptation data# -lrb- taken from the phonebook training corpus -rrb- is necessary to achieve a reasonable recognition performance in spite of the high degree of mismatch ?	0	used-for	used-for	1
first , starting from the wsj-trained recognizer , how much #adaptation data# -lrb- taken from the $phonebook training corpus$ -rrb- is necessary to achieve a reasonable recognition performance in spite of the high degree of mismatch ?	4	part-of	part-of	1
second , is it possible to improve the $recognition$ performance of a #phonebook-trained baseline acoustic model# by using additional out-of-domain training data ?	0	used-for	used-for	1
second , is it possible to improve the recognition performance of a $phonebook-trained baseline acoustic model$ by using additional #out-of-domain training data# ?	0	used-for	used-for	1
this paper proposes an #approach# to $full parsing$ suitable for information extraction from texts .	0	used-for	used-for	1
this paper proposes an approach to #full parsing# suitable for $information extraction$ from texts .	0	used-for	used-for	1
#it# was implemented in the $ie module$ of facile , a eu project for multilingual text classification and ie .	0	used-for	used-for	1
it was implemented in the #ie module# of $facile , a eu project for multilingual text classification and ie$ .	4	part-of	part-of	1
it then presents an implemented $graphic interpretation system$ that takes into account a variety of #communicative signals# , and an evaluation study showing that evidence obtained from shallow processing of the graphic 's caption has a significant impact on the system 's success .	0	used-for	used-for	1
it then presents an implemented graphic interpretation system that takes into account a variety of communicative signals , and an evaluation study showing that evidence obtained from #shallow processing# of the graphic 's caption has a significant impact on the $system$ 's success .	0	used-for	used-for	1
it then presents an implemented graphic interpretation system that takes into account a variety of communicative signals , and an evaluation study showing that evidence obtained from $shallow processing$ of the #graphic 's caption# has a significant impact on the system 's success .	0	used-for	used-for	1
#graphical models# such as bayesian networks -lrb- bns -rrb- are being increasingly applied to various $computer vision problems$ .	0	used-for	used-for	1
$graphical models$ such as #bayesian networks -lrb- bns -rrb-# are being increasingly applied to various computer vision problems .	3	hyponym-of	hyponym-of	1
one bottleneck in using bn is that learning the $bn model parameters$ often requires a large amount of reliable and #representative training data# , which proves to be difficult to acquire for many computer vision tasks .	0	used-for	used-for	1
one bottleneck in using bn is that learning the bn model parameters often requires a large amount of reliable and #representative training data# , which proves to be difficult to acquire for many $computer vision tasks$ .	0	used-for	used-for	1
on the other hand , there is often available #qualitative prior knowledge# about the $model$ .	5	feature-of	feature-of	1
such $knowledge$ comes either from #domain experts# based on their experience or from various physical or geometric constraints that govern the objects we try to model .	0	used-for	used-for	1
such knowledge comes either from #domain experts# based on their experience or from various $physical or geometric constraints$ that govern the objects we try to model .	1	conjunction	conjunction	1
such $knowledge$ comes either from domain experts based on their experience or from various #physical or geometric constraints# that govern the objects we try to model .	0	used-for	used-for	1
unlike the #quantitative prior# , the $qualitative prior$ is often ignored due to the difficulty of incorporating them into the model learning process .	6	compare	compare	1
unlike the quantitative prior , the qualitative prior is often ignored due to the difficulty of incorporating #them# into the $model learning process$ .	4	part-of	part-of	1
in this paper , we introduce a closed-form solution to systematically combine the #limited training data# with some generic $qualitative knowledge$ for bn parameter learning .	1	conjunction	conjunction	1
in this paper , we introduce a closed-form solution to systematically combine the #limited training data# with some generic qualitative knowledge for $bn parameter learning$ .	0	used-for	used-for	1
in this paper , we introduce a closed-form solution to systematically combine the limited training data with some generic #qualitative knowledge# for $bn parameter learning$ .	0	used-for	used-for	1
in this paper , we introduce a $closed-form solution$ to systematically combine the limited training data with some generic qualitative knowledge for #bn parameter learning# .	0	used-for	used-for	1
to validate our method , we compare #it# with the $maximum likelihood -lrb- ml -rrb- estimation method$ under sparse data and with the expectation maximization -lrb- em -rrb- algorithm under incomplete data respectively .	6	compare	compare	1
to validate our method , we compare #it# with the maximum likelihood -lrb- ml -rrb- estimation method under sparse data and with the $expectation maximization -lrb- em -rrb- algorithm$ under incomplete data respectively .	6	compare	compare	1
to validate our method , we compare $it$ with the maximum likelihood -lrb- ml -rrb- estimation method under #sparse data# and with the expectation maximization -lrb- em -rrb- algorithm under incomplete data respectively .	0	used-for	used-for	1
to validate our method , we compare it with the $maximum likelihood -lrb- ml -rrb- estimation method$ under #sparse data# and with the expectation maximization -lrb- em -rrb- algorithm under incomplete data respectively .	0	used-for	used-for	1
to validate our method , we compare $it$ with the maximum likelihood -lrb- ml -rrb- estimation method under sparse data and with the expectation maximization -lrb- em -rrb- algorithm under #incomplete data# respectively .	0	used-for	used-for	1
to validate our method , we compare it with the maximum likelihood -lrb- ml -rrb- estimation method under sparse data and with the $expectation maximization -lrb- em -rrb- algorithm$ under #incomplete data# respectively .	0	used-for	used-for	1
to further demonstrate its applications for $computer vision$ , we apply #it# to learn a bn model for facial action unit -lrb- au -rrb- recognition from real image data .	0	used-for	used-for	1
to further demonstrate its applications for computer vision , we apply #it# to learn a $bn model$ for facial action unit -lrb- au -rrb- recognition from real image data .	0	used-for	used-for	1
to further demonstrate its applications for computer vision , we apply it to learn a #bn model# for $facial action unit -lrb- au -rrb- recognition$ from real image data .	0	used-for	used-for	1
to further demonstrate its applications for computer vision , we apply it to learn a bn model for $facial action unit -lrb- au -rrb- recognition$ from #real image data# .	0	used-for	used-for	1
the experimental results show that with simple and #generic qualitative constraints# and using only a small amount of $training data$ , our method can robustly and accurately estimate the bn model parameters .	1	conjunction	conjunction	1
the experimental results show that with simple and #generic qualitative constraints# and using only a small amount of training data , our $method$ can robustly and accurately estimate the bn model parameters .	0	used-for	used-for	1
the experimental results show that with simple and generic qualitative constraints and using only a small amount of #training data# , our $method$ can robustly and accurately estimate the bn model parameters .	0	used-for	used-for	1
the experimental results show that with simple and generic qualitative constraints and using only a small amount of training data , our #method# can robustly and accurately estimate the $bn model parameters$ .	0	used-for	used-for	1
in this paper we introduce a #modal language lt# for imposing $constraints on trees$ , and an extension lt -lrb- lf -rrb- for imposing constraints on trees decorated with feature structures .	0	used-for	used-for	1
in this paper we introduce a modal language lt for imposing constraints on trees , and an #extension lt -lrb- lf -rrb-# for imposing $constraints on trees decorated with feature structures$ .	0	used-for	used-for	1
the motivation for introducing these #languages# is to provide tools for formalising $grammatical frameworks$ perspicuously , and the paper illustrates this by showing how the leading ideas of gpsg can be captured in lt -lrb- lf -rrb- .	0	used-for	used-for	1
the motivation for introducing these languages is to provide tools for formalising grammatical frameworks perspicuously , and the paper illustrates this by showing how the leading ideas of #gpsg# can be captured in $lt -lrb- lf -rrb-$ .	0	used-for	used-for	1
previous research has demonstrated the utility of #clustering# in $inducing semantic verb classes$ from undisambiguated corpus data .	0	used-for	used-for	1
previous research has demonstrated the utility of $clustering$ in inducing semantic verb classes from #undisambiguated corpus data# .	0	used-for	used-for	1
we describe a new $approach$ which involves #clustering subcategorization frame -lrb- scf -rrb- distributions# using the information bottleneck and nearest neighbour methods .	4	part-of	part-of	1
we describe a new approach which involves $clustering subcategorization frame -lrb- scf -rrb- distributions$ using the #information bottleneck and nearest neighbour methods# .	0	used-for	used-for	1
a novel #evaluation scheme# is proposed which accounts for the effect of $polysemy$ on the clusters , offering us a good insight into the potential and limitations of semantically classifying undisambiguated scf data .	0	used-for	used-for	1
a novel #evaluation scheme# is proposed which accounts for the effect of polysemy on the clusters , offering us a good insight into the potential and limitations of $semantically classifying undisambiguated scf data$ .	2	evaluate-for	evaluate-for	1
a novel evaluation scheme is proposed which accounts for the effect of #polysemy# on the $clusters$ , offering us a good insight into the potential and limitations of semantically classifying undisambiguated scf data .	5	feature-of	feature-of	1
due to the capacity of #pan-tilt-zoom -lrb- ptz -rrb- cameras# to simultaneously cover a $panoramic area$ and maintain high resolution imagery , researches in automated surveillance systems with multiple ptz cameras have become increasingly important .	0	used-for	used-for	1
due to the capacity of #pan-tilt-zoom -lrb- ptz -rrb- cameras# to simultaneously cover a panoramic area and maintain $high resolution imagery$ , researches in automated surveillance systems with multiple ptz cameras have become increasingly important .	0	used-for	used-for	1
due to the capacity of pan-tilt-zoom -lrb- ptz -rrb- cameras to simultaneously cover a panoramic area and maintain high resolution imagery , researches in $automated surveillance systems$ with multiple #ptz cameras# have become increasingly important .	5	feature-of	feature-of	1
most existing #algorithms# require the prior knowledge of intrinsic parameters of the ptz camera to infer the $relative positioning$ and orientation among multiple ptz cameras .	0	used-for	used-for	1
most existing #algorithms# require the prior knowledge of intrinsic parameters of the ptz camera to infer the relative positioning and $orientation$ among multiple ptz cameras .	0	used-for	used-for	1
most existing $algorithms$ require the #prior knowledge of intrinsic parameters of the ptz camera# to infer the relative positioning and orientation among multiple ptz cameras .	0	used-for	used-for	1
most existing algorithms require the prior knowledge of intrinsic parameters of the ptz camera to infer the #relative positioning# and $orientation$ among multiple ptz cameras .	1	conjunction	conjunction	1
most existing algorithms require the prior knowledge of intrinsic parameters of the ptz camera to infer the #relative positioning# and orientation among multiple $ptz cameras$ .	5	feature-of	feature-of	1
most existing algorithms require the prior knowledge of intrinsic parameters of the ptz camera to infer the relative positioning and #orientation# among multiple $ptz cameras$ .	5	feature-of	feature-of	1
to overcome this limitation , we propose a novel #mapping algorithm# that derives the $relative positioning$ and orientation between two ptz cameras based on a unified polynomial model .	0	used-for	used-for	1
to overcome this limitation , we propose a novel #mapping algorithm# that derives the relative positioning and $orientation$ between two ptz cameras based on a unified polynomial model .	0	used-for	used-for	1
to overcome this limitation , we propose a novel mapping algorithm that derives the #relative positioning# and $orientation$ between two ptz cameras based on a unified polynomial model .	1	conjunction	conjunction	1
to overcome this limitation , we propose a novel mapping algorithm that derives the #relative positioning# and orientation between two $ptz cameras$ based on a unified polynomial model .	5	feature-of	feature-of	1
to overcome this limitation , we propose a novel mapping algorithm that derives the relative positioning and #orientation# between two $ptz cameras$ based on a unified polynomial model .	5	feature-of	feature-of	1
to overcome this limitation , we propose a novel $mapping algorithm$ that derives the relative positioning and orientation between two ptz cameras based on a #unified polynomial model# .	0	used-for	used-for	1
experimental results demonstrate that our proposed $algorithm$ presents substantially reduced #computational complexity# and improved flexibility at the cost of slightly decreased pixel accuracy , as compared with the work of chen and wang .	2	evaluate-for	evaluate-for	1
experimental results demonstrate that our proposed $algorithm$ presents substantially reduced computational complexity and improved #flexibility# at the cost of slightly decreased pixel accuracy , as compared with the work of chen and wang .	2	evaluate-for	evaluate-for	1
experimental results demonstrate that our proposed $algorithm$ presents substantially reduced computational complexity and improved flexibility at the cost of slightly decreased #pixel accuracy# , as compared with the work of chen and wang .	2	evaluate-for	evaluate-for	1
this slightly decreased $pixel accuracy$ can be compensated by #consistent labeling approaches# without added cost for the application of automated surveillance systems along with changing configurations and a larger number of ptz cameras .	0	used-for	used-for	1
this paper presents a new #two-pass algorithm# for $extra large -lrb- more than 1m words -rrb- vocabulary continuous speech recognition$ based on the information retrieval -lrb- elvircos -rrb- .	0	used-for	used-for	1
this paper presents a new $two-pass algorithm$ for extra large -lrb- more than 1m words -rrb- vocabulary continuous speech recognition based on the #information retrieval -lrb- elvircos -rrb-# .	0	used-for	used-for	1
the principle of this approach is to decompose a recognition process into two $passes$ where the #first pass# builds the words subset for the second pass recognition by using information retrieval procedure .	3	hyponym-of	hyponym-of	1
the principle of this approach is to decompose a recognition process into two $passes$ where the first pass builds the words subset for the #second pass recognition# by using information retrieval procedure .	3	hyponym-of	hyponym-of	1
the principle of this approach is to decompose a recognition process into two passes where the first pass builds the words subset for the $second pass recognition$ by using #information retrieval procedure# .	0	used-for	used-for	1
#word graph composition# for $continuous speech$ is presented .	0	used-for	used-for	1
with this #approach# a high performances for $large vocabulary speech recognition$ can be obtained .	0	used-for	used-for	1
first , images are partitioned into regions using $one-class classification$ and #patch-based clustering algorithms# where one-class classifiers model the regions with relatively uniform color and texture properties , and clustering of patches aims to detect structures in the remaining regions .	1	conjunction	conjunction	1
first , images are partitioned into regions using one-class classification and patch-based clustering algorithms where $one-class classifiers$ model the regions with relatively #uniform color and texture properties# , and clustering of patches aims to detect structures in the remaining regions .	0	used-for	used-for	1
next , the resulting regions are clustered to obtain a codebook of region types , and two #models# are constructed for $scene representation$ : a '' bag of individual regions '' representation where each region is regarded separately , and a '' bag of region pairs '' representation where regions with particular spatial relationships are considered together .	0	used-for	used-for	1
given these representations , $scene classification$ is done using #bayesian classifiers# .	0	used-for	used-for	1
experiments on the #labelme data set# showed that the proposed $models$ significantly out-perform a baseline global feature-based approach .	2	evaluate-for	evaluate-for	1
experiments on the #labelme data set# showed that the proposed models significantly out-perform a $baseline global feature-based approach$ .	2	evaluate-for	evaluate-for	1
experiments on the labelme data set showed that the proposed #models# significantly out-perform a $baseline global feature-based approach$ .	6	compare	compare	1
the #model# is designed for use in $error correction$ , with a focus on post-processing the output of black-box ocr systems in order to make it more useful for nlp tasks .	0	used-for	used-for	1
the #model# is designed for use in error correction , with a focus on $post-processing$ the output of black-box ocr systems in order to make it more useful for nlp tasks .	0	used-for	used-for	1
the model is designed for use in $error correction$ , with a focus on #post-processing# the output of black-box ocr systems in order to make it more useful for nlp tasks .	4	part-of	part-of	1
the model is designed for use in error correction , with a focus on post-processing the output of black-box ocr systems in order to make #it# more useful for $nlp tasks$ .	0	used-for	used-for	1
we present an implementation of the $model$ based on #finite-state models# , demonstrate the model 's ability to significantly reduce character and word error rate , and provide evaluation results involving automatic extraction of translation lexicons from printed text .	0	used-for	used-for	1
we present an implementation of the model based on finite-state models , demonstrate the $model$ 's ability to significantly reduce #character and word error rate# , and provide evaluation results involving automatic extraction of translation lexicons from printed text .	2	evaluate-for	evaluate-for	1
we present an implementation of the model based on finite-state models , demonstrate the $model$ 's ability to significantly reduce character and word error rate , and provide evaluation results involving #automatic extraction of translation lexicons# from printed text .	2	evaluate-for	evaluate-for	1
we present an implementation of the model based on finite-state models , demonstrate the model 's ability to significantly reduce character and word error rate , and provide evaluation results involving $automatic extraction of translation lexicons$ from #printed text# .	0	used-for	used-for	1
we present a #framework# for $word alignment$ based on log-linear models .	0	used-for	used-for	1
we present a $framework$ for word alignment based on #log-linear models# .	0	used-for	used-for	1
all #knowledge sources# are treated as $feature functions$ , which depend on the source langauge sentence , the target language sentence and possible additional variables .	0	used-for	used-for	1
#log-linear models# allow $statistical alignment models$ to be easily extended by incorporating syntactic information .	0	used-for	used-for	1
$log-linear models$ allow statistical alignment models to be easily extended by incorporating #syntactic information# .	0	used-for	used-for	1
in this paper , we use #ibm model 3 alignment probabilities# , $pos correspondence$ , and bilingual dictionary coverage as features .	1	conjunction	conjunction	1
in this paper , we use #ibm model 3 alignment probabilities# , pos correspondence , and bilingual dictionary coverage as $features$ .	0	used-for	used-for	1
in this paper , we use ibm model 3 alignment probabilities , #pos correspondence# , and $bilingual dictionary coverage$ as features .	1	conjunction	conjunction	1
in this paper , we use ibm model 3 alignment probabilities , #pos correspondence# , and bilingual dictionary coverage as $features$ .	0	used-for	used-for	1
in this paper , we use ibm model 3 alignment probabilities , pos correspondence , and #bilingual dictionary coverage# as $features$ .	0	used-for	used-for	1
our experiments show that #log-linear models# significantly outperform $ibm translation models$ .	6	compare	compare	1
#hough voting# in a geometric transformation space allows us to realize $spatial verification$ , but remains sensitive to feature detection errors because of the inflexible quan-tization of single feature correspondences .	0	used-for	used-for	1
$hough voting$ in a #geometric transformation space# allows us to realize spatial verification , but remains sensitive to feature detection errors because of the inflexible quan-tization of single feature correspondences .	5	feature-of	feature-of	1
to handle this problem , we propose a new #method# , called adaptive dither voting , for $robust spatial verification$ .	0	used-for	used-for	1
for each correspondence , instead of hard-mapping it to a single transformation , the $method$ augments its description by using #multiple dithered transformations# that are deterministically generated by the other correspondences .	0	used-for	used-for	1
we also propose exploiting the #non-uniformity# of a $hough histogram$ as the spatial similarity to handle multiple matching surfaces .	5	feature-of	feature-of	1
we also propose exploiting the #non-uniformity# of a hough histogram as the spatial similarity to handle $multiple matching surfaces$ .	0	used-for	used-for	1
the #method# outperforms its state-of-the-art counterparts in both accuracy and scalability , especially when it comes to the $retrieval of small , rotated objects$ .	0	used-for	used-for	1
the $method$ outperforms its state-of-the-art #counterparts# in both accuracy and scalability , especially when it comes to the retrieval of small , rotated objects .	6	compare	compare	1
the $method$ outperforms its state-of-the-art counterparts in both #accuracy# and scalability , especially when it comes to the retrieval of small , rotated objects .	2	evaluate-for	evaluate-for	1
the method outperforms its state-of-the-art $counterparts$ in both #accuracy# and scalability , especially when it comes to the retrieval of small , rotated objects .	2	evaluate-for	evaluate-for	1
the $method$ outperforms its state-of-the-art counterparts in both accuracy and #scalability# , especially when it comes to the retrieval of small , rotated objects .	2	evaluate-for	evaluate-for	1
the method outperforms its state-of-the-art $counterparts$ in both accuracy and #scalability# , especially when it comes to the retrieval of small , rotated objects .	2	evaluate-for	evaluate-for	1
we propose a novel technique called #bispectral photo-metric stereo# that makes effective use of fluorescence for $shape reconstruction$ .	0	used-for	used-for	1
we propose a novel technique called $bispectral photo-metric stereo$ that makes effective use of #fluorescence# for shape reconstruction .	0	used-for	used-for	1
due to the #complexity# of its $emission process$ , fluo-rescence tends to be excluded from most algorithms in computer vision and image processing .	2	evaluate-for	evaluate-for	1
due to the complexity of its emission process , fluo-rescence tends to be excluded from most #algorithms# in $computer vision$ and image processing .	0	used-for	used-for	1
due to the complexity of its emission process , fluo-rescence tends to be excluded from most #algorithms# in computer vision and $image processing$ .	0	used-for	used-for	1
due to the complexity of its emission process , fluo-rescence tends to be excluded from most algorithms in #computer vision# and $image processing$ .	1	conjunction	conjunction	1
moreover , #fluorescence 's wavelength-shifting property# enables us to estimate the $shape$ of an object by applying photomet-ric stereo to emission-only images without suffering from specular reflection .	0	used-for	used-for	1
moreover , fluorescence 's wavelength-shifting property enables us to estimate the $shape$ of an object by applying #photomet-ric stereo# to emission-only images without suffering from specular reflection .	0	used-for	used-for	1
moreover , fluorescence 's wavelength-shifting property enables us to estimate the shape of an object by applying $photomet-ric stereo$ to #emission-only images# without suffering from specular reflection .	0	used-for	used-for	1
this is the significant advantage of the $fluorescence-based method$ over previous #methods# based on reflection .	6	compare	compare	1
in this paper , we present an #approach# for learning a $visual representation$ from the raw spatiotemporal signals in videos .	0	used-for	used-for	1
in this paper , we present an approach for learning a $visual representation$ from the #raw spatiotemporal signals in videos# .	0	used-for	used-for	1
we formulate our $method$ as an #unsupervised sequential verification task# , i.e. , we determine whether a sequence of frames from a video is in the correct temporal order .	0	used-for	used-for	1
with this simple #task# and no semantic labels , we learn a powerful $visual representation$ using a convolutional neural network -lrb- cnn -rrb- .	0	used-for	used-for	1
with this simple task and no semantic labels , we learn a powerful $visual representation$ using a #convolutional neural network -lrb- cnn -rrb-# .	0	used-for	used-for	1
the $representation$ contains #complementary information# to that learned from supervised image datasets like imagenet .	4	part-of	part-of	1
the representation contains $complementary information$ to that learned from #supervised image datasets# like imagenet .	0	used-for	used-for	1
the representation contains complementary information to that learned from $supervised image datasets$ like #imagenet# .	3	hyponym-of	hyponym-of	1
qualitative results show that our #method# captures information that is temporally varying , such as $human pose$ .	0	used-for	used-for	1
when used as #pre-training# for $action recognition$ , our method gives significant gains over learning without external data on benchmark datasets like ucf101 and hmdb51 .	0	used-for	used-for	1
when used as $pre-training$ for action recognition , #our method# gives significant gains over learning without external data on benchmark datasets like ucf101 and hmdb51 .	0	used-for	used-for	1
when used as pre-training for action recognition , #our method# gives significant gains over $learning without external data$ on benchmark datasets like ucf101 and hmdb51 .	6	compare	compare	1
when used as pre-training for action recognition , $our method$ gives significant gains over learning without external data on #benchmark datasets# like ucf101 and hmdb51 .	2	evaluate-for	evaluate-for	1
when used as pre-training for action recognition , our method gives significant gains over $learning without external data$ on #benchmark datasets# like ucf101 and hmdb51 .	2	evaluate-for	evaluate-for	1
when used as pre-training for action recognition , our method gives significant gains over learning without external data on $benchmark datasets$ like #ucf101# and hmdb51 .	3	hyponym-of	hyponym-of	1
when used as pre-training for action recognition , our method gives significant gains over learning without external data on benchmark datasets like #ucf101# and $hmdb51$ .	1	conjunction	conjunction	1
when used as pre-training for action recognition , our method gives significant gains over learning without external data on $benchmark datasets$ like ucf101 and #hmdb51# .	3	hyponym-of	hyponym-of	1
to demonstrate its sensitivity to human pose , we show results for $pose estimation$ on the #flic and mpii datasets# that are competitive , or better than approaches using significantly more supervision .	2	evaluate-for	evaluate-for	1
to demonstrate its sensitivity to human pose , we show results for pose estimation on the flic and mpii datasets that are competitive , or better than $approaches$ using significantly more #supervision# .	0	used-for	used-for	1
$our method$ can be combined with #supervised representations# to provide an additional boost in accuracy .	1	conjunction	conjunction	1
$our method$ can be combined with supervised representations to provide an additional boost in #accuracy# .	2	evaluate-for	evaluate-for	1
`` to explain complex phenomena , an #explanation system# must be able to select information from a formal representation of domain knowledge , organize the selected information into multisentential discourse plans , and realize the $discourse plans$ in text .	0	used-for	used-for	1
this paper reports on a seven-year effort to empirically study $explanation generation$ from #semantically rich , large-scale knowledge bases# .	0	used-for	used-for	1
in particular , it describes a #robust explanation system# that constructs $multisentential and multi-paragraph explanations$ from the a large-scale knowledge base in the domain of botanical anatomy , physiology , and development .	0	used-for	used-for	1
in particular , it describes a $robust explanation system$ that constructs multisentential and multi-paragraph explanations from the a #large-scale knowledge base# in the domain of botanical anatomy , physiology , and development .	0	used-for	used-for	1
in particular , it describes a robust explanation system that constructs multisentential and multi-paragraph explanations from the a $large-scale knowledge base$ in the domain of #botanical anatomy# , physiology , and development .	5	feature-of	feature-of	1
in particular , it describes a robust explanation system that constructs multisentential and multi-paragraph explanations from the a large-scale knowledge base in the domain of #botanical anatomy# , $physiology$ , and development .	1	conjunction	conjunction	1
in particular , it describes a robust explanation system that constructs multisentential and multi-paragraph explanations from the a $large-scale knowledge base$ in the domain of botanical anatomy , #physiology# , and development .	5	feature-of	feature-of	1
in particular , it describes a robust explanation system that constructs multisentential and multi-paragraph explanations from the a large-scale knowledge base in the domain of botanical anatomy , #physiology# , and $development$ .	1	conjunction	conjunction	1
in particular , it describes a robust explanation system that constructs multisentential and multi-paragraph explanations from the a $large-scale knowledge base$ in the domain of botanical anatomy , physiology , and #development# .	5	feature-of	feature-of	1
we introduce the evaluation methodology and describe how performance was assessed with this #methodology# in the most extensive empirical evaluation conducted on an $explanation system$ .	2	evaluate-for	evaluate-for	1
we present an $operable definition$ of focus which is argued to be of a #cognito-pragmatic nature# and explore how it is determined in discourse in a formalized manner .	5	feature-of	feature-of	1
for this purpose , a file card model of discourse model and knowledge store is introduced enabling the decomposition and formal representation of its determination process as a $programmable algorithm$ -lrb- #fda# -rrb- .	3	hyponym-of	hyponym-of	1
interdisciplinary evidence from social and cognitive psychology is cited and the prospect of the integration of focus via #fda# as a $discourse-level construct$ into speech synthesis systems , in particular , concept-to-speech systems , is also briefly discussed .	0	used-for	used-for	1
interdisciplinary evidence from social and cognitive psychology is cited and the prospect of the integration of focus via fda as a #discourse-level construct# into $speech synthesis systems$ , in particular , concept-to-speech systems , is also briefly discussed .	4	part-of	part-of	1
interdisciplinary evidence from social and cognitive psychology is cited and the prospect of the integration of focus via fda as a discourse-level construct into $speech synthesis systems$ , in particular , #concept-to-speech systems# , is also briefly discussed .	3	hyponym-of	hyponym-of	1
#inference# in these $models$ involves solving a combinatorial optimization problem , with methods such as graph cuts , belief propagation .	0	used-for	used-for	1
$inference$ in these models involves solving a #combinatorial optimization problem# , with methods such as graph cuts , belief propagation .	4	part-of	part-of	1
inference in these models involves solving a $combinatorial optimization problem$ , with #methods# such as graph cuts , belief propagation .	0	used-for	used-for	1
inference in these models involves solving a combinatorial optimization problem , with $methods$ such as #graph cuts# , belief propagation .	0	used-for	used-for	1
inference in these models involves solving a combinatorial optimization problem , with methods such as #graph cuts# , $belief propagation$ .	1	conjunction	conjunction	1
inference in these models involves solving a combinatorial optimization problem , with $methods$ such as graph cuts , #belief propagation# .	0	used-for	used-for	1
to overcome this , state-of-the-art #structured learning methods# frame the $problem$ as one of large margin estimation .	0	used-for	used-for	1
to overcome this , state-of-the-art structured learning methods frame the $problem$ as one of #large margin estimation# .	0	used-for	used-for	1
#iterative solutions# have been proposed to solve the resulting $convex optimization problem$ .	0	used-for	used-for	1
we show how the resulting $optimization problem$ can be reduced to an equivalent #convex problem# with a small number of constraints , and solve it using an efficient scheme .	0	used-for	used-for	1
#interpreting metaphors# is an integral and inescapable process in $human understanding of natural language$ .	3	hyponym-of	hyponym-of	1
this paper discusses a #method# of $analyzing metaphors$ based on the existence of a small number of generalized metaphor mappings .	0	used-for	used-for	1
this paper discusses a method of $analyzing metaphors$ based on the existence of a small number of #generalized metaphor mappings# .	0	used-for	used-for	1
each $generalized metaphor$ contains a #recognition network# , a basic mapping , additional transfer mappings , and an implicit intention component .	4	part-of	part-of	1
each generalized metaphor contains a #recognition network# , a $basic mapping$ , additional transfer mappings , and an implicit intention component .	1	conjunction	conjunction	1
each $generalized metaphor$ contains a recognition network , a #basic mapping# , additional transfer mappings , and an implicit intention component .	4	part-of	part-of	1
each $generalized metaphor$ contains a recognition network , a basic mapping , additional #transfer mappings# , and an implicit intention component .	4	part-of	part-of	1
each generalized metaphor contains a recognition network , a $basic mapping$ , additional #transfer mappings# , and an implicit intention component .	1	conjunction	conjunction	1
each generalized metaphor contains a recognition network , a basic mapping , additional #transfer mappings# , and an $implicit intention component$ .	1	conjunction	conjunction	1
each $generalized metaphor$ contains a recognition network , a basic mapping , additional transfer mappings , and an #implicit intention component# .	4	part-of	part-of	1
it is argued that the #method# reduces $metaphor interpretation$ from a reconstruction to a recognition task .	0	used-for	used-for	1
it is argued that the method reduces $metaphor interpretation$ from a reconstruction to a #recognition task# .	0	used-for	used-for	1
this study presents a $method to automatically acquire paraphrases$ using #bilingual corpora# , which utilizes the bilingual dependency relations obtained by projecting a monolingual dependency parse onto the other language sentence based on statistical alignment techniques .	0	used-for	used-for	1
this study presents a $method to automatically acquire paraphrases$ using bilingual corpora , which utilizes the #bilingual dependency relations# obtained by projecting a monolingual dependency parse onto the other language sentence based on statistical alignment techniques .	0	used-for	used-for	1
this study presents a method to automatically acquire paraphrases using bilingual corpora , which utilizes the $bilingual dependency relations$ obtained by projecting a #monolingual dependency parse# onto the other language sentence based on statistical alignment techniques .	0	used-for	used-for	1
this study presents a method to automatically acquire paraphrases using bilingual corpora , which utilizes the $bilingual dependency relations$ obtained by projecting a monolingual dependency parse onto the other language sentence based on #statistical alignment techniques# .	0	used-for	used-for	1
since the $paraphrasing method$ is capable of clearly disambiguating the sense of an original phrase using the #bilingual context of dependency relation# , it would be possible to obtain interchangeable paraphrases under a given context .	0	used-for	used-for	1
also , we provide an advanced #method# to acquire $generalized translation knowledge$ using the extracted paraphrases .	0	used-for	used-for	1
also , we provide an advanced $method$ to acquire generalized translation knowledge using the extracted #paraphrases# .	0	used-for	used-for	1
we applied the #method# to acquire the $generalized translation knowledge$ for korean-english translation .	0	used-for	used-for	1
we applied the method to acquire the #generalized translation knowledge# for $korean-english translation$ .	0	used-for	used-for	1
through experiments with parallel corpora of a korean and english language pairs , we show that our #paraphrasing method# effectively extracts $paraphrases$ with high precision , 94.3 % and 84.6 % respectively for korean and english , and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5 % compression ratio .	0	used-for	used-for	1
through experiments with parallel corpora of a korean and english language pairs , we show that our $paraphrasing method$ effectively extracts paraphrases with high #precision# , 94.3 % and 84.6 % respectively for korean and english , and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5 % compression ratio .	2	evaluate-for	evaluate-for	1
through experiments with parallel corpora of a korean and english language pairs , we show that our paraphrasing method effectively extracts paraphrases with high precision , 94.3 % and 84.6 % respectively for #korean# and $english$ , and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5 % compression ratio .	1	conjunction	conjunction	1
through experiments with parallel corpora of a korean and english language pairs , we show that our paraphrasing method effectively extracts paraphrases with high precision , 94.3 % and 84.6 % respectively for korean and english , and the $translation knowledge$ extracted from the #bilingual corpora# could be generalized successfully using the paraphrases with the 12.5 % compression ratio .	0	used-for	used-for	1
through experiments with parallel corpora of a korean and english language pairs , we show that our paraphrasing method effectively extracts paraphrases with high precision , 94.3 % and 84.6 % respectively for korean and english , and the $translation knowledge$ extracted from the bilingual corpora could be generalized successfully using the #paraphrases# with the 12.5 % compression ratio .	0	used-for	used-for	1
through experiments with parallel corpora of a korean and english language pairs , we show that our paraphrasing method effectively extracts paraphrases with high precision , 94.3 % and 84.6 % respectively for korean and english , and the $translation knowledge$ extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5 % #compression ratio# .	2	evaluate-for	evaluate-for	1
we provide a $logical definition of minimalist grammars$ , that are #stabler 's formalization of chomsky 's minimalist program# .	3	hyponym-of	hyponym-of	1
our #logical definition# leads to a neat relation to categorial grammar , -lrb- yielding a treatment of $montague semantics$ -rrb- , a parsing-as-deduction in a resource sensitive logic , and a learning algorithm from structured data -lrb- based on a typing-algorithm and type-unification -rrb- .	0	used-for	used-for	1
our logical definition leads to a neat relation to categorial grammar , -lrb- yielding a treatment of montague semantics -rrb- , a #parsing-as-deduction# in a $resource sensitive logic$ , and a learning algorithm from structured data -lrb- based on a typing-algorithm and type-unification -rrb- .	0	used-for	used-for	1
our logical definition leads to a neat relation to categorial grammar , -lrb- yielding a treatment of montague semantics -rrb- , a parsing-as-deduction in a resource sensitive logic , and a $learning algorithm$ from #structured data# -lrb- based on a typing-algorithm and type-unification -rrb- .	0	used-for	used-for	1
our logical definition leads to a neat relation to categorial grammar , -lrb- yielding a treatment of montague semantics -rrb- , a parsing-as-deduction in a resource sensitive logic , and a $learning algorithm$ from structured data -lrb- based on a #typing-algorithm# and type-unification -rrb- .	0	used-for	used-for	1
our logical definition leads to a neat relation to categorial grammar , -lrb- yielding a treatment of montague semantics -rrb- , a parsing-as-deduction in a resource sensitive logic , and a learning algorithm from structured data -lrb- based on a #typing-algorithm# and $type-unification$ -rrb- .	1	conjunction	conjunction	1
our logical definition leads to a neat relation to categorial grammar , -lrb- yielding a treatment of montague semantics -rrb- , a parsing-as-deduction in a resource sensitive logic , and a $learning algorithm$ from structured data -lrb- based on a typing-algorithm and #type-unification# -rrb- .	0	used-for	used-for	1
there are several #approaches# that model $information extraction$ as a token classification task , using various tagging strategies to combine multiple tokens .	0	used-for	used-for	1
there are several approaches that model #information extraction# as a $token classification task$ , using various tagging strategies to combine multiple tokens .	3	hyponym-of	hyponym-of	1
there are several approaches that model information extraction as a $token classification task$ , using various #tagging strategies# to combine multiple tokens .	0	used-for	used-for	1
we also introduce a new strategy , called begin/after tagging or bia , and show that #it# is competitive to the best other $strategies$ .	6	compare	compare	1
the objective is a generic #system# of tools , including a core english lexicon , grammar , and concept representations , for building $natural language processing -lrb- nlp -rrb- systems$ for text understanding .	0	used-for	used-for	1
the objective is a generic $system$ of tools , including a #core english lexicon# , grammar , and concept representations , for building natural language processing -lrb- nlp -rrb- systems for text understanding .	4	part-of	part-of	1
the objective is a generic $system$ of tools , including a core english lexicon , #grammar# , and concept representations , for building natural language processing -lrb- nlp -rrb- systems for text understanding .	4	part-of	part-of	1
the objective is a generic $system$ of tools , including a core english lexicon , grammar , and #concept representations# , for building natural language processing -lrb- nlp -rrb- systems for text understanding .	4	part-of	part-of	1
the objective is a generic system of tools , including a core english lexicon , grammar , and concept representations , for building #natural language processing -lrb- nlp -rrb- systems# for $text understanding$ .	0	used-for	used-for	1
systems built with #paktus# are intended to generate input to $knowledge based systems$ ordata base systems .	0	used-for	used-for	1
input to the $nlp system$ is typically derived from an existing #electronic message stream# , such as a news wire .	0	used-for	used-for	1
input to the nlp system is typically derived from an existing $electronic message stream$ , such as a #news wire# .	3	hyponym-of	hyponym-of	1
#paktus# supports the adaptation of the generic core to a variety of domains : $jintaccs messages$ , rainform messages , news reports about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , words , conceptual mappings , and discourse patterns .	0	used-for	used-for	1
#paktus# supports the adaptation of the generic core to a variety of domains : jintaccs messages , $rainform messages$ , news reports about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , words , conceptual mappings , and discourse patterns .	0	used-for	used-for	1
#paktus# supports the adaptation of the generic core to a variety of domains : jintaccs messages , rainform messages , $news reports$ about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , words , conceptual mappings , and discourse patterns .	0	used-for	used-for	1
paktus supports the adaptation of the generic core to a variety of domains : #jintaccs messages# , $rainform messages$ , news reports about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , words , conceptual mappings , and discourse patterns .	1	conjunction	conjunction	1
paktus supports the adaptation of the generic core to a variety of domains : jintaccs messages , #rainform messages# , $news reports$ about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , words , conceptual mappings , and discourse patterns .	1	conjunction	conjunction	1
paktus supports the adaptation of the generic core to a variety of domains : jintaccs messages , rainform messages , $news reports$ about a specific type of #event# , such as financial transfers or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , words , conceptual mappings , and discourse patterns .	5	feature-of	feature-of	1
paktus supports the adaptation of the generic core to a variety of domains : jintaccs messages , rainform messages , news reports about a specific type of $event$ , such as #financial transfers# or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , words , conceptual mappings , and discourse patterns .	3	hyponym-of	hyponym-of	1
paktus supports the adaptation of the generic core to a variety of domains : jintaccs messages , rainform messages , news reports about a specific type of event , such as #financial transfers# or $terrorist acts$ , etc. , by acquiring sublanguage and domain-specific grammar , words , conceptual mappings , and discourse patterns .	1	conjunction	conjunction	1
paktus supports the adaptation of the generic core to a variety of domains : jintaccs messages , rainform messages , news reports about a specific type of $event$ , such as financial transfers or #terrorist acts# , etc. , by acquiring sublanguage and domain-specific grammar , words , conceptual mappings , and discourse patterns .	3	hyponym-of	hyponym-of	1
$paktus$ supports the adaptation of the generic core to a variety of domains : jintaccs messages , rainform messages , news reports about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring #sublanguage and domain-specific grammar# , words , conceptual mappings , and discourse patterns .	0	used-for	used-for	1
paktus supports the adaptation of the generic core to a variety of domains : jintaccs messages , rainform messages , news reports about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring #sublanguage and domain-specific grammar# , $words$ , conceptual mappings , and discourse patterns .	1	conjunction	conjunction	1
$paktus$ supports the adaptation of the generic core to a variety of domains : jintaccs messages , rainform messages , news reports about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , #words# , conceptual mappings , and discourse patterns .	0	used-for	used-for	1
paktus supports the adaptation of the generic core to a variety of domains : jintaccs messages , rainform messages , news reports about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , #words# , $conceptual mappings$ , and discourse patterns .	1	conjunction	conjunction	1
$paktus$ supports the adaptation of the generic core to a variety of domains : jintaccs messages , rainform messages , news reports about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , words , #conceptual mappings# , and discourse patterns .	0	used-for	used-for	1
paktus supports the adaptation of the generic core to a variety of domains : jintaccs messages , rainform messages , news reports about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , words , #conceptual mappings# , and $discourse patterns$ .	1	conjunction	conjunction	1
$paktus$ supports the adaptation of the generic core to a variety of domains : jintaccs messages , rainform messages , news reports about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , words , conceptual mappings , and #discourse patterns# .	0	used-for	used-for	1
in this paper the $limsi recognizer$ which was evaluated in the #arpa nov93 csr test# is described , and experimental results on the wsj and bref corpora under closely matched conditions are reported .	2	evaluate-for	evaluate-for	1
in this paper the $limsi recognizer$ which was evaluated in the arpa nov93 csr test is described , and experimental results on the #wsj and bref corpora# under closely matched conditions are reported .	2	evaluate-for	evaluate-for	1
for both #corpora# $word recognition$ experiments were carried out with vocabularies containing up to 20k words .	2	evaluate-for	evaluate-for	1
the $recognizer$ makes use of #continuous density hmm# with gaussian mixture for acoustic modeling and n-gram statistics estimated on the newspaper texts for language modeling .	0	used-for	used-for	1
the recognizer makes use of #continuous density hmm# with $gaussian mixture$ for acoustic modeling and n-gram statistics estimated on the newspaper texts for language modeling .	1	conjunction	conjunction	1
the recognizer makes use of #continuous density hmm# with gaussian mixture for $acoustic modeling$ and n-gram statistics estimated on the newspaper texts for language modeling .	0	used-for	used-for	1
the recognizer makes use of #continuous density hmm# with gaussian mixture for acoustic modeling and $n-gram statistics$ estimated on the newspaper texts for language modeling .	1	conjunction	conjunction	1
the recognizer makes use of continuous density hmm with #gaussian mixture# for $acoustic modeling$ and n-gram statistics estimated on the newspaper texts for language modeling .	0	used-for	used-for	1
the $recognizer$ makes use of continuous density hmm with gaussian mixture for acoustic modeling and #n-gram statistics# estimated on the newspaper texts for language modeling .	0	used-for	used-for	1
the recognizer makes use of continuous density hmm with gaussian mixture for acoustic modeling and #n-gram statistics# estimated on the newspaper texts for $language modeling$ .	0	used-for	used-for	1
the recognizer makes use of continuous density hmm with gaussian mixture for acoustic modeling and $n-gram statistics$ estimated on the #newspaper texts# for language modeling .	2	evaluate-for	evaluate-for	1
the $recognizer$ uses a #time-synchronous graph-search strategy# which is shown to still be viable with a 20k-word vocabulary when used with bigram back-off language models .	0	used-for	used-for	1
the $recognizer$ uses a time-synchronous graph-search strategy which is shown to still be viable with a 20k-word vocabulary when used with #bigram back-off language models# .	0	used-for	used-for	1
the recognizer uses a $time-synchronous graph-search strategy$ which is shown to still be viable with a 20k-word vocabulary when used with #bigram back-off language models# .	1	conjunction	conjunction	1
a second forward pass , which makes use of a $word graph$ generated with the #bigram# , incorporates a trigram language model .	0	used-for	used-for	1
a second forward pass , which makes use of a $word graph$ generated with the bigram , incorporates a #trigram language model# .	1	conjunction	conjunction	1
$acoustic modeling$ uses #cepstrum-based features# , context-dependent phone models -lrb- intra and interword -rrb- , phone duration models , and sex-dependent models .	0	used-for	used-for	1
acoustic modeling uses #cepstrum-based features# , $context-dependent phone models -lrb- intra and interword -rrb-$ , phone duration models , and sex-dependent models .	1	conjunction	conjunction	1
$acoustic modeling$ uses cepstrum-based features , #context-dependent phone models -lrb- intra and interword -rrb-# , phone duration models , and sex-dependent models .	0	used-for	used-for	1
acoustic modeling uses cepstrum-based features , #context-dependent phone models -lrb- intra and interword -rrb-# , $phone duration models$ , and sex-dependent models .	1	conjunction	conjunction	1
$acoustic modeling$ uses cepstrum-based features , context-dependent phone models -lrb- intra and interword -rrb- , #phone duration models# , and sex-dependent models .	0	used-for	used-for	1
acoustic modeling uses cepstrum-based features , context-dependent phone models -lrb- intra and interword -rrb- , #phone duration models# , and $sex-dependent models$ .	1	conjunction	conjunction	1
$acoustic modeling$ uses cepstrum-based features , context-dependent phone models -lrb- intra and interword -rrb- , phone duration models , and #sex-dependent models# .	0	used-for	used-for	1
the #co-occurrence pattern# , a combination of binary or local features , is more discriminative than individual features and has shown its advantages in $object , scene , and action recognition$ .	0	used-for	used-for	1
the $co-occurrence pattern$ , a combination of #binary or local features# , is more discriminative than individual features and has shown its advantages in object , scene , and action recognition .	4	part-of	part-of	1
then we propose a novel #data mining method# to efficiently discover the $optimal co-occurrence pattern$ with minimum empirical error , despite the noisy training dataset .	0	used-for	used-for	1
then we propose a novel data mining method to efficiently discover the $optimal co-occurrence pattern$ with #minimum empirical error# , despite the noisy training dataset .	5	feature-of	feature-of	1
then we propose a novel $data mining method$ to efficiently discover the optimal co-occurrence pattern with minimum empirical error , despite the #noisy training dataset# .	0	used-for	used-for	1
this #mining procedure# of $and and or patterns$ is readily integrated to boosting , which improves the generalization ability over the conventional boosting decision trees and boosting decision stumps .	0	used-for	used-for	1
this mining procedure of #and and or patterns# is readily integrated to $boosting$ , which improves the generalization ability over the conventional boosting decision trees and boosting decision stumps .	4	part-of	part-of	1
this mining procedure of and and or patterns is readily integrated to #boosting# , which improves the generalization ability over the conventional $boosting decision trees$ and boosting decision stumps .	6	compare	compare	1
this mining procedure of and and or patterns is readily integrated to #boosting# , which improves the generalization ability over the conventional boosting decision trees and $boosting decision stumps$ .	6	compare	compare	1
this mining procedure of and and or patterns is readily integrated to $boosting$ , which improves the #generalization ability# over the conventional boosting decision trees and boosting decision stumps .	2	evaluate-for	evaluate-for	1
this mining procedure of and and or patterns is readily integrated to boosting , which improves the #generalization ability# over the conventional $boosting decision trees$ and boosting decision stumps .	2	evaluate-for	evaluate-for	1
this mining procedure of and and or patterns is readily integrated to boosting , which improves the #generalization ability# over the conventional boosting decision trees and $boosting decision stumps$ .	2	evaluate-for	evaluate-for	1
this mining procedure of and and or patterns is readily integrated to boosting , which improves the generalization ability over the conventional #boosting decision trees# and $boosting decision stumps$ .	1	conjunction	conjunction	1
our versatile experiments on #object , scene , and action cat-egorization# validate the advantages of the discovered $dis-criminative co-occurrence patterns$ .	2	evaluate-for	evaluate-for	1
empirical experience and observations have shown us when powerful and highly tunable #classifiers# such as maximum entropy classifiers , boosting and svms are applied to $language processing tasks$ , it is possible to achieve high accuracies , but eventually their performances all tend to plateau out at around the same point .	0	used-for	used-for	1
empirical experience and observations have shown us when powerful and highly tunable $classifiers$ such as #maximum entropy classifiers# , boosting and svms are applied to language processing tasks , it is possible to achieve high accuracies , but eventually their performances all tend to plateau out at around the same point .	3	hyponym-of	hyponym-of	1
empirical experience and observations have shown us when powerful and highly tunable classifiers such as #maximum entropy classifiers# , $boosting$ and svms are applied to language processing tasks , it is possible to achieve high accuracies , but eventually their performances all tend to plateau out at around the same point .	1	conjunction	conjunction	1
empirical experience and observations have shown us when powerful and highly tunable $classifiers$ such as maximum entropy classifiers , #boosting# and svms are applied to language processing tasks , it is possible to achieve high accuracies , but eventually their performances all tend to plateau out at around the same point .	3	hyponym-of	hyponym-of	1
empirical experience and observations have shown us when powerful and highly tunable classifiers such as maximum entropy classifiers , #boosting# and $svms$ are applied to language processing tasks , it is possible to achieve high accuracies , but eventually their performances all tend to plateau out at around the same point .	1	conjunction	conjunction	1
empirical experience and observations have shown us when powerful and highly tunable $classifiers$ such as maximum entropy classifiers , boosting and #svms# are applied to language processing tasks , it is possible to achieve high accuracies , but eventually their performances all tend to plateau out at around the same point .	3	hyponym-of	hyponym-of	1
in recent work , we introduced #n-fold templated piped correction , or ntpc -lrb- `` nitpick '' -rrb-# , an intriguing $error corrector$ that is designed to work in these extreme operating conditions .	3	hyponym-of	hyponym-of	1
despite its simplicity , #it# consistently and robustly improves the accuracy of existing highly accurate $base models$ .	6	compare	compare	1
focused interaction of this kind is facilitated by a #construction-specific approach# to $flexible parsing$ , with specialized parsing techniques for each type of construction , and specialized ambiguity representations for each type of ambiguity that a particular construction can give rise to .	0	used-for	used-for	1
focused interaction of this kind is facilitated by a #construction-specific approach# to flexible parsing , with $specialized parsing techniques$ for each type of construction , and specialized ambiguity representations for each type of ambiguity that a particular construction can give rise to .	1	conjunction	conjunction	1
focused interaction of this kind is facilitated by a construction-specific approach to flexible parsing , with #specialized parsing techniques# for each type of $construction$ , and specialized ambiguity representations for each type of ambiguity that a particular construction can give rise to .	0	used-for	used-for	1
focused interaction of this kind is facilitated by a construction-specific approach to flexible parsing , with #specialized parsing techniques# for each type of construction , and specialized $ambiguity representations$ for each type of ambiguity that a particular construction can give rise to .	1	conjunction	conjunction	1
focused interaction of this kind is facilitated by a construction-specific approach to flexible parsing , with specialized parsing techniques for each type of construction , and specialized #ambiguity representations# for each type of $ambiguity$ that a particular construction can give rise to .	0	used-for	used-for	1
a #construction-specific approach# also aids in $task-specific language development$ by allowing a language definition that is natural in terms of the task domain to be interpreted directly without compilation into a uniform grammar formalism , thus greatly speeding the testing of changes to the language definition .	0	used-for	used-for	1
a proposal to deal with $french tenses$ in the framework of #discourse representation theory# is presented , as it has been implemented for a fragment at the ims .	0	used-for	used-for	1
a proposal to deal with french tenses in the framework of discourse representation theory is presented , as #it# has been implemented for a fragment at the $ims$ .	0	used-for	used-for	1
$it$ is based on the #theory of tenses# of h. kamp and ch .	0	used-for	used-for	1
instead of using #operators# to express the $meaning of the tenses$ the reichenbachian point of view is adopted and refined such that the impact of the tenses with respect to the meaning of the text is understood as contribution to the integration of the events of a sentence in the event structure of the preceeding text .	0	used-for	used-for	1
thereby a $system of relevant times$ provided by the #preceeding text# and by the temporal adverbials of the sentence being processed is used .	0	used-for	used-for	1
thereby a system of relevant times provided by the #preceeding text# and by the $temporal adverbials$ of the sentence being processed is used .	1	conjunction	conjunction	1
thereby a $system of relevant times$ provided by the preceeding text and by the #temporal adverbials# of the sentence being processed is used .	0	used-for	used-for	1
this $system$ consists of one or more #reference times# and temporal perspective times , the speech time and the location time .	4	part-of	part-of	1
this system consists of one or more #reference times# and $temporal perspective times$ , the speech time and the location time .	1	conjunction	conjunction	1
this $system$ consists of one or more reference times and #temporal perspective times# , the speech time and the location time .	4	part-of	part-of	1
this system consists of one or more reference times and #temporal perspective times# , the $speech time$ and the location time .	1	conjunction	conjunction	1
this $system$ consists of one or more reference times and temporal perspective times , the #speech time# and the location time .	4	part-of	part-of	1
this system consists of one or more reference times and temporal perspective times , the #speech time# and the $location time$ .	1	conjunction	conjunction	1
this $system$ consists of one or more reference times and temporal perspective times , the speech time and the #location time# .	4	part-of	part-of	1
in opposition to the approach of kamp and rohrer the exact $meaning of the tenses$ is fixed by the #resolution component# and not in the process of syntactic analysis .	0	used-for	used-for	1
in opposition to the approach of kamp and rohrer the exact meaning of the tenses is fixed by the #resolution component# and not in the process of $syntactic analysis$ .	6	compare	compare	1
the work presented in this paper is the first step in a project which aims to cluster and summarise #electronic discussions# in the context of $help-desk applications$ .	4	part-of	part-of	1
in this paper , we identify #features# of $electronic discussions$ that influence the clustering process , and offer a filtering mechanism that removes undesirable influences .	5	feature-of	feature-of	1
we tested the $clustering and filtering processes$ on #electronic newsgroup discussions# , and evaluated their performance by means of two experiments : coarse-level clustering simple information retrieval .	2	evaluate-for	evaluate-for	1
we tested the $clustering and filtering processes$ on electronic newsgroup discussions , and evaluated their performance by means of two #experiments# : coarse-level clustering simple information retrieval .	2	evaluate-for	evaluate-for	1
we tested the $clustering and filtering processes$ on electronic newsgroup discussions , and evaluated their performance by means of two #experiments# : coarse-level clustering simple information retrieval .	2	evaluate-for	evaluate-for	1
we tested the clustering and filtering processes on electronic newsgroup discussions , and evaluated their performance by means of two $experiments$ : #coarse-level clustering# simple information retrieval .	3	hyponym-of	hyponym-of	1
we tested the clustering and filtering processes on electronic newsgroup discussions , and evaluated their performance by means of two $experiments$ : coarse-level clustering simple #information retrieval# .	3	hyponym-of	hyponym-of	1
the paper presents a #method# for $word sense disambiguation$ based on parallel corpora .	0	used-for	used-for	1
the paper presents a $method$ for word sense disambiguation based on #parallel corpora# .	0	used-for	used-for	1
the #method# exploits recent advances in $word alignment$ and word clustering based on automatic extraction of translation equivalents and being supported by available aligned wordnets for the languages in the corpus .	0	used-for	used-for	1
the #method# exploits recent advances in word alignment and $word clustering$ based on automatic extraction of translation equivalents and being supported by available aligned wordnets for the languages in the corpus .	0	used-for	used-for	1
the method exploits recent advances in #word alignment# and $word clustering$ based on automatic extraction of translation equivalents and being supported by available aligned wordnets for the languages in the corpus .	1	conjunction	conjunction	1
the $method$ exploits recent advances in word alignment and word clustering based on #automatic extraction of translation equivalents# and being supported by available aligned wordnets for the languages in the corpus .	0	used-for	used-for	1
the $method$ exploits recent advances in word alignment and word clustering based on automatic extraction of translation equivalents and being supported by available #aligned wordnets# for the languages in the corpus .	0	used-for	used-for	1
the same #system# used in a validation mode , can be used to check and spot $alignment errors in multilingually aligned wordnets$ as balkanet and eurowordnet .	0	used-for	used-for	1
the same system used in a validation mode , can be used to check and spot alignment errors in $multilingually aligned wordnets$ as #balkanet# and eurowordnet .	3	hyponym-of	hyponym-of	1
the same system used in a validation mode , can be used to check and spot alignment errors in multilingually aligned wordnets as #balkanet# and $eurowordnet$ .	1	conjunction	conjunction	1
the same system used in a validation mode , can be used to check and spot alignment errors in $multilingually aligned wordnets$ as balkanet and #eurowordnet# .	3	hyponym-of	hyponym-of	1
this paper investigates critical configurations for $projective reconstruction$ from multiple #images# taken by a camera moving in a straight line .	0	used-for	used-for	1
projective reconstruction refers to a determination of the #3d geometrical configuration# of a set of $3d points and cameras$ , given only correspondences between points in the images .	5	feature-of	feature-of	1
porting a #natural language processing -lrb- nlp -rrb- system# to a $new domain$ remains one of the bottlenecks in syntactic parsing , because of the amount of effort required to fix gaps in the lexicon , and to attune the existing grammar to the idiosyncracies of the new sublanguage .	0	used-for	used-for	1
porting a natural language processing -lrb- nlp -rrb- system to a new domain remains one of the bottlenecks in syntactic parsing , because of the amount of effort required to fix gaps in the lexicon , and to attune the existing #grammar# to the $idiosyncracies of the new sublanguage$ .	0	used-for	used-for	1
this paper shows how the process of fitting a lexicalized grammar to a domain can be automated to a great extent by using a $hybrid system$ that combines traditional #knowledge-based techniques# with a corpus-based approach .	4	part-of	part-of	1
this paper shows how the process of fitting a lexicalized grammar to a domain can be automated to a great extent by using a hybrid system that combines traditional #knowledge-based techniques# with a $corpus-based approach$ .	1	conjunction	conjunction	1
this paper shows how the process of fitting a lexicalized grammar to a domain can be automated to a great extent by using a $hybrid system$ that combines traditional knowledge-based techniques with a #corpus-based approach# .	4	part-of	part-of	1
unification is often the appropriate #method# for expressing $relations between representations$ in the form of feature structures ; however , there are circumstances in which a different approach is desirable .	0	used-for	used-for	1
unification is often the appropriate method for expressing $relations between representations$ in the form of #feature structures# ; however , there are circumstances in which a different approach is desirable .	0	used-for	used-for	1
unification is often the appropriate $method$ for expressing relations between representations in the form of feature structures ; however , there are circumstances in which a different #approach# is desirable .	6	compare	compare	1
a $declarative formalism$ is presented which permits #direct mappings of one feature structure into another# , and illustrative examples are given of its application to areas of current interest .	5	feature-of	feature-of	1
to support engaging human users in robust , mixed-initiative speech dialogue interactions which reach beyond current capabilities in dialogue systems , the darpa communicator program -lsb- 1 -rsb- is funding the development of a #distributed message-passing infrastructure# for $dialogue systems$ which all communicator participants are using .	0	used-for	used-for	1
we propose a novel #limited-memory stochastic block bfgs update# for $incorporating enriched curvature information in stochastic approximation methods$ .	0	used-for	used-for	1
in our method , the estimate of the $inverse hessian matrix$ that is maintained by #it# , is updated at each iteration using a sketch of the hessian , i.e. , a randomly generated compressed form of the hessian .	0	used-for	used-for	1
in our method , the estimate of the inverse hessian matrix that is maintained by $it$ , is updated at each iteration using a sketch of the #hessian# , i.e. , a randomly generated compressed form of the hessian .	0	used-for	used-for	1
in our method , the estimate of the inverse hessian matrix that is maintained by it , is updated at each iteration using a sketch of the $hessian$ , i.e. , a #randomly generated compressed form of the hessian# .	3	hyponym-of	hyponym-of	1
we propose several sketching strategies , present a new #quasi-newton method# that uses stochastic block bfgs updates combined with the variance reduction approach svrg to compute $batch stochastic gradients$ , and prove linear convergence of the resulting method .	0	used-for	used-for	1
we propose several sketching strategies , present a new $quasi-newton method$ that uses #stochastic block bfgs updates# combined with the variance reduction approach svrg to compute batch stochastic gradients , and prove linear convergence of the resulting method .	0	used-for	used-for	1
we propose several sketching strategies , present a new quasi-newton method that uses #stochastic block bfgs updates# combined with the $variance reduction approach svrg$ to compute batch stochastic gradients , and prove linear convergence of the resulting method .	1	conjunction	conjunction	1
we propose several sketching strategies , present a new $quasi-newton method$ that uses stochastic block bfgs updates combined with the #variance reduction approach svrg# to compute batch stochastic gradients , and prove linear convergence of the resulting method .	0	used-for	used-for	1
we propose several sketching strategies , present a new quasi-newton method that uses stochastic block bfgs updates combined with the variance reduction approach svrg to compute batch stochastic gradients , and prove #linear convergence# of the resulting $method$ .	5	feature-of	feature-of	1
numerical tests on #large-scale logistic regression problems# reveal that our $method$ is more robust and substantially outperforms current state-of-the-art methods .	2	evaluate-for	evaluate-for	1
numerical tests on #large-scale logistic regression problems# reveal that our method is more robust and substantially outperforms current $state-of-the-art methods$ .	2	evaluate-for	evaluate-for	1
numerical tests on large-scale logistic regression problems reveal that our #method# is more robust and substantially outperforms current $state-of-the-art methods$ .	6	compare	compare	1
the goal of this research is to develop a #spoken language system# that will demonstrate the usefulness of voice input for $interactive problem solving$ .	0	used-for	used-for	1
the goal of this research is to develop a spoken language system that will demonstrate the usefulness of #voice input# for $interactive problem solving$ .	0	used-for	used-for	1
combining #speech recognition# and $natural language processing$ to achieve speech understanding , the system will be demonstrated in an application domain relevant to the dod .	1	conjunction	conjunction	1
combining #speech recognition# and natural language processing to achieve $speech understanding$ , the system will be demonstrated in an application domain relevant to the dod .	0	used-for	used-for	1
combining speech recognition and #natural language processing# to achieve $speech understanding$ , the system will be demonstrated in an application domain relevant to the dod .	0	used-for	used-for	1
the objective of this project is to develop a $robust and high-performance speech recognition system$ using a #segment-based approach# to phonetic recognition .	0	used-for	used-for	1
the objective of this project is to develop a robust and high-performance speech recognition system using a #segment-based approach# to $phonetic recognition$ .	0	used-for	used-for	1
the objective of this project is to develop a $robust and high-performance speech recognition system$ using a segment-based approach to #phonetic recognition# .	0	used-for	used-for	1
the #recognition system# will eventually be integrated with natural language processing to achieve $spoken language understanding$ .	0	used-for	used-for	1
the $recognition system$ will eventually be integrated with #natural language processing# to achieve spoken language understanding .	1	conjunction	conjunction	1
the recognition system will eventually be integrated with #natural language processing# to achieve $spoken language understanding$ .	0	used-for	used-for	1
#spelling-checkers# have become an integral part of most $text processing software$ .	4	part-of	part-of	1
from different reasons among which the speed of processing prevails $they$ are usually based on #dictionaries of word forms# instead of words .	0	used-for	used-for	1
this $approach$ is sufficient for #languages# with little inflection such as english , but fails for highly inflective languages such as czech , russian , slovak or other slavonic languages .	0	used-for	used-for	1
this approach is sufficient for $languages$ with little #inflection# such as english , but fails for highly inflective languages such as czech , russian , slovak or other slavonic languages .	5	feature-of	feature-of	1
this approach is sufficient for $languages$ with little inflection such as #english# , but fails for highly inflective languages such as czech , russian , slovak or other slavonic languages .	3	hyponym-of	hyponym-of	1
this approach is sufficient for languages with little inflection such as english , but fails for $highly inflective languages$ such as #czech# , russian , slovak or other slavonic languages .	3	hyponym-of	hyponym-of	1
this approach is sufficient for languages with little inflection such as english , but fails for highly inflective languages such as #czech# , $russian$ , slovak or other slavonic languages .	1	conjunction	conjunction	1
this approach is sufficient for languages with little inflection such as english , but fails for $highly inflective languages$ such as czech , #russian# , slovak or other slavonic languages .	3	hyponym-of	hyponym-of	1
this approach is sufficient for languages with little inflection such as english , but fails for highly inflective languages such as czech , #russian# , $slovak$ or other slavonic languages .	1	conjunction	conjunction	1
this approach is sufficient for languages with little inflection such as english , but fails for $highly inflective languages$ such as czech , russian , #slovak# or other slavonic languages .	3	hyponym-of	hyponym-of	1
this approach is sufficient for languages with little inflection such as english , but fails for highly inflective languages such as czech , russian , #slovak# or other $slavonic languages$ .	1	conjunction	conjunction	1
this approach is sufficient for languages with little inflection such as english , but fails for $highly inflective languages$ such as czech , russian , slovak or other #slavonic languages# .	3	hyponym-of	hyponym-of	1
we have developed a special #method# for describing $inflection$ for the purpose of building spelling-checkers for such languages .	0	used-for	used-for	1
we have developed a special #method# for describing inflection for the purpose of building $spelling-checkers$ for such languages .	0	used-for	used-for	1
we have developed a special method for describing inflection for the purpose of building #spelling-checkers# for such $languages$ .	0	used-for	used-for	1
the speed of the resulting program lies somewhere in the middle of the scale of existing $spelling-checkers$ for #english# and the main dictionary fits into the standard 360k floppy , whereas the number of recognized word forms exceeds 6 million -lrb- for czech -rrb- .	0	used-for	used-for	1
further , a special #method# has been developed for easy $word classification$ .	0	used-for	used-for	1
we present a new hmm tagger that exploits context on both sides of a word to be tagged , and evaluate $it$ in both the #unsupervised and supervised case# .	2	evaluate-for	evaluate-for	1
along the way , we present the first comprehensive comparison of #unsupervised methods# for $part-of-speech tagging$ , noting that published results to date have not been comparable across corpora or lexicons .	0	used-for	used-for	1
observing that the quality of the lexicon greatly impacts the #accuracy# that can be achieved by the $algorithms$ , we present a method of hmm training that improves accuracy when training of lexical probabilities is unstable .	2	evaluate-for	evaluate-for	1
finally , we show how this new $tagger$ achieves state-of-the-art results in a #supervised , non-training intensive framework# .	2	evaluate-for	evaluate-for	1
we propose a family of #non-uniform sampling strategies# to provably speed up a class of $stochastic optimization algorithms$ with linear convergence including stochastic variance reduced gradient -lrb- svrg -rrb- and stochastic dual coordinate ascent -lrb- sdca -rrb- .	0	used-for	used-for	1
we propose a family of non-uniform sampling strategies to provably speed up a class of $stochastic optimization algorithms$ with #linear convergence# including stochastic variance reduced gradient -lrb- svrg -rrb- and stochastic dual coordinate ascent -lrb- sdca -rrb- .	5	feature-of	feature-of	1
we propose a family of non-uniform sampling strategies to provably speed up a class of $stochastic optimization algorithms$ with linear convergence including #stochastic variance reduced gradient -lrb- svrg -rrb-# and stochastic dual coordinate ascent -lrb- sdca -rrb- .	3	hyponym-of	hyponym-of	1
we propose a family of non-uniform sampling strategies to provably speed up a class of stochastic optimization algorithms with linear convergence including #stochastic variance reduced gradient -lrb- svrg -rrb-# and $stochastic dual coordinate ascent -lrb- sdca -rrb-$ .	1	conjunction	conjunction	1
we propose a family of non-uniform sampling strategies to provably speed up a class of $stochastic optimization algorithms$ with linear convergence including stochastic variance reduced gradient -lrb- svrg -rrb- and #stochastic dual coordinate ascent -lrb- sdca -rrb-# .	3	hyponym-of	hyponym-of	1
for a large family of $penalized empirical risk minimization problems$ , our #methods# exploit data dependent local smoothness of the loss functions near the optimum , while maintaining convergence guarantees .	0	used-for	used-for	1
for a large family of penalized empirical risk minimization problems , our $methods$ exploit #data dependent local smoothness# of the loss functions near the optimum , while maintaining convergence guarantees .	0	used-for	used-for	1
for a large family of penalized empirical risk minimization problems , our methods exploit #data dependent local smoothness# of the $loss functions$ near the optimum , while maintaining convergence guarantees .	5	feature-of	feature-of	1
additionally we present $algorithms$ exploiting #local smoothness# in more aggressive ways , which perform even better in practice .	0	used-for	used-for	1
statistical language modeling remains a challenging #task# , in particular for $morphologically rich languages$ .	0	used-for	used-for	1
recently , new $approaches$ based on #factored language models# have been developed to address this problem .	0	used-for	used-for	1
these models provide principled ways of including additional $conditioning variables$ other than the preceding words , such as #morphological or syntactic features# .	3	hyponym-of	hyponym-of	1
this paper presents an #entirely data-driven model selection procedure# based on genetic search , which is shown to outperform both $knowledge-based and random selection procedures$ on two different language modeling tasks -lrb- arabic and turkish -rrb- .	6	compare	compare	1
this paper presents an $entirely data-driven model selection procedure$ based on #genetic search# , which is shown to outperform both knowledge-based and random selection procedures on two different language modeling tasks -lrb- arabic and turkish -rrb- .	0	used-for	used-for	1
this paper presents an entirely data-driven model selection procedure based on genetic search , which is shown to outperform both #knowledge-based and random selection procedures# on two different $language modeling tasks$ -lrb- arabic and turkish -rrb- .	0	used-for	used-for	1
this paper presents an entirely data-driven model selection procedure based on genetic search , which is shown to outperform both knowledge-based and random selection procedures on two different $language modeling tasks$ -lrb- #arabic# and turkish -rrb- .	3	hyponym-of	hyponym-of	1
this paper presents an entirely data-driven model selection procedure based on genetic search , which is shown to outperform both knowledge-based and random selection procedures on two different language modeling tasks -lrb- #arabic# and $turkish$ -rrb- .	1	conjunction	conjunction	1
this paper presents an entirely data-driven model selection procedure based on genetic search , which is shown to outperform both knowledge-based and random selection procedures on two different $language modeling tasks$ -lrb- arabic and #turkish# -rrb- .	3	hyponym-of	hyponym-of	1
we address appropriate #user modeling# in order to generate $cooperative responses$ to each user in spoken dialogue systems .	0	used-for	used-for	1
we address appropriate #user modeling# in order to generate cooperative responses to each user in $spoken dialogue systems$ .	4	part-of	part-of	1
unlike previous #studies# that focus on user 's knowledge or typical kinds of users , the $user model$ we propose is more comprehensive .	6	compare	compare	1
moreover , the $models$ are automatically derived by #decision tree learning# using real dialogue data collected by the system .	0	used-for	used-for	1
moreover , the models are automatically derived by $decision tree learning$ using #real dialogue data# collected by the system .	0	used-for	used-for	1
moreover , the models are automatically derived by decision tree learning using $real dialogue data$ collected by the #system# .	0	used-for	used-for	1
#dialogue strategies# based on the user modeling are implemented in $kyoto city bus information system$ that has been developed at our laboratory .	0	used-for	used-for	1
$dialogue strategies$ based on the #user modeling# are implemented in kyoto city bus information system that has been developed at our laboratory .	0	used-for	used-for	1
this paper proposes a novel #method# of $building polarity-tagged corpus$ from html documents .	0	used-for	used-for	1
this paper proposes a novel $method$ of building polarity-tagged corpus from #html documents# .	0	used-for	used-for	1
the characteristics of this method is that #it# is fully automatic and can be applied to arbitrary $html documents$ .	0	used-for	used-for	1
the idea behind our $method$ is to utilize certain #layout structures# and linguistic pattern .	0	used-for	used-for	1
the idea behind our method is to utilize certain #layout structures# and $linguistic pattern$ .	1	conjunction	conjunction	1
the idea behind our $method$ is to utilize certain layout structures and #linguistic pattern# .	0	used-for	used-for	1
previous work has used #monolingual parallel corpora# to extract and generate $paraphrases$ .	0	used-for	used-for	1
we show that this $task$ can be done using #bilingual parallel corpora# , a much more commonly available resource .	0	used-for	used-for	1
using #alignment techniques# from $phrase-based statistical machine translation$ , we show how paraphrases in one language can be identified using a phrase in another language as a pivot .	0	used-for	used-for	1
we define a paraphrase probability that allows #paraphrases# extracted from a $bilingual parallel corpus$ to be ranked using translation probabilities , and show how it can be refined to take contextual information into account .	4	part-of	part-of	1
we define a paraphrase probability that allows $paraphrases$ extracted from a bilingual parallel corpus to be ranked using #translation probabilities# , and show how it can be refined to take contextual information into account .	0	used-for	used-for	1
we define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities , and show how $it$ can be refined to take #contextual information# into account .	0	used-for	used-for	1
we evaluate our $paraphrase extraction and ranking methods$ using a set of #manual word alignments# , and contrast the quality with paraphrases extracted from automatic alignments .	2	evaluate-for	evaluate-for	1
we evaluate our paraphrase extraction and ranking methods using a set of manual word alignments , and contrast the #quality# with $paraphrases$ extracted from automatic alignments .	2	evaluate-for	evaluate-for	1
we evaluate our paraphrase extraction and ranking methods using a set of manual word alignments , and contrast the quality with #paraphrases# extracted from $automatic alignments$ .	4	part-of	part-of	1
this paper proposes an automatic , essentially $domain-independent means of evaluating spoken language systems -lrb- sls -rrb-$ which combines #software# we have developed for that purpose -lrb- the '' comparator '' -rrb- and a set of specifications for answer expressions -lrb- the '' common answer specification '' , or cas -rrb- .	4	part-of	part-of	1
this paper proposes an automatic , essentially domain-independent means of evaluating spoken language systems -lrb- sls -rrb- which combines #software# we have developed for that purpose -lrb- the '' comparator '' -rrb- and a set of $specifications$ for answer expressions -lrb- the '' common answer specification '' , or cas -rrb- .	1	conjunction	conjunction	1
this paper proposes an automatic , essentially $domain-independent means of evaluating spoken language systems -lrb- sls -rrb-$ which combines software we have developed for that purpose -lrb- the '' comparator '' -rrb- and a set of #specifications# for answer expressions -lrb- the '' common answer specification '' , or cas -rrb- .	4	part-of	part-of	1
this paper proposes an automatic , essentially domain-independent means of evaluating spoken language systems -lrb- sls -rrb- which combines software we have developed for that purpose -lrb- the '' comparator '' -rrb- and a set of #specifications# for $answer expressions$ -lrb- the '' common answer specification '' , or cas -rrb- .	0	used-for	used-for	1
the #common answer specification# determines the $syntax of answer expressions$ , the minimal content that must be included in them , the data to be included in and excluded from test corpora , and the procedures used by the comparator .	0	used-for	used-for	1
this paper describes an #unsupervised learning method# for $associative relationships between verb phrases$ , which is important in developing reliable q&a systems .	0	used-for	used-for	1
this paper describes an unsupervised learning method for #associative relationships between verb phrases# , which is important in developing reliable $q&a systems$ .	0	used-for	used-for	1
our aim is to develop an #unsupervised learning method# that can obtain such an $associative relationship$ , which we call scenario consistency .	0	used-for	used-for	1
the $method$ we are currently working on uses an #expectation-maximization -lrb- em -rrb- based word-clustering algorithm# , and we have evaluated the effectiveness of this method using japanese verb phrases .	0	used-for	used-for	1
the method we are currently working on uses an expectation-maximization -lrb- em -rrb- based word-clustering algorithm , and we have evaluated the effectiveness of this $method$ using #japanese verb phrases# .	0	used-for	used-for	1
we describe the use of #text data# scraped from the web to augment $language models$ for automatic speech recognition and keyword search for low resource languages .	0	used-for	used-for	1
we describe the use of $text data$ scraped from the #web# to augment language models for automatic speech recognition and keyword search for low resource languages .	5	feature-of	feature-of	1
we describe the use of text data scraped from the web to augment #language models# for $automatic speech recognition$ and keyword search for low resource languages .	0	used-for	used-for	1
we describe the use of text data scraped from the web to augment #language models# for automatic speech recognition and $keyword search$ for low resource languages .	0	used-for	used-for	1
we describe the use of text data scraped from the web to augment language models for #automatic speech recognition# and $keyword search$ for low resource languages .	1	conjunction	conjunction	1
we describe the use of text data scraped from the web to augment language models for $automatic speech recognition$ and keyword search for #low resource languages# .	0	used-for	used-for	1
we describe the use of text data scraped from the web to augment language models for automatic speech recognition and $keyword search$ for #low resource languages# .	0	used-for	used-for	1
we scrape text from multiple $genres$ including #blogs# , online news , translated ted talks , and subtitles .	3	hyponym-of	hyponym-of	1
we scrape text from multiple genres including #blogs# , $online news$ , translated ted talks , and subtitles .	1	conjunction	conjunction	1
we scrape text from multiple $genres$ including blogs , #online news# , translated ted talks , and subtitles .	3	hyponym-of	hyponym-of	1
we scrape text from multiple genres including blogs , #online news# , $translated ted talks$ , and subtitles .	1	conjunction	conjunction	1
we scrape text from multiple $genres$ including blogs , online news , #translated ted talks# , and subtitles .	3	hyponym-of	hyponym-of	1
we scrape text from multiple genres including blogs , online news , #translated ted talks# , and $subtitles$ .	1	conjunction	conjunction	1
we scrape text from multiple $genres$ including blogs , online news , translated ted talks , and #subtitles# .	3	hyponym-of	hyponym-of	1
using #linearly interpolated language models# , we find that blogs and movie subtitles are more relevant for $language modeling of conversational telephone speech$ and obtain large reductions in out-of-vocabulary keywords .	0	used-for	used-for	1
using linearly interpolated language models , we find that #blogs# and $movie subtitles$ are more relevant for language modeling of conversational telephone speech and obtain large reductions in out-of-vocabulary keywords .	1	conjunction	conjunction	1
using linearly interpolated language models , we find that #blogs# and movie subtitles are more relevant for $language modeling of conversational telephone speech$ and obtain large reductions in out-of-vocabulary keywords .	0	used-for	used-for	1
using linearly interpolated language models , we find that blogs and #movie subtitles# are more relevant for $language modeling of conversational telephone speech$ and obtain large reductions in out-of-vocabulary keywords .	0	used-for	used-for	1
furthermore , we show that the #web data# can improve term error rate performance by 3.8 % absolute and maximum term-weighted value in $keyword search$ by 0.0076-0 .1059 absolute points .	0	used-for	used-for	1
furthermore , we show that the web data can improve #term error rate performance# by 3.8 % absolute and maximum term-weighted value in $keyword search$ by 0.0076-0 .1059 absolute points .	2	evaluate-for	evaluate-for	1
furthermore , we show that the web data can improve term error rate performance by 3.8 % absolute and #maximum term-weighted value# in $keyword search$ by 0.0076-0 .1059 absolute points .	2	evaluate-for	evaluate-for	1
pipelined natural language generation -lrb- nlg -rrb- systems have grown increasingly complex as #architectural modules# were added to support $language functionalities$ such as referring expressions , lexical choice , and revision .	0	used-for	used-for	1
pipelined natural language generation -lrb- nlg -rrb- systems have grown increasingly complex as architectural modules were added to support $language functionalities$ such as #referring expressions# , lexical choice , and revision .	3	hyponym-of	hyponym-of	1
pipelined natural language generation -lrb- nlg -rrb- systems have grown increasingly complex as architectural modules were added to support language functionalities such as #referring expressions# , $lexical choice$ , and revision .	1	conjunction	conjunction	1
pipelined natural language generation -lrb- nlg -rrb- systems have grown increasingly complex as architectural modules were added to support $language functionalities$ such as referring expressions , #lexical choice# , and revision .	3	hyponym-of	hyponym-of	1
pipelined natural language generation -lrb- nlg -rrb- systems have grown increasingly complex as architectural modules were added to support language functionalities such as referring expressions , #lexical choice# , and $revision$ .	1	conjunction	conjunction	1
pipelined natural language generation -lrb- nlg -rrb- systems have grown increasingly complex as architectural modules were added to support $language functionalities$ such as referring expressions , lexical choice , and #revision# .	3	hyponym-of	hyponym-of	1
this has given rise to discussions about the relative placement of these new #modules# in the $overall architecture$ .	4	part-of	part-of	1
we present examples which suggest that in a pipelined nlg architecture , the best approach is to strongly tie #it# to a $revision component$ .	1	conjunction	conjunction	1
we present examples which suggest that in a $pipelined nlg architecture$ , the best approach is to strongly tie it to a #revision component# .	4	part-of	part-of	1
finally , we evaluate the $approach$ in a working #multi-page system# .	2	evaluate-for	evaluate-for	1
in this paper a #system# which understands and conceptualizes $scenes descriptions in natural language$ is presented .	0	used-for	used-for	1
specifically , the following #components# of the $system$ are described : the syntactic analyzer , based on a procedural systemic grammar , the semantic analyzer relying on the conceptual dependency theory , and the dictionary .	4	part-of	part-of	1
specifically , the following $components$ of the system are described : the #syntactic analyzer# , based on a procedural systemic grammar , the semantic analyzer relying on the conceptual dependency theory , and the dictionary .	4	part-of	part-of	1
specifically , the following components of the system are described : the #syntactic analyzer# , based on a procedural systemic grammar , the $semantic analyzer$ relying on the conceptual dependency theory , and the dictionary .	1	conjunction	conjunction	1
specifically , the following components of the system are described : the $syntactic analyzer$ , based on a #procedural systemic grammar# , the semantic analyzer relying on the conceptual dependency theory , and the dictionary .	0	used-for	used-for	1
specifically , the following $components$ of the system are described : the syntactic analyzer , based on a procedural systemic grammar , the #semantic analyzer# relying on the conceptual dependency theory , and the dictionary .	4	part-of	part-of	1
specifically , the following components of the system are described : the syntactic analyzer , based on a procedural systemic grammar , the #semantic analyzer# relying on the conceptual dependency theory , and the $dictionary$ .	1	conjunction	conjunction	1
specifically , the following components of the system are described : the syntactic analyzer , based on a procedural systemic grammar , the $semantic analyzer$ relying on the #conceptual dependency theory# , and the dictionary .	0	used-for	used-for	1
specifically , the following $components$ of the system are described : the syntactic analyzer , based on a procedural systemic grammar , the semantic analyzer relying on the conceptual dependency theory , and the #dictionary# .	4	part-of	part-of	1
the base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial #ranking# of these $parses$ .	5	feature-of	feature-of	1
a second #model# then attempts to improve upon this initial $ranking$ , using additional features of the tree as evidence .	0	used-for	used-for	1
a second $model$ then attempts to improve upon this initial ranking , using additional #features# of the tree as evidence .	0	used-for	used-for	1
the strength of our approach is that it allows a tree to be represented as an arbitrary set of features , without concerns about how these features interact or overlap and without the need to define a derivation or a $generative model$ which takes these #features# into account .	0	used-for	used-for	1
we introduce a new #method# for the $reranking task$ , based on the boosting approach to ranking problems described in freund et al. -lrb- 1998 -rrb- .	0	used-for	used-for	1
we introduce a new $method$ for the reranking task , based on the #boosting approach# to ranking problems described in freund et al. -lrb- 1998 -rrb- .	0	used-for	used-for	1
we introduce a new method for the reranking task , based on the #boosting approach# to $ranking problems$ described in freund et al. -lrb- 1998 -rrb- .	0	used-for	used-for	1
we apply the #boosting method# to $parsing$ the wall street journal treebank .	0	used-for	used-for	1
we apply the $boosting method$ to parsing the #wall street journal treebank# .	0	used-for	used-for	1
the $method$ combined the #log-likelihood# under a baseline model -lrb- that of collins -lsb- 1999 -rsb- -rrb- with evidence from an additional 500,000 features over parse trees that were not included in the original model .	4	part-of	part-of	1
the method combined the #log-likelihood# under a $baseline model$ -lrb- that of collins -lsb- 1999 -rsb- -rrb- with evidence from an additional 500,000 features over parse trees that were not included in the original model .	1	conjunction	conjunction	1
the new $model$ achieved 89.75 % #f-measure# , a 13 % relative decrease in f-measure error over the baseline model 's score of 88.2 % .	2	evaluate-for	evaluate-for	1
the new model achieved 89.75 % f-measure , a 13 % relative decrease in #f-measure# error over the $baseline model$ 's score of 88.2 % .	2	evaluate-for	evaluate-for	1
the new $model$ achieved 89.75 % f-measure , a 13 % relative decrease in f-measure error over the #baseline model# 's score of 88.2 % .	6	compare	compare	1
the article also introduces a new #algorithm# for the $boosting approach$ which takes advantage of the sparsity of the feature space in the parsing data .	0	used-for	used-for	1
the article also introduces a new $algorithm$ for the boosting approach which takes advantage of the #sparsity of the feature space# in the parsing data .	0	used-for	used-for	1
the article also introduces a new algorithm for the boosting approach which takes advantage of the #sparsity of the feature space# in the $parsing data$ .	5	feature-of	feature-of	1
experiments show significant efficiency gains for the new #algorithm# over the obvious implementation of the $boosting approach$ .	6	compare	compare	1
we argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on #feature selection methods# within $log-linear -lrb- maximum-entropy -rrb- models$ .	4	part-of	part-of	1
although the experiments in this article are on natural language parsing -lrb- nlp -rrb- , the approach should be applicable to many other $nlp problems$ which are naturally framed as #ranking tasks# , for example , speech recognition , machine translation , or natural language generation .	0	used-for	used-for	1
although the experiments in this article are on natural language parsing -lrb- nlp -rrb- , the approach should be applicable to many other $nlp problems$ which are naturally framed as ranking tasks , for example , #speech recognition# , machine translation , or natural language generation .	3	hyponym-of	hyponym-of	1
although the experiments in this article are on natural language parsing -lrb- nlp -rrb- , the approach should be applicable to many other nlp problems which are naturally framed as $ranking tasks$ , for example , #speech recognition# , machine translation , or natural language generation .	3	hyponym-of	hyponym-of	1
although the experiments in this article are on natural language parsing -lrb- nlp -rrb- , the approach should be applicable to many other nlp problems which are naturally framed as ranking tasks , for example , #speech recognition# , $machine translation$ , or natural language generation .	1	conjunction	conjunction	1
although the experiments in this article are on natural language parsing -lrb- nlp -rrb- , the approach should be applicable to many other $nlp problems$ which are naturally framed as ranking tasks , for example , speech recognition , #machine translation# , or natural language generation .	3	hyponym-of	hyponym-of	1
although the experiments in this article are on natural language parsing -lrb- nlp -rrb- , the approach should be applicable to many other nlp problems which are naturally framed as $ranking tasks$ , for example , speech recognition , #machine translation# , or natural language generation .	3	hyponym-of	hyponym-of	1
although the experiments in this article are on natural language parsing -lrb- nlp -rrb- , the approach should be applicable to many other nlp problems which are naturally framed as ranking tasks , for example , speech recognition , #machine translation# , or $natural language generation$ .	1	conjunction	conjunction	1
although the experiments in this article are on natural language parsing -lrb- nlp -rrb- , the approach should be applicable to many other $nlp problems$ which are naturally framed as ranking tasks , for example , speech recognition , machine translation , or #natural language generation# .	3	hyponym-of	hyponym-of	1
although the experiments in this article are on natural language parsing -lrb- nlp -rrb- , the approach should be applicable to many other nlp problems which are naturally framed as $ranking tasks$ , for example , speech recognition , machine translation , or #natural language generation# .	3	hyponym-of	hyponym-of	1
a #model# is presented to characterize the $class of languages$ obtained by adding reduplication to context-free languages .	0	used-for	used-for	1
a model is presented to characterize the class of languages obtained by adding #reduplication# to $context-free languages$ .	0	used-for	used-for	1
the #model# is a $pushdown automaton$ augmented with the ability to check reduplication by using the stack in a new way .	3	hyponym-of	hyponym-of	1
the model is a $pushdown automaton$ augmented with the ability to check reduplication by using the #stack# in a new way .	0	used-for	used-for	1
the model is a pushdown automaton augmented with the ability to check $reduplication$ by using the #stack# in a new way .	0	used-for	used-for	1
the class of languages generated is shown to lie strictly between the #context-free languages# and the $indexed languages$ .	1	conjunction	conjunction	1
the #model# appears capable of accommodating the sort of $reduplications$ that have been observed to occur in natural languages , but it excludes many of the unnatural constructions that other formal models have permitted .	0	used-for	used-for	1
we present an $image set classification algorithm$ based on #unsupervised clustering# of labeled training and unla-beled test data where labels are only used in the stopping criterion .	0	used-for	used-for	1
we present an image set classification algorithm based on $unsupervised clustering$ of #labeled training and unla-beled test data# where labels are only used in the stopping criterion .	0	used-for	used-for	1
the #probability distribution# of each class over the set of clusters is used to define a true $set based similarity measure$ .	0	used-for	used-for	1
in each iteration , a #proximity matrix# is efficiently recomputed to better represent the $local subspace structure$ .	0	used-for	used-for	1
#initial clusters# capture the $global data structure$ and finer clusters at the later stages capture the subtle class differences not visible at the global scale .	0	used-for	used-for	1
initial clusters capture the global data structure and #finer clusters# at the later stages capture the $subtle class differences$ not visible at the global scale .	0	used-for	used-for	1
$image sets$ are compactly represented with multiple #grass-mannian manifolds# which are subsequently embedded in euclidean space with the proposed spectral clustering algorithm .	0	used-for	used-for	1
image sets are compactly represented with multiple grass-mannian manifolds which are subsequently embedded in $euclidean space$ with the proposed #spectral clustering algorithm# .	0	used-for	used-for	1
we also propose an efficient #eigenvector solver# which not only reduces the computational cost of $spectral clustering$ by many folds but also improves the clustering quality and final classification results .	0	used-for	used-for	1
we also propose an efficient eigenvector solver which not only reduces the #computational cost# of $spectral clustering$ by many folds but also improves the clustering quality and final classification results .	2	evaluate-for	evaluate-for	1
we also propose an efficient eigenvector solver which not only reduces the computational cost of $spectral clustering$ by many folds but also improves the #clustering quality# and final classification results .	2	evaluate-for	evaluate-for	1
we also propose an efficient eigenvector solver which not only reduces the computational cost of $spectral clustering$ by many folds but also improves the clustering quality and final #classification results# .	2	evaluate-for	evaluate-for	1
this paper investigates some #computational problems# associated with $probabilistic translation models$ that have recently been adopted in the literature on machine translation .	5	feature-of	feature-of	1
this paper investigates some computational problems associated with #probabilistic translation models# that have recently been adopted in the literature on $machine translation$ .	0	used-for	used-for	1
these $models$ can be viewed as pairs of #probabilistic context-free grammars# working in a ` synchronous ' way .	5	feature-of	feature-of	1
#active shape models# are a powerful and widely used tool to interpret $complex image data$ .	0	used-for	used-for	1
by building $models of shape variation$ they enable #search algorithms# to use a pri-ori knowledge in an efficient and gainful way .	0	used-for	used-for	1
by building models of shape variation they enable $search algorithms$ to use a #pri-ori knowledge# in an efficient and gainful way .	0	used-for	used-for	1
however , due to the #linearity# of $pca$ , non-linearities like rotations or independently moving sub-parts in the data can deteriorate the resulting model considerably .	5	feature-of	feature-of	1
however , due to the linearity of pca , $non-linearities$ like #rotations# or independently moving sub-parts in the data can deteriorate the resulting model considerably .	3	hyponym-of	hyponym-of	1
although $non-linear extensions of active shape models$ have been proposed and application specific solutions have been used , they still need a certain amount of #user interaction# during model building .	0	used-for	used-for	1
in particular , we propose an $algorithm$ based on the #minimum description length principle# to find an optimal subdivision of the data into sub-parts , each adequate for linear modeling .	0	used-for	used-for	1
which in turn leads to a better $model$ in terms of #modes of variations# .	5	feature-of	feature-of	1
the proposed $method$ is evaluated on #synthetic data# , medical images and hand contours .	2	evaluate-for	evaluate-for	1
the proposed method is evaluated on #synthetic data# , $medical images$ and hand contours .	1	conjunction	conjunction	1
the proposed $method$ is evaluated on synthetic data , #medical images# and hand contours .	2	evaluate-for	evaluate-for	1
the proposed method is evaluated on synthetic data , #medical images# and $hand contours$ .	1	conjunction	conjunction	1
the proposed $method$ is evaluated on synthetic data , medical images and #hand contours# .	2	evaluate-for	evaluate-for	1
we describe a set of experiments to explore #statistical techniques# for $ranking$ and selecting the best translations in a graph of translation hypotheses .	0	used-for	used-for	1
in a previous paper -lrb- carl , 2007 -rrb- we have described how the $hypotheses graph$ is generated through #shallow mapping# and permutation rules .	0	used-for	used-for	1
in a previous paper -lrb- carl , 2007 -rrb- we have described how the hypotheses graph is generated through #shallow mapping# and $permutation rules$ .	1	conjunction	conjunction	1
in a previous paper -lrb- carl , 2007 -rrb- we have described how the $hypotheses graph$ is generated through shallow mapping and #permutation rules# .	0	used-for	used-for	1
this paper describes a number of #methods# for elaborating $statistical feature functions$ from some of the vector components .	0	used-for	used-for	1
this paper describes a number of $methods$ for elaborating statistical feature functions from some of the #vector components# .	0	used-for	used-for	1
the feature functions are trained off-line on different types of text and their #log-linear combination# is then used to retrieve the best m $translation paths$ in the graph .	0	used-for	used-for	1
the feature functions are trained off-line on different types of text and their log-linear combination is then used to retrieve the best m #translation paths# in the $graph$ .	4	part-of	part-of	1
we compare two $language modelling toolkits$ , the #cmu and the sri toolkit# and arrive at three results : 1 -rrb- word-lemma based feature function models produce better results than token-based models , 2 -rrb- adding a pos-tag feature function to the word-lemma model improves the output and 3 -rrb- weights for lexical translations are suitable if the training material is similar to the texts to be translated .	3	hyponym-of	hyponym-of	1
we compare two language modelling toolkits , the cmu and the sri toolkit and arrive at three results : 1 -rrb- #word-lemma based feature function models# produce better results than $token-based models$ , 2 -rrb- adding a pos-tag feature function to the word-lemma model improves the output and 3 -rrb- weights for lexical translations are suitable if the training material is similar to the texts to be translated .	6	compare	compare	1
we compare two language modelling toolkits , the cmu and the sri toolkit and arrive at three results : 1 -rrb- word-lemma based feature function models produce better results than token-based models , 2 -rrb- adding a #pos-tag feature function# to the $word-lemma model$ improves the output and 3 -rrb- weights for lexical translations are suitable if the training material is similar to the texts to be translated .	4	part-of	part-of	1
this paper presents a specialized $editor$ for a highly #structured dictionary# .	0	used-for	used-for	1
the basic goal in building that #editor# was to provide an adequate tool to help lexicologists produce a valid and coherent $dictionary$ on the basis of a linguistic theory .	0	used-for	used-for	1
the basic goal in building that editor was to provide an adequate tool to help lexicologists produce a valid and coherent $dictionary$ on the basis of a #linguistic theory# .	0	used-for	used-for	1
existing techniques extract term candidates by looking for $internal and contextual information$ associated with #domain specific terms# .	5	feature-of	feature-of	1
this paper presents a novel #approach# for $term extraction$ based on delimiters which are much more stable and domain independent .	0	used-for	used-for	1
this paper presents a novel $approach$ for term extraction based on #delimiters# which are much more stable and domain independent .	0	used-for	used-for	1
the proposed #approach# is not as sensitive to $term frequency$ as that of previous works .	6	compare	compare	1
consequently , the proposed approach can be applied to different domains easily and #it# is especially useful for $resource-limited domains$ .	0	used-for	used-for	1
#evaluations# conducted on two different domains for $chinese term extraction$ show significant improvements over existing techniques which verifies its efficiency and domain independent nature .	2	evaluate-for	evaluate-for	1
experiments on new term extraction indicate that the proposed #approach# can also serve as an effective tool for $domain lexicon expansion$ .	0	used-for	used-for	1
we describe a #method# for identifying $systematic patterns in translation data$ using part-of-speech tag sequences .	0	used-for	used-for	1
we describe a $method$ for identifying systematic patterns in translation data using #part-of-speech tag sequences# .	0	used-for	used-for	1
we incorporate this #analysis# into a $diagnostic tool$ intended for developers of machine translation systems , and demonstrate how our application can be used by developers to explore patterns in machine translation output .	4	part-of	part-of	1
we incorporate this analysis into a #diagnostic tool# intended for developers of $machine translation systems$ , and demonstrate how our application can be used by developers to explore patterns in machine translation output .	0	used-for	used-for	1
we incorporate this analysis into a diagnostic tool intended for developers of machine translation systems , and demonstrate how our #application# can be used by developers to explore $patterns in machine translation output$ .	0	used-for	used-for	1
we study the #number of hidden layers# required by a $multilayer neu-ral network$ with threshold units to compute a function f from n d to -lcb- o , i -rcb- .	0	used-for	used-for	1
we study the $number of hidden layers$ required by a multilayer neu-ral network with #threshold units# to compute a function f from n d to -lcb- o , i -rcb- .	0	used-for	used-for	1
we show that adding these conditions to gib-son 's assumptions is not sufficient to ensure global computability with one hidden layer , by exhibiting a new $non-local configuration$ , the #`` critical cycle ''# , which implies that f is not computable with one hidden layer .	3	hyponym-of	hyponym-of	1
this paper presents an #approach# to estimate the $intrinsic texture properties -lrb- albedo , shading , normal -rrb- of scenes$ from multiple view acquisition under unknown illumination conditions .	0	used-for	used-for	1
this paper presents an approach to estimate the $intrinsic texture properties -lrb- albedo , shading , normal -rrb- of scenes$ from #multiple view acquisition# under unknown illumination conditions .	0	used-for	used-for	1
this paper presents an approach to estimate the intrinsic texture properties -lrb- albedo , shading , normal -rrb- of scenes from $multiple view acquisition$ under #unknown illumination conditions# .	5	feature-of	feature-of	1
unlike previous $video relighting methods$ , the #approach# does not assume regions of uniform albedo , which makes it applicable to richly textured scenes .	6	compare	compare	1
unlike previous video relighting methods , the approach does not assume regions of uniform albedo , which makes #it# applicable to $richly textured scenes$ .	0	used-for	used-for	1
we show that #intrinsic image methods# can be used to refine an $initial , low-frequency shading estimate$ based on a global lighting reconstruction from an original texture and coarse scene geometry in order to resolve the inherent global ambiguity in shading .	0	used-for	used-for	1
we show that intrinsic image methods can be used to refine an #initial , low-frequency shading estimate# based on a global lighting reconstruction from an original texture and coarse scene geometry in order to resolve the $inherent global ambiguity in shading$ .	0	used-for	used-for	1
we show that $intrinsic image methods$ can be used to refine an initial , low-frequency shading estimate based on a #global lighting reconstruction# from an original texture and coarse scene geometry in order to resolve the inherent global ambiguity in shading .	0	used-for	used-for	1
we show that intrinsic image methods can be used to refine an initial , low-frequency shading estimate based on a $global lighting reconstruction$ from an original #texture and coarse scene geometry# in order to resolve the inherent global ambiguity in shading .	5	feature-of	feature-of	1
the #method# is applied to $relight-ing of free-viewpoint rendering$ from multiple view video capture .	0	used-for	used-for	1
the method is applied to $relight-ing of free-viewpoint rendering$ from #multiple view video capture# .	0	used-for	used-for	1
this demonstrates $relighting$ with #reproduction of fine surface detail# .	5	feature-of	feature-of	1
following recent developments in the #automatic evaluation# of $machine translation$ and document summarization , we present a similar approach , implemented in a measure called pourpre , for automatically evaluating answers to definition questions .	2	evaluate-for	evaluate-for	1
following recent developments in the #automatic evaluation# of machine translation and $document summarization$ , we present a similar approach , implemented in a measure called pourpre , for automatically evaluating answers to definition questions .	2	evaluate-for	evaluate-for	1
following recent developments in the automatic evaluation of #machine translation# and $document summarization$ , we present a similar approach , implemented in a measure called pourpre , for automatically evaluating answers to definition questions .	1	conjunction	conjunction	1
following recent developments in the automatic evaluation of machine translation and document summarization , we present a similar $approach$ , implemented in a #measure# called pourpre , for automatically evaluating answers to definition questions .	0	used-for	used-for	1
following recent developments in the automatic evaluation of machine translation and document summarization , we present a similar approach , implemented in a #measure# called pourpre , for $automatically evaluating answers to definition questions$ .	0	used-for	used-for	1
experiments with the #trec 2003 and trec 2004 qa tracks# indicate that rankings produced by our $metric$ correlate highly with official rankings , and that pourpre outperforms direct application of existing metrics .	2	evaluate-for	evaluate-for	1
experiments with the #trec 2003 and trec 2004 qa tracks# indicate that rankings produced by our metric correlate highly with official rankings , and that $pourpre$ outperforms direct application of existing metrics .	2	evaluate-for	evaluate-for	1
experiments with the #trec 2003 and trec 2004 qa tracks# indicate that rankings produced by our metric correlate highly with official rankings , and that pourpre outperforms direct application of existing $metrics$ .	2	evaluate-for	evaluate-for	1
experiments with the trec 2003 and trec 2004 qa tracks indicate that $rankings$ produced by our #metric# correlate highly with official rankings , and that pourpre outperforms direct application of existing metrics .	0	used-for	used-for	1
experiments with the trec 2003 and trec 2004 qa tracks indicate that rankings produced by our metric correlate highly with official rankings , and that #pourpre# outperforms direct application of existing $metrics$ .	6	compare	compare	1
recent advances in #automatic speech recognition technology# have put the goal of naturally sounding $dialog systems$ within reach .	0	used-for	used-for	1
the issue of #system response# to users has been extensively studied by the $natural language generation community$ , though rarely in the context of dialog systems .	4	part-of	part-of	1
the issue of system response to users has been extensively studied by the #natural language generation community# , though rarely in the context of $dialog systems$ .	6	compare	compare	1
we show how research in #generation# can be adapted to $dialog systems$ , and how the high cost of hand-crafting knowledge-based generation systems can be overcome by employing machine learning techniques .	0	used-for	used-for	1
we show how research in generation can be adapted to dialog systems , and how the high cost of $hand-crafting knowledge-based generation systems$ can be overcome by employing #machine learning techniques# .	0	used-for	used-for	1
we present a $tool$ , called ilimp , which takes as input a #raw text in french# and produces as output the same text in which every occurrence of the pronoun il is tagged either with tag -lsb- ana -rsb- for anaphoric or -lsb- imp -rsb- for impersonal or expletive .	0	used-for	used-for	1
this #tool# is therefore designed to distinguish between the $anaphoric occurrences of il$ , for which an anaphora resolution system has to look for an antecedent , and the expletive occurrences of this pronoun , for which it does not make sense to look for an antecedent .	0	used-for	used-for	1
this tool is therefore designed to distinguish between the $anaphoric occurrences of il$ , for which an #anaphora resolution system# has to look for an antecedent , and the expletive occurrences of this pronoun , for which it does not make sense to look for an antecedent .	0	used-for	used-for	1
the #precision rate# for $ilimp$ is 97,5 % .	2	evaluate-for	evaluate-for	1
other $tasks$ using the #method# developed for ilimp are described briefly , as well as the use of ilimp in a modular syntactic analysis system .	0	used-for	used-for	1
other tasks using the #method# developed for $ilimp$ are described briefly , as well as the use of ilimp in a modular syntactic analysis system .	0	used-for	used-for	1
other tasks using the method developed for ilimp are described briefly , as well as the use of #ilimp# in a $modular syntactic analysis system$ .	0	used-for	used-for	1
little is thus known about the $robustness$ of #speech cues# in the wild .	5	feature-of	feature-of	1
this study compares the effect of #noise# and $reverberation$ on depression prediction using 1 -rrb- standard mel-frequency cepstral coefficients -lrb- mfccs -rrb- , and 2 -rrb- features designed for noise robustness , damped oscillator cepstral coefficients -lrb- doccs -rrb- .	1	conjunction	conjunction	1
this study compares the effect of #noise# and reverberation on $depression prediction$ using 1 -rrb- standard mel-frequency cepstral coefficients -lrb- mfccs -rrb- , and 2 -rrb- features designed for noise robustness , damped oscillator cepstral coefficients -lrb- doccs -rrb- .	5	feature-of	feature-of	1
this study compares the effect of noise and #reverberation# on $depression prediction$ using 1 -rrb- standard mel-frequency cepstral coefficients -lrb- mfccs -rrb- , and 2 -rrb- features designed for noise robustness , damped oscillator cepstral coefficients -lrb- doccs -rrb- .	5	feature-of	feature-of	1
this study compares the effect of noise and reverberation on $depression prediction$ using 1 -rrb- standard #mel-frequency cepstral coefficients -lrb- mfccs -rrb-# , and 2 -rrb- features designed for noise robustness , damped oscillator cepstral coefficients -lrb- doccs -rrb- .	0	used-for	used-for	1
this study compares the effect of noise and reverberation on depression prediction using 1 -rrb- standard #mel-frequency cepstral coefficients -lrb- mfccs -rrb-# , and 2 -rrb- $features$ designed for noise robustness , damped oscillator cepstral coefficients -lrb- doccs -rrb- .	1	conjunction	conjunction	1
this study compares the effect of noise and reverberation on depression prediction using 1 -rrb- standard mel-frequency cepstral coefficients -lrb- mfccs -rrb- , and 2 -rrb- #features# designed for $noise robustness$ , damped oscillator cepstral coefficients -lrb- doccs -rrb- .	0	used-for	used-for	1
this study compares the effect of noise and reverberation on depression prediction using 1 -rrb- standard mel-frequency cepstral coefficients -lrb- mfccs -rrb- , and 2 -rrb- #features# designed for noise robustness , $damped oscillator cepstral coefficients -lrb- doccs -rrb-$ .	1	conjunction	conjunction	1
results using #additive noise# and $reverberation$ reveal a consistent pattern of findings for multiple evaluation metrics under both matched and mismatched conditions .	1	conjunction	conjunction	1
first and most notably : standard mfcc features suffer dramatically under test/train mismatch for both #noise# and $reverberation$ ; docc features are far more robust .	1	conjunction	conjunction	1
first and most notably : standard $mfcc features$ suffer dramatically under test/train mismatch for both noise and reverberation ; #docc features# are far more robust .	6	compare	compare	1
third , #artificial neural networks# tend to outperform $support vector regression$ .	6	compare	compare	1
fourth , #spontaneous speech# appears to offer better robustness than $read speech$ .	6	compare	compare	1
finally , a #cross-corpus -lrb- and cross-language -rrb- experiment# reveals better noise and reverberation robustness for $doccs$ than for mfccs .	2	evaluate-for	evaluate-for	1
finally , a #cross-corpus -lrb- and cross-language -rrb- experiment# reveals better noise and reverberation robustness for doccs than for $mfccs$ .	2	evaluate-for	evaluate-for	1
finally , a cross-corpus -lrb- and cross-language -rrb- experiment reveals better #noise and reverberation robustness# for $doccs$ than for mfccs .	2	evaluate-for	evaluate-for	1
finally , a cross-corpus -lrb- and cross-language -rrb- experiment reveals better #noise and reverberation robustness# for doccs than for $mfccs$ .	2	evaluate-for	evaluate-for	1
finally , a cross-corpus -lrb- and cross-language -rrb- experiment reveals better noise and reverberation robustness for #doccs# than for $mfccs$ .	6	compare	compare	1
this paper proposes #document oriented preference sets -lrb- dops -rrb-# for the $disambiguation of the dependency structure$ of sentences .	0	used-for	used-for	1
$sentence ambiguities$ can be resolved by using #domain targeted preference knowledge# without using complicated large knowledgebases .	0	used-for	used-for	1
sentence ambiguities can be resolved by using #domain targeted preference knowledge# without using complicated large $knowledgebases$ .	6	compare	compare	1
implementation and empirical results are described for the the analysis of #dependency structures# of $japanese patent claim sentences$ .	5	feature-of	feature-of	1
$multimodal interfaces$ require effective #parsing# and understanding of utterances whose content is distributed across multiple input modes .	0	used-for	used-for	1
johnston 1998 presents an #approach# in which strategies for $multimodal integration$ are stated declaratively using a unification-based grammar that is used by a multidimensional chart parser to compose inputs .	0	used-for	used-for	1
johnston 1998 presents an approach in which strategies for $multimodal integration$ are stated declaratively using a #unification-based grammar# that is used by a multidimensional chart parser to compose inputs .	0	used-for	used-for	1
johnston 1998 presents an approach in which strategies for multimodal integration are stated declaratively using a #unification-based grammar# that is used by a $multidimensional chart parser$ to compose inputs .	0	used-for	used-for	1
in this paper , we present an alternative #approach# in which $multimodal parsing and understanding$ are achieved using a weighted finite-state device which takes speech and gesture streams as inputs and outputs their joint interpretation .	0	used-for	used-for	1
in this paper , we present an alternative approach in which $multimodal parsing and understanding$ are achieved using a #weighted finite-state device# which takes speech and gesture streams as inputs and outputs their joint interpretation .	0	used-for	used-for	1
in this paper , we present an alternative approach in which multimodal parsing and understanding are achieved using a $weighted finite-state device$ which takes #speech and gesture streams# as inputs and outputs their joint interpretation .	0	used-for	used-for	1
this #approach# is significantly more efficient , enables tight-coupling of multimodal understanding with speech recognition , and provides a general probabilistic framework for $multimodal ambiguity resolution$ .	0	used-for	used-for	1
this approach is significantly more efficient , enables tight-coupling of $multimodal understanding$ with #speech recognition# , and provides a general probabilistic framework for multimodal ambiguity resolution .	1	conjunction	conjunction	1
recently , we initiated a project to develop a $phonetically-based spoken language understanding system$ called #summit# .	3	hyponym-of	hyponym-of	1
in contrast to many of the past efforts that make use of $heuristic rules$ whose development requires intense #knowledge engineering# , our approach attempts to express the speech knowledge within a formal framework using well-defined mathematical tools .	0	used-for	used-for	1
in contrast to many of the past efforts that make use of heuristic rules whose development requires intense knowledge engineering , our #approach# attempts to express the $speech knowledge$ within a formal framework using well-defined mathematical tools .	0	used-for	used-for	1
in contrast to many of the past efforts that make use of heuristic rules whose development requires intense knowledge engineering , our approach attempts to express the $speech knowledge$ within a formal framework using well-defined #mathematical tools# .	0	used-for	used-for	1
in our system , #features# and $decision strategies$ are discovered and trained automatically , using a large body of speech data .	1	conjunction	conjunction	1
in our system , features and $decision strategies$ are discovered and trained automatically , using a large body of #speech data# .	0	used-for	used-for	1
this paper describes an implemented $program$ that takes a #tagged text corpus# and generates a partial list of the subcategorization frames in which each verb occurs .	2	evaluate-for	evaluate-for	1
we present a #method# for estimating the $relative pose of two calibrated or uncalibrated non-overlapping surveillance cameras$ from observing a moving object .	0	used-for	used-for	1
we show how to tackle the problem of $missing point correspondences$ heavily required by #sfm pipelines# and how to go beyond this basic paradigm .	0	used-for	used-for	1
we relax the #non-linear nature# of the $problem$ by accepting two assumptions which surveillance scenarios offer , ie .	5	feature-of	feature-of	1
by those assumptions we cast the $problem$ as a #quadratic eigenvalue problem# offering an elegant way of treating nonlinear monomials and delivering a quasi closed-form solution as a reliable starting point for a further bundle adjustment .	0	used-for	used-for	1
by those assumptions we cast the problem as a #quadratic eigenvalue problem# offering an elegant way of treating $nonlinear monomials$ and delivering a quasi closed-form solution as a reliable starting point for a further bundle adjustment .	0	used-for	used-for	1
by those assumptions we cast the problem as a #quadratic eigenvalue problem# offering an elegant way of treating nonlinear monomials and delivering a $quasi closed-form solution$ as a reliable starting point for a further bundle adjustment .	0	used-for	used-for	1
by those assumptions we cast the problem as a quadratic eigenvalue problem offering an elegant way of treating nonlinear monomials and delivering a #quasi closed-form solution# as a reliable starting point for a further $bundle adjustment$ .	0	used-for	used-for	1
we are the first to bring the #closed form solution# to such a very practical $problem$ arising in video surveillance .	0	used-for	used-for	1
we are the first to bring the closed form solution to such a very practical $problem$ arising in #video surveillance# .	5	feature-of	feature-of	1
in this paper , we propose a #human action recognition system# suitable for $embedded computer vision applications$ in security systems , human-computer interaction and intelligent environments .	0	used-for	used-for	1
in this paper , we propose a human action recognition system suitable for #embedded computer vision applications# in $security systems$ , human-computer interaction and intelligent environments .	0	used-for	used-for	1
in this paper , we propose a human action recognition system suitable for #embedded computer vision applications# in security systems , $human-computer interaction$ and intelligent environments .	0	used-for	used-for	1
in this paper , we propose a human action recognition system suitable for #embedded computer vision applications# in security systems , human-computer interaction and $intelligent environments$ .	0	used-for	used-for	1
in this paper , we propose a human action recognition system suitable for embedded computer vision applications in #security systems# , $human-computer interaction$ and intelligent environments .	1	conjunction	conjunction	1
in this paper , we propose a human action recognition system suitable for embedded computer vision applications in security systems , #human-computer interaction# and $intelligent environments$ .	1	conjunction	conjunction	1
our #system# is suitable for $embedded computer vision application$ based on three reasons .	0	used-for	used-for	1
firstly , the $system$ was based on a #linear support vector machine -lrb- svm -rrb- classifier# where classification progress can be implemented easily and quickly in embedded hardware .	0	used-for	used-for	1
firstly , the system was based on a linear support vector machine -lrb- svm -rrb- classifier where $classification progress$ can be implemented easily and quickly in #embedded hardware# .	0	used-for	used-for	1
secondly , we use $compacted motion features$ easily obtained from #videos# .	0	used-for	used-for	1
we address the limitations of the well known motion history image -lrb- mhi -rrb- and propose a new #hierarchical motion history histogram -lrb- hmhh -rrb- feature# to represent the $motion information$ .	0	used-for	used-for	1
#hmhh# not only provides $rich motion information$ , but also remains computationally inexpensive .	0	used-for	used-for	1
finally , we combine #mhi# and $hmhh$ together and extract a low dimension feature vector to be used in the svm classifiers .	1	conjunction	conjunction	1
finally , we combine #mhi# and hmhh together and extract a $low dimension feature vector$ to be used in the svm classifiers .	0	used-for	used-for	1
finally , we combine mhi and #hmhh# together and extract a $low dimension feature vector$ to be used in the svm classifiers .	0	used-for	used-for	1
finally , we combine mhi and hmhh together and extract a #low dimension feature vector# to be used in the $svm classifiers$ .	0	used-for	used-for	1
experimental results show that our $system$ achieves significant improvement on the #recognition# performance .	2	evaluate-for	evaluate-for	1
in this paper i will argue for a $model of grammatical processing$ that is based on #uniform processing# and knowledge sources .	0	used-for	used-for	1
in this paper i will argue for a $model of grammatical processing$ that is based on uniform processing and #knowledge sources# .	0	used-for	used-for	1
in this paper i will argue for a model of grammatical processing that is based on $uniform processing$ and #knowledge sources# .	1	conjunction	conjunction	1
the main feature of this model is to view #parsing# and $generation$ as two strongly interleaved tasks performed by a single parametrized deduction process .	1	conjunction	conjunction	1
the main feature of this model is to view #parsing# and generation as two strongly interleaved $tasks$ performed by a single parametrized deduction process .	3	hyponym-of	hyponym-of	1
the main feature of this model is to view parsing and #generation# as two strongly interleaved $tasks$ performed by a single parametrized deduction process .	3	hyponym-of	hyponym-of	1
the main feature of this model is to view parsing and generation as two strongly interleaved $tasks$ performed by a single #parametrized deduction process# .	0	used-for	used-for	1
#link detection# has been regarded as a core technology for the $topic detection and tracking tasks of new event detection$ .	0	used-for	used-for	1
in this paper we formulate #story link detection# and $new event detection$ as information retrieval task and hypothesize on the impact of precision and recall on both systems .	1	conjunction	conjunction	1
in this paper we formulate #story link detection# and new event detection as information retrieval task and hypothesize on the impact of precision and recall on both $systems$ .	3	hyponym-of	hyponym-of	1
in this paper we formulate story link detection and #new event detection# as information retrieval task and hypothesize on the impact of precision and recall on both $systems$ .	3	hyponym-of	hyponym-of	1
in this paper we formulate $story link detection$ and new event detection as #information retrieval task# and hypothesize on the impact of precision and recall on both systems .	0	used-for	used-for	1
in this paper we formulate story link detection and $new event detection$ as #information retrieval task# and hypothesize on the impact of precision and recall on both systems .	0	used-for	used-for	1
in this paper we formulate story link detection and new event detection as information retrieval task and hypothesize on the impact of #precision# and $recall$ on both systems .	1	conjunction	conjunction	1
in this paper we formulate story link detection and new event detection as information retrieval task and hypothesize on the impact of #precision# and recall on both $systems$ .	2	evaluate-for	evaluate-for	1
in this paper we formulate story link detection and new event detection as information retrieval task and hypothesize on the impact of precision and #recall# on both $systems$ .	2	evaluate-for	evaluate-for	1
motivated by these arguments , we introduce a number of new $performance enhancing techniques$ including #part of speech tagging# , new similarity measures and expanded stop lists .	4	part-of	part-of	1
motivated by these arguments , we introduce a number of new performance enhancing techniques including #part of speech tagging# , new $similarity measures$ and expanded stop lists .	1	conjunction	conjunction	1
motivated by these arguments , we introduce a number of new $performance enhancing techniques$ including part of speech tagging , new #similarity measures# and expanded stop lists .	4	part-of	part-of	1
motivated by these arguments , we introduce a number of new performance enhancing techniques including part of speech tagging , new #similarity measures# and $expanded stop lists$ .	1	conjunction	conjunction	1
motivated by these arguments , we introduce a number of new $performance enhancing techniques$ including part of speech tagging , new similarity measures and #expanded stop lists# .	4	part-of	part-of	1
we attempt to understand $visual classification$ in humans using both #psy-chophysical and machine learning techniques# .	0	used-for	used-for	1
#frontal views of human faces# were used for a $gender classification task$ .	0	used-for	used-for	1
several #hyperplane learning algorithms# were used on the same $classification task$ using the principal components of the texture and flowfield representation of the faces .	0	used-for	used-for	1
several $hyperplane learning algorithms$ were used on the same classification task using the #principal components of the texture# and flowfield representation of the faces .	0	used-for	used-for	1
several $hyperplane learning algorithms$ were used on the same classification task using the principal components of the texture and #flowfield representation of the faces# .	0	used-for	used-for	1
several hyperplane learning algorithms were used on the same classification task using the $principal components of the texture$ and #flowfield representation of the faces# .	1	conjunction	conjunction	1
the $classification$ performance of the #learning algorithms# was estimated using the face database with the true gender of the faces as labels , and also with the gender estimated by the subjects .	0	used-for	used-for	1
the classification performance of the $learning algorithms$ was estimated using the #face database# with the true gender of the faces as labels , and also with the gender estimated by the subjects .	2	evaluate-for	evaluate-for	1
our results suggest that $human classification$ can be modeled by some #hyperplane algorithms# in the feature space we used .	0	used-for	used-for	1
our results suggest that human classification can be modeled by some $hyperplane algorithms$ in the #feature space# we used .	5	feature-of	feature-of	1
for classification , the brain needs more processing for stimuli close to that #hyperplane# than for $those$ further away .	6	compare	compare	1
in this paper , we present a #corpus-based supervised word sense disambiguation -lrb- wsd -rrb- system# for $dutch$ which combines statistical classification -lrb- maximum entropy -rrb- with linguistic information .	0	used-for	used-for	1
in this paper , we present a $corpus-based supervised word sense disambiguation -lrb- wsd -rrb- system$ for dutch which combines #statistical classification# -lrb- maximum entropy -rrb- with linguistic information .	4	part-of	part-of	1
in this paper , we present a $corpus-based supervised word sense disambiguation -lrb- wsd -rrb- system$ for dutch which combines statistical classification -lrb- #maximum entropy# -rrb- with linguistic information .	4	part-of	part-of	1
in this paper , we present a $corpus-based supervised word sense disambiguation -lrb- wsd -rrb- system$ for dutch which combines statistical classification -lrb- maximum entropy -rrb- with #linguistic information# .	4	part-of	part-of	1
in this paper , we present a corpus-based supervised word sense disambiguation -lrb- wsd -rrb- system for dutch which combines statistical classification -lrb- $maximum entropy$ -rrb- with #linguistic information# .	1	conjunction	conjunction	1
instead of building individual #classifiers# per ambiguous wordform , we introduce a $lemma-based approach$ .	6	compare	compare	1
instead of building individual $classifiers$ per #ambiguous wordform# , we introduce a lemma-based approach .	0	used-for	used-for	1
the advantage of this novel method is that it clusters all #inflected forms# of an $ambiguous word$ in one classifier , therefore augmenting the training material available to the algorithm .	5	feature-of	feature-of	1
testing the #lemma-based model# on the dutch senseval-2 test data , we achieve a significant increase in accuracy over the $wordform model$ .	6	compare	compare	1
testing the $lemma-based model$ on the #dutch senseval-2 test data# , we achieve a significant increase in accuracy over the wordform model .	2	evaluate-for	evaluate-for	1
we propose an exact , general and efficient #coarse-to-fine energy minimization strategy# for $semantic video segmenta-tion$ .	0	used-for	used-for	1
our $strategy$ is based on a #hierarchical abstraction of the supervoxel graph# that allows us to minimize an energy defined at the finest level of the hierarchy by minimizing a series of simpler energies defined over coarser graphs .	0	used-for	used-for	1
it is general , i.e. , #it# can be used to minimize any $energy function$ -lrb- e.g. , unary , pairwise , and higher-order terms -rrb- with any existing energy minimization algorithm -lrb- e.g. , graph cuts and belief propagation -rrb- .	0	used-for	used-for	1
it is general , i.e. , #it# can be used to minimize any energy function -lrb- e.g. , unary , pairwise , and higher-order terms -rrb- with any existing $energy minimization algorithm$ -lrb- e.g. , graph cuts and belief propagation -rrb- .	1	conjunction	conjunction	1
it is general , i.e. , it can be used to minimize any $energy function$ -lrb- e.g. , unary , pairwise , and higher-order terms -rrb- with any existing #energy minimization algorithm# -lrb- e.g. , graph cuts and belief propagation -rrb- .	0	used-for	used-for	1
it is general , i.e. , it can be used to minimize any energy function -lrb- e.g. , unary , pairwise , and higher-order terms -rrb- with any existing $energy minimization algorithm$ -lrb- e.g. , #graph cuts# and belief propagation -rrb- .	3	hyponym-of	hyponym-of	1
it is general , i.e. , it can be used to minimize any energy function -lrb- e.g. , unary , pairwise , and higher-order terms -rrb- with any existing energy minimization algorithm -lrb- e.g. , #graph cuts# and $belief propagation$ -rrb- .	1	conjunction	conjunction	1
it is general , i.e. , it can be used to minimize any energy function -lrb- e.g. , unary , pairwise , and higher-order terms -rrb- with any existing $energy minimization algorithm$ -lrb- e.g. , graph cuts and #belief propagation# -rrb- .	3	hyponym-of	hyponym-of	1
#it# also gives significant speedups in $inference$ for several datasets with varying degrees of spatio-temporal continuity .	0	used-for	used-for	1
$it$ also gives significant speedups in inference for several #datasets# with varying degrees of spatio-temporal continuity .	2	evaluate-for	evaluate-for	1
it also gives significant speedups in inference for several $datasets$ with varying degrees of #spatio-temporal continuity# .	5	feature-of	feature-of	1
we also discuss the strengths and weaknesses of our #strategy# relative to existing $hierarchical approaches$ , and the kinds of image and video data that provide the best speedups .	6	compare	compare	1
motivated by the success of #ensemble methods# in $machine learning$ and other areas of natural language processing , we developed a multi-strategy and multi-source approach to question answering which is based on combining the results from different answering agents searching for answers in multiple corpora .	0	used-for	used-for	1
motivated by the success of #ensemble methods# in machine learning and other areas of $natural language processing$ , we developed a multi-strategy and multi-source approach to question answering which is based on combining the results from different answering agents searching for answers in multiple corpora .	0	used-for	used-for	1
motivated by the success of ensemble methods in machine learning and other areas of natural language processing , we developed a #multi-strategy and multi-source approach# to $question answering$ which is based on combining the results from different answering agents searching for answers in multiple corpora .	0	used-for	used-for	1
the $answering agents$ adopt fundamentally different #strategies# , one utilizing primarily knowledge-based mechanisms and the other adopting statistical techniques .	0	used-for	used-for	1
the answering agents adopt fundamentally different $strategies$ , #one# utilizing primarily knowledge-based mechanisms and the other adopting statistical techniques .	3	hyponym-of	hyponym-of	1
the answering agents adopt fundamentally different strategies , $one$ utilizing primarily #knowledge-based mechanisms# and the other adopting statistical techniques .	0	used-for	used-for	1
the answering agents adopt fundamentally different $strategies$ , one utilizing primarily knowledge-based mechanisms and the #other# adopting statistical techniques .	3	hyponym-of	hyponym-of	1
the answering agents adopt fundamentally different strategies , one utilizing primarily knowledge-based mechanisms and the $other$ adopting #statistical techniques# .	0	used-for	used-for	1
we present our $multi-level answer resolution algorithm$ that combines results from the #answering agents# at the question , passage , and/or answer levels .	0	used-for	used-for	1
experiments evaluating the effectiveness of our #answer resolution algorithm# show a 35.0 % relative improvement over our $baseline system$ in the number of questions correctly answered , and a 32.8 % improvement according to the average precision metric .	6	compare	compare	1
experiments evaluating the effectiveness of our $answer resolution algorithm$ show a 35.0 % relative improvement over our baseline system in the number of questions correctly answered , and a 32.8 % improvement according to the #average precision metric# .	2	evaluate-for	evaluate-for	1
experiments evaluating the effectiveness of our answer resolution algorithm show a 35.0 % relative improvement over our $baseline system$ in the number of questions correctly answered , and a 32.8 % improvement according to the #average precision metric# .	2	evaluate-for	evaluate-for	1
#word identification# has been an important and active issue in $chinese natural language processing$ .	3	hyponym-of	hyponym-of	1
in this paper , a new #mechanism# , based on the concept of sublanguage , is proposed for identifying $unknown words$ , especially personal names , in chinese newspapers .	0	used-for	used-for	1
in this paper , a new $mechanism$ , based on the concept of #sublanguage# , is proposed for identifying unknown words , especially personal names , in chinese newspapers .	0	used-for	used-for	1
in this paper , a new mechanism , based on the concept of sublanguage , is proposed for identifying $unknown words$ , especially #personal names# , in chinese newspapers .	3	hyponym-of	hyponym-of	1
in this paper , a new $mechanism$ , based on the concept of sublanguage , is proposed for identifying unknown words , especially personal names , in #chinese newspapers# .	0	used-for	used-for	1
the proposed $mechanism$ includes #title-driven name recognition# , adaptive dynamic word formation , identification of 2-character and 3-character chinese names without title .	4	part-of	part-of	1
the proposed mechanism includes #title-driven name recognition# , $adaptive dynamic word formation$ , identification of 2-character and 3-character chinese names without title .	1	conjunction	conjunction	1
the proposed $mechanism$ includes title-driven name recognition , #adaptive dynamic word formation# , identification of 2-character and 3-character chinese names without title .	4	part-of	part-of	1
the proposed mechanism includes title-driven name recognition , #adaptive dynamic word formation# , $identification of 2-character and 3-character chinese names without title$ .	1	conjunction	conjunction	1
the proposed $mechanism$ includes title-driven name recognition , adaptive dynamic word formation , #identification of 2-character and 3-character chinese names without title# .	4	part-of	part-of	1
this report describes #paul# , a $computer text generation system$ designed to create cohesive text through the use of lexical substitutions .	3	hyponym-of	hyponym-of	1
this report describes paul , a #computer text generation system# designed to create $cohesive text$ through the use of lexical substitutions .	0	used-for	used-for	1
this report describes $paul$ , a computer text generation system designed to create cohesive text through the use of #lexical substitutions# .	0	used-for	used-for	1
specifically , this system is designed to deterministically choose between #pronominalization# , $superordinate substitution$ , and definite noun phrase reiteration .	6	compare	compare	1
specifically , this system is designed to deterministically choose between pronominalization , #superordinate substitution# , and $definite noun phrase reiteration$ .	6	compare	compare	1
the #system# identifies a strength of $antecedence recovery$ for each of the lexical substitutions .	0	used-for	used-for	1
the system identifies a strength of #antecedence recovery# for each of the $lexical substitutions$ .	0	used-for	used-for	1
it describes the automated training and evaluation of an optimal position policy , a #method# of locating the likely $positions of topic-bearing sentences$ based on genre-specific regularities of discourse structure .	0	used-for	used-for	1
it describes the automated training and evaluation of an optimal position policy , a $method$ of locating the likely positions of topic-bearing sentences based on #genre-specific regularities of discourse structure# .	0	used-for	used-for	1
this #method# can be used in $applications$ such as information retrieval , routing , and text summarization .	0	used-for	used-for	1
this method can be used in $applications$ such as #information retrieval# , routing , and text summarization .	3	hyponym-of	hyponym-of	1
this method can be used in applications such as #information retrieval# , $routing$ , and text summarization .	1	conjunction	conjunction	1
this method can be used in $applications$ such as information retrieval , #routing# , and text summarization .	3	hyponym-of	hyponym-of	1
this method can be used in applications such as information retrieval , #routing# , and $text summarization$ .	1	conjunction	conjunction	1
this method can be used in $applications$ such as information retrieval , routing , and #text summarization# .	3	hyponym-of	hyponym-of	1
we describe a general #framework# for $online multiclass learning$ based on the notion of hypothesis sharing .	0	used-for	used-for	1
we describe a general $framework$ for online multiclass learning based on the #notion of hypothesis sharing# .	0	used-for	used-for	1
we generalize the #multiclass perceptron# to our $framework$ and derive a unifying mistake bound analysis .	0	used-for	used-for	1
we demonstrate the merits of our approach by comparing #it# to previous $methods$ on both synthetic and natural datasets .	6	compare	compare	1
we demonstrate the merits of our approach by comparing $it$ to previous methods on both #synthetic and natural datasets# .	2	evaluate-for	evaluate-for	1
we demonstrate the merits of our approach by comparing it to previous $methods$ on both #synthetic and natural datasets# .	2	evaluate-for	evaluate-for	1
we describe a set of #supervised machine learning# experiments centering on the construction of $statistical models of wh-questions$ .	0	used-for	used-for	1
these $models$ , which are built from #shallow linguistic features of questions# , are employed to predict target variables which represent a user 's informational goals .	0	used-for	used-for	1
we argue in favor of the the use of #labeled directed graph# to represent various types of $linguistic structures$ , and illustrate how this allows one to view nlp tasks as graph transformations .	0	used-for	used-for	1
we argue in favor of the the use of #labeled directed graph# to represent various types of linguistic structures , and illustrate how this allows one to view $nlp tasks$ as graph transformations .	0	used-for	used-for	1
we argue in favor of the the use of labeled directed graph to represent various types of linguistic structures , and illustrate how #this# allows one to view $nlp tasks$ as graph transformations .	0	used-for	used-for	1
we present a general #method# for learning such $transformations$ from an annotated corpus and describe experiments with two applications of the method : identification of non-local depenencies -lrb- using penn treebank data -rrb- and semantic role labeling -lrb- using proposition bank data -rrb- .	0	used-for	used-for	1
we present a general $method$ for learning such transformations from an #annotated corpus# and describe experiments with two applications of the method : identification of non-local depenencies -lrb- using penn treebank data -rrb- and semantic role labeling -lrb- using proposition bank data -rrb- .	0	used-for	used-for	1
we present a general method for learning such transformations from an annotated corpus and describe experiments with two $applications$ of the #method# : identification of non-local depenencies -lrb- using penn treebank data -rrb- and semantic role labeling -lrb- using proposition bank data -rrb- .	0	used-for	used-for	1
we present a general method for learning such transformations from an annotated corpus and describe experiments with two $applications$ of the method : #identification of non-local depenencies# -lrb- using penn treebank data -rrb- and semantic role labeling -lrb- using proposition bank data -rrb- .	3	hyponym-of	hyponym-of	1
we present a general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method : $identification of non-local depenencies$ -lrb- using #penn treebank data# -rrb- and semantic role labeling -lrb- using proposition bank data -rrb- .	0	used-for	used-for	1
we present a general method for learning such transformations from an annotated corpus and describe experiments with two $applications$ of the method : identification of non-local depenencies -lrb- using penn treebank data -rrb- and #semantic role labeling# -lrb- using proposition bank data -rrb- .	3	hyponym-of	hyponym-of	1
we present a general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method : identification of non-local depenencies -lrb- using penn treebank data -rrb- and $semantic role labeling$ -lrb- using #proposition bank data# -rrb- .	0	used-for	used-for	1
we describe a generative probabilistic model of natural language , which we call hbg , that takes advantage of detailed #linguistic information# to resolve $ambiguity$ .	0	used-for	used-for	1
#hbg# incorporates lexical , syntactic , semantic , and structural information from the parse tree into the $disambiguation process$ in a novel way .	0	used-for	used-for	1
$hbg$ incorporates #lexical , syntactic , semantic , and structural information# from the parse tree into the disambiguation process in a novel way .	0	used-for	used-for	1
we use a #corpus of bracketed sentences# , called a treebank , in combination with $decision tree building$ to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence .	1	conjunction	conjunction	1
we use a #corpus of bracketed sentences# , called a treebank , in combination with decision tree building to tease out the relevant aspects of a $parse tree$ that will determine the correct parse of a sentence .	0	used-for	used-for	1
we use a corpus of bracketed sentences , called a treebank , in combination with #decision tree building# to tease out the relevant aspects of a $parse tree$ that will determine the correct parse of a sentence .	0	used-for	used-for	1
we use a corpus of bracketed sentences , called a treebank , in combination with decision tree building to tease out the relevant aspects of a #parse tree# that will determine the correct $parse$ of a sentence .	0	used-for	used-for	1
this stands in contrast to the usual approach of further #grammar tailoring# via the usual linguistic introspection in the hope of generating the correct $parse$ .	0	used-for	used-for	1
this stands in contrast to the usual approach of further $grammar tailoring$ via the usual #linguistic introspection# in the hope of generating the correct parse .	0	used-for	used-for	1
in head-to-head tests against one of the best existing $robust probabilistic parsing models$ , which we call #p-cfg# , the hbg model significantly outperforms p-cfg , increasing the parsing accuracy rate from 60 % to 75 % , a 37 % reduction in error .	3	hyponym-of	hyponym-of	1
in head-to-head tests against one of the best existing robust probabilistic parsing models , which we call p-cfg , the #hbg model# significantly outperforms $p-cfg$ , increasing the parsing accuracy rate from 60 % to 75 % , a 37 % reduction in error .	6	compare	compare	1
in head-to-head tests against one of the best existing robust probabilistic parsing models , which we call p-cfg , the $hbg model$ significantly outperforms p-cfg , increasing the #parsing accuracy rate# from 60 % to 75 % , a 37 % reduction in error .	2	evaluate-for	evaluate-for	1
the framework of the $analysis$ is #model-theoretic semantics# .	0	used-for	used-for	1
this paper addresses the issue of $word-sense ambiguity$ in extraction from #machine-readable resources# for the construction of large-scale knowledge sources .	0	used-for	used-for	1
this paper addresses the issue of word-sense ambiguity in extraction from #machine-readable resources# for the $construction of large-scale knowledge sources$ .	0	used-for	used-for	1
we describe two experiments : one which ignored word-sense distinctions , resulting in 6.3 % #accuracy# for $semantic classification$ of verbs based on -lrb- levin , 1993 -rrb- ; and one which exploited word-sense distinctions , resulting in 97.9 % accuracy .	2	evaluate-for	evaluate-for	1
these experiments were dual purpose : -lrb- 1 -rrb- to validate the central thesis of the work of -lrb- levin , 1993 -rrb- , i.e. , that #verb semantics# and $syntactic behavior$ are predictably related ; -lrb- 2 -rrb- to demonstrate that a 15-fold improvement can be achieved in deriving semantic information from syntactic cues if we first divide the syntactic cues into distinct groupings that correlate with different word senses .	1	conjunction	conjunction	1
these experiments were dual purpose : -lrb- 1 -rrb- to validate the central thesis of the work of -lrb- levin , 1993 -rrb- , i.e. , that verb semantics and syntactic behavior are predictably related ; -lrb- 2 -rrb- to demonstrate that a 15-fold improvement can be achieved in deriving $semantic information$ from #syntactic cues# if we first divide the syntactic cues into distinct groupings that correlate with different word senses .	0	used-for	used-for	1
finally , we show that we can provide effective acquisition #techniques# for novel $word senses$ using a combination of online sources .	0	used-for	used-for	1
finally , we show that we can provide effective acquisition $techniques$ for novel word senses using a combination of #online sources# .	0	used-for	used-for	1
the #tipster architecture# has been designed to enable a variety of different $text applications$ to use a set of common text processing modules .	0	used-for	used-for	1
the tipster architecture has been designed to enable a variety of different $text applications$ to use a set of #common text processing modules# .	0	used-for	used-for	1
since #user interfaces# work best when customized for particular $applications$ , it is appropriator that no particular user interface styles or conventions are described in the tipster architecture specification .	0	used-for	used-for	1
however , the computing research laboratory -lrb- crl -rrb- has constructed several $tipster applications$ that use a common set of configurable #graphical user interface -lrb- gui -rrb- functions# .	0	used-for	used-for	1
these $guis$ were constructed using #crl 's tipster user interface toolkit -lrb- tuit -rrb-# .	0	used-for	used-for	1
#tuit# is a $software library$ that can be used to construct multilingual tipster user interfaces for a set of common user tasks .	3	hyponym-of	hyponym-of	1
#tuit# is a software library that can be used to construct $multilingual tipster user interfaces$ for a set of common user tasks .	0	used-for	used-for	1
crl developed #tuit# to support their work to integrate $tipster modules$ for the 6 and 12 month tipster ii demonstrations as well as their oleada and temple demonstration projects .	0	used-for	used-for	1
while such decoding is an essential underpinning , much recent work suggests that natural language interfaces will never appear cooperative or graceful unless $they$ also incorporate numerous #non-literal aspects of communication# , such as robust communication procedures .	4	part-of	part-of	1
while such decoding is an essential underpinning , much recent work suggests that natural language interfaces will never appear cooperative or graceful unless they also incorporate numerous $non-literal aspects of communication$ , such as #robust communication procedures# .	3	hyponym-of	hyponym-of	1
this paper defends that view , but claims that direct imitation of human performance is not the best way to implement many of these non-literal aspects of communication ; that the new technology of powerful $personal computers$ with integral #graphics displays# offers techniques superior to those of humans for these aspects , while still satisfying human communication needs .	4	part-of	part-of	1
this paper proposes a framework in which #lagrangian particle dynamics# is used for the $segmentation of high density crowd flows$ and detection of flow instabilities .	0	used-for	used-for	1
this paper proposes a framework in which #lagrangian particle dynamics# is used for the segmentation of high density crowd flows and $detection of flow instabilities$ .	0	used-for	used-for	1
this paper proposes a framework in which lagrangian particle dynamics is used for the #segmentation of high density crowd flows# and $detection of flow instabilities$ .	1	conjunction	conjunction	1
for this purpose , a $flow field$ generated by a #moving crowd# is treated as an aperiodic dynamical system .	0	used-for	used-for	1
for this purpose , a $flow field$ generated by a moving crowd is treated as an #aperiodic dynamical system# .	0	used-for	used-for	1
a #grid of particles# is overlaid on the $flow field$ , and is advected using a numerical integration scheme .	0	used-for	used-for	1
a $grid of particles$ is overlaid on the flow field , and is advected using a #numerical integration scheme# .	0	used-for	used-for	1
the $evolution of particles$ through the flow is tracked using a #flow map# , whose spatial gradients are subsequently used to setup a cauchy green deformation tensor for quantifying the amount by which the neighboring particles have diverged over the length of the integration .	0	used-for	used-for	1
the evolution of particles through the flow is tracked using a flow map , whose #spatial gradients# are subsequently used to setup a $cauchy green deformation tensor$ for quantifying the amount by which the neighboring particles have diverged over the length of the integration .	0	used-for	used-for	1
the #maximum eigenvalue# of the $tensor$ is used to construct a finite time lyapunov exponent -lrb- ftle -rrb- field , which reveals the lagrangian coherent structures -lrb- lcs -rrb- present in the underlying flow .	5	feature-of	feature-of	1
the #maximum eigenvalue# of the tensor is used to construct a $finite time lyapunov exponent -lrb- ftle -rrb- field$ , which reveals the lagrangian coherent structures -lrb- lcs -rrb- present in the underlying flow .	0	used-for	used-for	1
the maximum eigenvalue of the tensor is used to construct a #finite time lyapunov exponent -lrb- ftle -rrb- field# , which reveals the $lagrangian coherent structures -lrb- lcs -rrb-$ present in the underlying flow .	0	used-for	used-for	1
the #lcs# divide flow into regions of qualitatively different dynamics and are used to locate $boundaries of the flow segments$ in a normalized cuts framework .	0	used-for	used-for	1
the lcs divide flow into regions of qualitatively different dynamics and are used to locate $boundaries of the flow segments$ in a #normalized cuts framework# .	0	used-for	used-for	1
the experiments are conducted on a challenging set of videos taken from #google video# and a $national geographic documentary$ .	1	conjunction	conjunction	1
over the last decade , a variety of smt algorithms have been built and empirically tested whereas little is known about the #computational complexity# of some of the fundamental $problems$ of smt .	2	evaluate-for	evaluate-for	1
over the last decade , a variety of smt algorithms have been built and empirically tested whereas little is known about the computational complexity of some of the fundamental #problems# of $smt$ .	4	part-of	part-of	1
our work aims at providing useful insights into the the #computational complexity# of those $problems$ .	2	evaluate-for	evaluate-for	1
we prove that while #ibm models 1-2# are conceptually and computationally simple , computations involving the higher -lrb- and more useful -rrb- $models$ are hard .	6	compare	compare	1
since it is unlikely that there exists a #polynomial time solution# for any of these $hard problems$ -lrb- unless p = np and p #p = p -rrb- , our results highlight and justify the need for developing polynomial time approximations for these computations .	0	used-for	used-for	1
since it is unlikely that there exists a polynomial time solution for any of these hard problems -lrb- unless p = np and p #p = p -rrb- , our results highlight and justify the need for developing #polynomial time approximations# for these $computations$ .	0	used-for	used-for	1
most state-of-the-art #evaluation measures# for $machine translation$ assign high costs to movements of word blocks .	2	evaluate-for	evaluate-for	1
in this paper , we will present a new #evaluation measure# which explicitly models $block reordering$ as an edit operation .	0	used-for	used-for	1
in this paper , we will present a new evaluation measure which explicitly models $block reordering$ as an #edit operation# .	0	used-for	used-for	1
our $measure$ can be exactly calculated in #quadratic time# .	5	feature-of	feature-of	1
furthermore , we will show how some $evaluation measures$ can be improved by the introduction of #word-dependent substitution costs# .	0	used-for	used-for	1
the correlation of the new #measure# with $human judgment$ has been investigated systematically on two different language pairs .	6	compare	compare	1
the experimental results will show that #it# significantly outperforms state-of-the-art $approaches$ in sentence-level correlation .	6	compare	compare	1
the experimental results will show that $it$ significantly outperforms state-of-the-art approaches in #sentence-level correlation# .	2	evaluate-for	evaluate-for	1
the experimental results will show that it significantly outperforms state-of-the-art $approaches$ in #sentence-level correlation# .	2	evaluate-for	evaluate-for	1
results from experiments with word dependent substitution costs will demonstrate an additional increase of correlation between #automatic evaluation measures# and $human judgment$ .	1	conjunction	conjunction	1
the #rete and treat algorithms# are considered the most efficient $implementation techniques$ for forward chaining rule systems .	3	hyponym-of	hyponym-of	1
the #rete and treat algorithms# are considered the most efficient implementation techniques for $forward chaining rule systems$ .	0	used-for	used-for	1
these #algorithms# support a $language of limited expressive power$ .	0	used-for	used-for	1
in this paper we show how to support $full unification$ in these #algorithms# .	0	used-for	used-for	1
we also show that : supporting full unification is costly ; full unification is not used frequently ; a combination of #compile time# and $run time$ checks can determine when full unification is not needed .	1	conjunction	conjunction	1
we also show that : supporting full unification is costly ; full unification is not used frequently ; a combination of #compile time# and run time checks can determine when $full unification$ is not needed .	2	evaluate-for	evaluate-for	1
we also show that : supporting full unification is costly ; full unification is not used frequently ; a combination of compile time and #run time# checks can determine when $full unification$ is not needed .	2	evaluate-for	evaluate-for	1
a #method# for $error correction$ of ill-formed input is described that acquires dialogue patterns in typical usage and uses these patterns to predict new inputs .	0	used-for	used-for	1
a method for $error correction$ of #ill-formed input# is described that acquires dialogue patterns in typical usage and uses these patterns to predict new inputs .	0	used-for	used-for	1
a #dialogue acquisition and tracking algorithm# is presented along with a description of its implementation in a $voice interactive system$ .	0	used-for	used-for	1
a series of tests are described that show the power of the $error correction methodology$ when #stereotypic dialogue# occurs .	0	used-for	used-for	1
traditional #linear fukunaga-koontz transform -lrb- fkt -rrb-# -lsb- 1 -rsb- is a powerful $discriminative subspaces building approach$ .	3	hyponym-of	hyponym-of	1
previous work has successfully extended #fkt# to be able to deal with $small-sample-size$ .	0	used-for	used-for	1
in this paper , we extend traditional #linear fkt# to enable $it$ to work in multi-class problem and also in higher dimensional -lrb- kernel -rrb- subspaces and therefore provide enhanced discrimination ability .	0	used-for	used-for	1
in this paper , we extend traditional linear fkt to enable #it# to work in $multi-class problem$ and also in higher dimensional -lrb- kernel -rrb- subspaces and therefore provide enhanced discrimination ability .	0	used-for	used-for	1
in this paper , we extend traditional linear fkt to enable #it# to work in multi-class problem and also in $higher dimensional -lrb- kernel -rrb- subspaces$ and therefore provide enhanced discrimination ability .	0	used-for	used-for	1
in this paper , we extend traditional linear fkt to enable #it# to work in multi-class problem and also in higher dimensional -lrb- kernel -rrb- subspaces and therefore provide enhanced $discrimination ability$ .	5	feature-of	feature-of	1
in this paper , we extend traditional linear fkt to enable it to work in #multi-class problem# and also in $higher dimensional -lrb- kernel -rrb- subspaces$ and therefore provide enhanced discrimination ability .	1	conjunction	conjunction	1
we verify the effectiveness of the proposed $kernel fukunaga-koontz transform$ by demonstrating its effectiveness in #face recognition applications# ; however the proposed non-linear generalization can be applied to any other domain specific problems .	2	evaluate-for	evaluate-for	1
we verify the effectiveness of the proposed kernel fukunaga-koontz transform by demonstrating its effectiveness in face recognition applications ; however the proposed #non-linear generalization# can be applied to any other $domain specific problems$ .	0	used-for	used-for	1
while this #task# has much in common with $paraphrases acquisition$ which aims to discover semantic equivalence between verbs , the main challenge of entailment acquisition is to capture asymmetric , or directional , relations .	6	compare	compare	1
while this task has much in common with #paraphrases acquisition# which aims to discover $semantic equivalence$ between verbs , the main challenge of entailment acquisition is to capture asymmetric , or directional , relations .	0	used-for	used-for	1
while this task has much in common with paraphrases acquisition which aims to discover semantic equivalence between verbs , the main challenge of #entailment acquisition# is to capture $asymmetric , or directional , relations$ .	0	used-for	used-for	1
motivated by the intuition that it often underlies the local structure of coherent text , we develop a #method# that discovers $verb entailment$ using evidence about discourse relations between clauses available in a parsed corpus .	0	used-for	used-for	1
motivated by the intuition that it often underlies the local structure of coherent text , we develop a $method$ that discovers verb entailment using evidence about #discourse relations# between clauses available in a parsed corpus .	0	used-for	used-for	1
motivated by the intuition that it often underlies the local structure of coherent text , we develop a method that discovers verb entailment using evidence about $discourse relations$ between clauses available in a #parsed corpus# .	0	used-for	used-for	1
in comparison with earlier work , the proposed #method# covers a much wider range of verb entailment types and learns the $mapping between verbs$ with highly varied argument structures .	0	used-for	used-for	1
in comparison with earlier work , the proposed method covers a much wider range of verb entailment types and learns the $mapping between verbs$ with #highly varied argument structures# .	5	feature-of	feature-of	1
in this paper , we cast the problem of $point cloud matching$ as a #shape matching problem# by transforming each of the given point clouds into a shape representation called the schrödinger distance transform -lrb- sdt -rrb- representation .	0	used-for	used-for	1
in this paper , we cast the problem of point cloud matching as a shape matching problem by transforming each of the given $point clouds$ into a #shape representation# called the schrödinger distance transform -lrb- sdt -rrb- representation .	0	used-for	used-for	1
in this paper , we cast the problem of point cloud matching as a shape matching problem by transforming each of the given point clouds into a $shape representation$ called the #schrödinger distance transform -lrb- sdt -rrb- representation# .	3	hyponym-of	hyponym-of	1
the #sdt representation# is an $analytic expression$ and following the theoretical physics literature , can be normalized to have unit l2 norm-making it a square-root density , which is identified with a point on a unit hilbert sphere , whose intrinsic geometry is fully known .	3	hyponym-of	hyponym-of	1
the sdt representation is an analytic expression and following the theoretical physics literature , can be normalized to have unit l2 norm-making $it$ a #square-root density# , which is identified with a point on a unit hilbert sphere , whose intrinsic geometry is fully known .	0	used-for	used-for	1
the sdt representation is an analytic expression and following the theoretical physics literature , can be normalized to have unit l2 norm-making it a square-root density , which is identified with a point on a $unit hilbert sphere$ , whose #intrinsic geometry# is fully known .	5	feature-of	feature-of	1
the fisher-rao metric , a #natural metric# for the $space of densities$ leads to analytic expressions for the geodesic distance between points on this sphere .	0	used-for	used-for	1
the fisher-rao metric , a natural metric for the space of densities leads to #analytic expressions# for the $geodesic distance$ between points on this sphere .	0	used-for	used-for	1
in this paper , we use the well known #riemannian framework# never before used for $point cloud matching$ , and present a novel matching algorithm .	0	used-for	used-for	1
we pose $point set matching$ under #rigid and non-rigid transformations# in this framework and solve for the transformations using standard nonlinear optimization techniques .	0	used-for	used-for	1
we pose $point set matching$ under rigid and non-rigid transformations in this #framework# and solve for the transformations using standard nonlinear optimization techniques .	0	used-for	used-for	1
we pose point set matching under rigid and non-rigid transformations in this framework and solve for the $transformations$ using standard #nonlinear optimization techniques# .	0	used-for	used-for	1
the experiments show that our #algorithm# outperforms state-of-the-art $point set registration algorithms$ on many quantitative metrics .	6	compare	compare	1
the experiments show that our $algorithm$ outperforms state-of-the-art point set registration algorithms on many #quantitative metrics# .	2	evaluate-for	evaluate-for	1
the experiments show that our algorithm outperforms state-of-the-art $point set registration algorithms$ on many #quantitative metrics# .	2	evaluate-for	evaluate-for	1
using #natural language processing# , we carried out a $trend survey on japanese natural language processing studies$ that have been done over the last ten years .	0	used-for	used-for	1
this paper is useful for both recognizing trends in japanese nlp and constructing a method of supporting $trend surveys$ using #nlp# .	0	used-for	used-for	1
hfos additionally serve as a prototypical example of challenges in the $analysis of discrete events$ in #high-temporal resolution , intracranial eeg data# .	0	used-for	used-for	1
however , previous $hfo analysis$ have assumed a #linear manifold# , global across time , space -lrb- i.e. recording electrode/channel -rrb- , and individual patients .	0	used-for	used-for	1
we also estimate bounds on the bayes classification error to quantify the distinction between two classes of $hfos$ -lrb- #those# occurring during seizures and those occurring due to other processes -rrb- .	3	hyponym-of	hyponym-of	1
we also estimate bounds on the bayes classification error to quantify the distinction between two classes of hfos -lrb- #those# occurring during seizures and $those$ occurring due to other processes -rrb- .	1	conjunction	conjunction	1
we also estimate bounds on the bayes classification error to quantify the distinction between two classes of $hfos$ -lrb- those occurring during seizures and #those# occurring due to other processes -rrb- .	3	hyponym-of	hyponym-of	1
this analysis provides the foundation for future clinical use of hfo features and guides the analysis for other $discrete events$ , such as individual #action potentials# or multi-unit activity .	3	hyponym-of	hyponym-of	1
this analysis provides the foundation for future clinical use of hfo features and guides the analysis for other discrete events , such as individual #action potentials# or $multi-unit activity$ .	1	conjunction	conjunction	1
this analysis provides the foundation for future clinical use of hfo features and guides the analysis for other $discrete events$ , such as individual action potentials or #multi-unit activity# .	3	hyponym-of	hyponym-of	1
in this paper we present ontoscore , a $system$ for scoring sets of concepts on the basis of an #ontology# .	0	used-for	used-for	1
we apply our #system# to the task of scoring alternative $speech recognition hypotheses -lrb- srh -rrb-$ in terms of their semantic coherence .	0	used-for	used-for	1
we propose an efficient #dialogue management# for an $information navigation system$ based on a document knowledge base .	0	used-for	used-for	1
we propose an efficient dialogue management for an $information navigation system$ based on a #document knowledge base# .	0	used-for	used-for	1
it is expected that incorporation of appropriate #n-best candidates of asr# and $contextual information$ will improve the system performance .	1	conjunction	conjunction	1
it is expected that incorporation of appropriate #n-best candidates of asr# and contextual information will improve the $system$ performance .	0	used-for	used-for	1
it is expected that incorporation of appropriate n-best candidates of asr and #contextual information# will improve the $system$ performance .	0	used-for	used-for	1
the #system# also has several choices in $generating responses or confirmations$ .	0	used-for	used-for	1
in this paper , this selection is optimized as $minimization of bayes risk$ based on #reward# for correct information presentation and penalty for redundant turns .	0	used-for	used-for	1
in this paper , this selection is optimized as minimization of bayes risk based on #reward# for $correct information presentation$ and penalty for redundant turns .	0	used-for	used-for	1
in this paper , this selection is optimized as minimization of bayes risk based on #reward# for correct information presentation and $penalty$ for redundant turns .	1	conjunction	conjunction	1
in this paper , this selection is optimized as $minimization of bayes risk$ based on reward for correct information presentation and #penalty# for redundant turns .	0	used-for	used-for	1
in this paper , this selection is optimized as minimization of bayes risk based on reward for correct information presentation and #penalty# for $redundant turns$ .	0	used-for	used-for	1
we have evaluated this $strategy$ with our #spoken dialogue system '' dialogue navigator for kyoto city ''# , which also has question-answering capability .	2	evaluate-for	evaluate-for	1
we have evaluated this strategy with our $spoken dialogue system '' dialogue navigator for kyoto city ''$ , which also has #question-answering capability# .	5	feature-of	feature-of	1
effectiveness of the proposed $framework$ was confirmed in the #success rate of retrieval# and the average number of turns for information access .	2	evaluate-for	evaluate-for	1
effectiveness of the proposed framework was confirmed in the #success rate of retrieval# and the $average number of turns$ for information access .	1	conjunction	conjunction	1
effectiveness of the proposed $framework$ was confirmed in the success rate of retrieval and the #average number of turns# for information access .	2	evaluate-for	evaluate-for	1
effectiveness of the proposed framework was confirmed in the success rate of retrieval and the #average number of turns# for $information access$ .	0	used-for	used-for	1
they are probability , #rank# , and $entropy$ .	1	conjunction	conjunction	1
we evaluated the performance of the three #pruning criteria# in a real application of $chinese text input$ in terms of character error rate -lrb- cer -rrb- .	0	used-for	used-for	1
we evaluated the performance of the three $pruning criteria$ in a real application of chinese text input in terms of #character error rate -lrb- cer -rrb-# .	2	evaluate-for	evaluate-for	1
we also show that the high-performance of $rank$ lies in its strong correlation with #error rate# .	2	evaluate-for	evaluate-for	1
we then present a novel #method# of combining two criteria in $model pruning$ .	0	used-for	used-for	1
this paper proposes an #annotating scheme# that encodes $honorifics$ -lrb- respectful words -rrb- .	0	used-for	used-for	1
this paper proposes an annotating scheme that encodes #honorifics# -lrb- $respectful words$ -rrb- .	3	hyponym-of	hyponym-of	1
#honorifics# are used extensively in $japanese$ , reflecting the social relationship -lrb- e.g. social ranks and age -rrb- of the referents .	0	used-for	used-for	1
this #referential information# is vital for resolving $zero pronouns$ and improving machine translation outputs .	0	used-for	used-for	1
this #referential information# is vital for resolving zero pronouns and improving $machine translation outputs$ .	0	used-for	used-for	1
$visually-guided arm reaching movements$ are produced by #distributed neural networks# within parietal and frontal regions of the cerebral cortex .	0	used-for	used-for	1
experimental data indicate that -lrb- i -rrb- single neurons in these regions are broadly tuned to parameters of movement ; -lrb- 2 -rrb- appropriate commands are elaborated by populations of neurons ; -lrb- 3 -rrb- the $coordinated action of neu-rons$ can be visualized using a #neuronal population vector -lrb- npv -rrb-# .	0	used-for	used-for	1
we designed a #model# of the $cortical motor command$ to investigate the relation between the desired direction of the movement , the actual direction of movement and the direction of the npv in motor cortex .	0	used-for	used-for	1
we designed a model of the cortical motor command to investigate the relation between the desired direction of the movement , the actual direction of movement and the direction of the #npv# in $motor cortex$ .	0	used-for	used-for	1
the model is a #two-layer self-organizing neural network# which combines broadly-tuned -lrb- muscular -rrb- proprioceptive and -lrb- cartesian -rrb- visual information to calculate $-lrb- angular -rrb- motor commands$ for the initial part of the movement of a two-link arm .	0	used-for	used-for	1
the model is a $two-layer self-organizing neural network$ which combines #broadly-tuned -lrb- muscular -rrb- proprioceptive# and -lrb- cartesian -rrb- visual information to calculate -lrb- angular -rrb- motor commands for the initial part of the movement of a two-link arm .	0	used-for	used-for	1
the model is a two-layer self-organizing neural network which combines #broadly-tuned -lrb- muscular -rrb- proprioceptive# and $-lrb- cartesian -rrb- visual information$ to calculate -lrb- angular -rrb- motor commands for the initial part of the movement of a two-link arm .	1	conjunction	conjunction	1
the model is a $two-layer self-organizing neural network$ which combines broadly-tuned -lrb- muscular -rrb- proprioceptive and #-lrb- cartesian -rrb- visual information# to calculate -lrb- angular -rrb- motor commands for the initial part of the movement of a two-link arm .	0	used-for	used-for	1
these results suggest the npv does not give a faithful $image of cortical processing$ during #arm reaching movements# .	5	feature-of	feature-of	1
it is well-known that diversity among #base classifiers# is crucial for constructing a strong $ensemble$ .	0	used-for	used-for	1
in this paper , we propose an alternative way for $ensemble construction$ by #resampling pairwise constraints# that specify whether a pair of instances belongs to the same class or not .	0	used-for	used-for	1
using #pairwise constraints# for $ensemble construction$ is challenging because it remains unknown how to influence the base classifiers with the sampled pairwise constraints .	0	used-for	used-for	1
first , we transform the original instances into a new $data representation$ using #projections# learnt from pairwise constraints .	0	used-for	used-for	1
first , we transform the original instances into a new data representation using $projections$ learnt from #pairwise constraints# .	0	used-for	used-for	1
then , we build the $base clas-sifiers$ with the new #data representation# .	0	used-for	used-for	1
we propose two methods for $resampling pairwise constraints$ following the standard #bagging and boosting algorithms# , respectively .	0	used-for	used-for	1
a new #algorithm# for solving the three $dimensional container packing problem$ is proposed in this paper .	0	used-for	used-for	1
this new #algorithm# deviates from the traditional $approach of wall building and layering$ .	6	compare	compare	1
we tested our $method$ using all 760 test cases from the #or-library# .	2	evaluate-for	evaluate-for	1
experimental results indicate that the new $algorithm$ is able to achieve an #average packing utilization# of more than 87 % .	2	evaluate-for	evaluate-for	1
current #approaches# to $object category recognition$ require datasets of training images to be manually prepared , with varying degrees of supervision .	0	used-for	used-for	1
current $approaches$ to object category recognition require #datasets# of training images to be manually prepared , with varying degrees of supervision .	0	used-for	used-for	1
we present an #approach# that can learn an $object category$ from just its name , by utilizing the raw output of image search engines available on the internet .	0	used-for	used-for	1
we develop a new model , $tsi-plsa$ , which extends #plsa# -lrb- as applied to visual words -rrb- to include spatial information in a translation and scale invariant manner .	0	used-for	used-for	1
we develop a new model , tsi-plsa , which extends #plsa# -lrb- as applied to $visual words$ -rrb- to include spatial information in a translation and scale invariant manner .	0	used-for	used-for	1
we develop a new model , $tsi-plsa$ , which extends plsa -lrb- as applied to visual words -rrb- to include #spatial information# in a translation and scale invariant manner .	4	part-of	part-of	1
our #approach# can handle the high $intra-class variability$ and large proportion of unrelated images returned by search engines .	0	used-for	used-for	1
our #approach# can handle the high intra-class variability and large proportion of $unrelated images$ returned by search engines .	0	used-for	used-for	1
our approach can handle the high #intra-class variability# and large proportion of $unrelated images$ returned by search engines .	1	conjunction	conjunction	1
our approach can handle the high intra-class variability and large proportion of $unrelated images$ returned by #search engines# .	0	used-for	used-for	1
we evaluate the $models$ on standard #test sets# , showing performance competitive with existing methods trained on hand prepared datasets .	2	evaluate-for	evaluate-for	1
we evaluate the models on standard #test sets# , showing performance competitive with existing $methods$ trained on hand prepared datasets .	2	evaluate-for	evaluate-for	1
we evaluate the $models$ on standard test sets , showing performance competitive with existing #methods# trained on hand prepared datasets .	6	compare	compare	1
we evaluate the models on standard test sets , showing performance competitive with existing $methods$ trained on #hand prepared datasets# .	0	used-for	used-for	1
the paper provides an overview of the research conducted at limsi in the field of #speech processing# , but also in the related areas of $human-machine communication$ , including natural language processing , non verbal and multimodal communication .	1	conjunction	conjunction	1
the paper provides an overview of the research conducted at limsi in the field of speech processing , but also in the related areas of $human-machine communication$ , including #natural language processing# , non verbal and multimodal communication .	3	hyponym-of	hyponym-of	1
the paper provides an overview of the research conducted at limsi in the field of speech processing , but also in the related areas of human-machine communication , including #natural language processing# , $non verbal and multimodal communication$ .	1	conjunction	conjunction	1
the paper provides an overview of the research conducted at limsi in the field of speech processing , but also in the related areas of $human-machine communication$ , including natural language processing , #non verbal and multimodal communication# .	3	hyponym-of	hyponym-of	1
we have calculated $analytical expressions$ for how the bias and variance of the estimators provided by various temporal difference value estimation algorithms change with offline updates over trials in absorbing markov chains using #lookup table representations# .	0	used-for	used-for	1
in this paper , we describe the #pronominal anaphora resolution module# of $lucy$ , a portable english understanding system .	4	part-of	part-of	1
in this paper , we describe the pronominal anaphora resolution module of #lucy# , a portable $english understanding system$ .	3	hyponym-of	hyponym-of	1
in this paper , we reported experiments of $unsupervised automatic acquisition of italian and english verb subcategorization frames -lrb- scfs -rrb-$ from #general and domain corpora# .	0	used-for	used-for	1
the proposed $technique$ operates on #syntactically shallow-parsed corpora# on the basis of a limited number of search heuristics not relying on any previous lexico-syntactic knowledge about scfs .	0	used-for	used-for	1
the proposed $technique$ operates on syntactically shallow-parsed corpora on the basis of a limited number of #search heuristics# not relying on any previous lexico-syntactic knowledge about scfs .	0	used-for	used-for	1
the proposed technique operates on syntactically shallow-parsed corpora on the basis of a limited number of search heuristics not relying on any previous $lexico-syntactic knowledge$ about #scfs# .	5	feature-of	feature-of	1
#graph-cuts optimization# is prevalent in $vision and graphics problems$ .	0	used-for	used-for	1
it is thus of great practical importance to parallelize the $graph-cuts optimization$ using to-day 's ubiquitous #multi-core machines# .	0	used-for	used-for	1
however , the current best $serial algorithm$ by boykov and kolmogorov -lsb- 4 -rsb- -lrb- called the #bk algorithm# -rrb- still has the superior empirical performance .	3	hyponym-of	hyponym-of	1
in this paper , we propose a novel #adaptive bottom-up approach# to parallelize the $bk algorithm$ .	0	used-for	used-for	1
extensive experiments in common #applications# such as 2d/3d image segmentations and 3d surface fitting demonstrate the effectiveness of our $approach$ .	2	evaluate-for	evaluate-for	1
extensive experiments in common $applications$ such as #2d/3d image segmentations# and 3d surface fitting demonstrate the effectiveness of our approach .	3	hyponym-of	hyponym-of	1
extensive experiments in common applications such as #2d/3d image segmentations# and $3d surface fitting$ demonstrate the effectiveness of our approach .	1	conjunction	conjunction	1
extensive experiments in common $applications$ such as 2d/3d image segmentations and #3d surface fitting# demonstrate the effectiveness of our approach .	3	hyponym-of	hyponym-of	1
we study the question of how to make loss-aware predictions in image segmentation settings where the $evaluation function$ is the #intersection-over-union -lrb- iou -rrb- measure# that is used widely in evaluating image segmentation systems .	3	hyponym-of	hyponym-of	1
we study the question of how to make loss-aware predictions in image segmentation settings where the evaluation function is the #intersection-over-union -lrb- iou -rrb- measure# that is used widely in evaluating $image segmentation systems$ .	2	evaluate-for	evaluate-for	1
currently , there are two $dominant approaches$ : the #first# approximates the expected-iou -lrb- eiou -rrb- score as expected-intersection-over-expected-union -lrb- eioeu -rrb- ; and the second approach is to compute exact eiou but only over a small set of high-quality candidate solutions .	3	hyponym-of	hyponym-of	1
currently , there are two $dominant approaches$ : the first approximates the expected-iou -lrb- eiou -rrb- score as expected-intersection-over-expected-union -lrb- eioeu -rrb- ; and the #second approach# is to compute exact eiou but only over a small set of high-quality candidate solutions .	3	hyponym-of	hyponym-of	1
our new $methods$ use the #eioeu approximation# paired with high quality candidate solutions .	0	used-for	used-for	1
experimentally we show that our new $approaches$ lead to improved performance on both #image segmentation tasks# .	2	evaluate-for	evaluate-for	1
later , however , breiman cast serious doubt on this explanation by introducing a $boosting algorithm$ , #arc-gv# , that can generate a higher margins distribution than adaboost and yet performs worse .	3	hyponym-of	hyponym-of	1
later , however , breiman cast serious doubt on this explanation by introducing a boosting algorithm , #arc-gv# , that can generate a higher $margins distribution$ than adaboost and yet performs worse .	0	used-for	used-for	1
later , however , breiman cast serious doubt on this explanation by introducing a boosting algorithm , #arc-gv# , that can generate a higher margins distribution than $adaboost$ and yet performs worse .	6	compare	compare	1
although we can reproduce his main finding , we find that the poorer performance of arc-gv can be explained by the increased #complexity# of the $base classifiers$ it uses , an explanation supported by our experiments and entirely consistent with the margins theory .	2	evaluate-for	evaluate-for	1
although we can reproduce his main finding , we find that the poorer performance of $arc-gv$ can be explained by the increased complexity of the #base classifiers# it uses , an explanation supported by our experiments and entirely consistent with the margins theory .	3	hyponym-of	hyponym-of	1
the #transfer phase# in $machine translation -lrb- mt -rrb- systems$ has been considered to be more complicated than analysis and generation , since it is inherently a conglomeration of individual lexical rules .	4	part-of	part-of	1
the #transfer phase# in machine translation -lrb- mt -rrb- systems has been considered to be more complicated than $analysis$ and generation , since it is inherently a conglomeration of individual lexical rules .	6	compare	compare	1
the #transfer phase# in machine translation -lrb- mt -rrb- systems has been considered to be more complicated than analysis and $generation$ , since it is inherently a conglomeration of individual lexical rules .	6	compare	compare	1
the transfer phase in machine translation -lrb- mt -rrb- systems has been considered to be more complicated than #analysis# and $generation$ , since it is inherently a conglomeration of individual lexical rules .	1	conjunction	conjunction	1
currently some attempts are being made to use #case-based reasoning# in $machine translation$ , that is , to make decisions on the basis of translation examples at appropriate pints in mt .	0	used-for	used-for	1
this paper proposes a new type of $transfer system$ , called a #similarity-driven transfer system -lrb- simtran -rrb-# , for use in such case-based mt -lrb- cbmt -rrb- .	3	hyponym-of	hyponym-of	1
this paper proposes a new type of transfer system , called a #similarity-driven transfer system -lrb- simtran -rrb-# , for use in such $case-based mt -lrb- cbmt -rrb-$ .	0	used-for	used-for	1
this paper addresses the problem of #optimal alignment of non-rigid surfaces# from multi-view video observations to obtain a $temporally consistent representation$ .	0	used-for	used-for	1
this paper addresses the problem of $optimal alignment of non-rigid surfaces$ from #multi-view video observations# to obtain a temporally consistent representation .	0	used-for	used-for	1
conventional $non-rigid surface tracking$ performs #frame-to-frame alignment# which is subject to the accumulation of errors resulting in a drift over time .	0	used-for	used-for	1
recently , $non-sequential tracking approaches$ have been introduced which reorder the input data based on a #dissimilarity measure# .	0	used-for	used-for	1
they demonstrate a reduced drift and increased #robustness# to large $non-rigid deformations$ .	5	feature-of	feature-of	1
#optimisation of the tree# for $non-sequential tracking$ , which minimises the errors in temporal consistency due to both the drift and the jumps , is proposed .	0	used-for	used-for	1
$optimisation of the tree$ for non-sequential tracking , which minimises the errors in #temporal consistency# due to both the drift and the jumps , is proposed .	2	evaluate-for	evaluate-for	1
a novel #cluster tree# enforces $sequential tracking in local segments$ of the sequence while allowing global non-sequential traversal among these segments .	0	used-for	used-for	1
a novel #cluster tree# enforces sequential tracking in local segments of the sequence while allowing $global non-sequential traversal$ among these segments .	0	used-for	used-for	1
comprehensive evaluation is performed on a variety of challenging $non-rigid surfaces$ including #face# , cloth and people .	3	hyponym-of	hyponym-of	1
comprehensive evaluation is performed on a variety of challenging non-rigid surfaces including #face# , $cloth$ and people .	1	conjunction	conjunction	1
comprehensive evaluation is performed on a variety of challenging $non-rigid surfaces$ including face , #cloth# and people .	3	hyponym-of	hyponym-of	1
comprehensive evaluation is performed on a variety of challenging non-rigid surfaces including face , #cloth# and $people$ .	1	conjunction	conjunction	1
comprehensive evaluation is performed on a variety of challenging $non-rigid surfaces$ including face , cloth and #people# .	3	hyponym-of	hyponym-of	1
it demonstrates that the proposed #cluster tree# achieves better temporal consistency than the previous $sequential and non-sequential tracking approaches$ .	6	compare	compare	1
it demonstrates that the proposed $cluster tree$ achieves better #temporal consistency# than the previous sequential and non-sequential tracking approaches .	2	evaluate-for	evaluate-for	1
quantitative analysis on a created #synthetic facial performance# also shows an improvement by the $cluster tree$ .	2	evaluate-for	evaluate-for	1
the $translation of english text into american sign language -lrb- asl -rrb- animation$ tests the limits of traditional #mt architectural designs# .	0	used-for	used-for	1
a new #semantic representation# is proposed that uses virtual reality 3d scene modeling software to produce $spatially complex asl phenomena$ called '' classifier predicates . ''	0	used-for	used-for	1
a new $semantic representation$ is proposed that uses #virtual reality 3d scene modeling software# to produce spatially complex asl phenomena called '' classifier predicates . ''	0	used-for	used-for	1
a new semantic representation is proposed that uses virtual reality 3d scene modeling software to produce $spatially complex asl phenomena$ called '' #classifier predicates# . ''	3	hyponym-of	hyponym-of	1
the model acts as an interlingua within a new multi-pathway mt architecture design that also incorporates #transfer# and $direct approaches$ into a single system .	1	conjunction	conjunction	1
the model acts as an interlingua within a new multi-pathway mt architecture design that also incorporates #transfer# and direct approaches into a single $system$ .	4	part-of	part-of	1
the model acts as an interlingua within a new multi-pathway mt architecture design that also incorporates transfer and #direct approaches# into a single $system$ .	4	part-of	part-of	1
an $extension$ to the #gpsg grammatical formalism# is proposed , allowing non-terminals to consist of finite sequences of category labels , and allowing schematic variables to range over such sequences .	0	used-for	used-for	1
the #extension# is shown to be sufficient to provide a strongly adequate $grammar$ for crossed serial dependencies , as found in e.g. dutch subordinate clauses .	0	used-for	used-for	1
the extension is shown to be sufficient to provide a strongly adequate #grammar# for $crossed serial dependencies$ , as found in e.g. dutch subordinate clauses .	0	used-for	used-for	1
the $extension$ is shown to be parseable by a simple #extension# to an existing parsing method for gpsg .	0	used-for	used-for	1
the extension is shown to be parseable by a simple $extension$ to an existing #parsing method# for gpsg .	0	used-for	used-for	1
the extension is shown to be parseable by a simple extension to an existing #parsing method# for $gpsg$ .	0	used-for	used-for	1
this paper presents an #approach# to $localizing functional objects$ in surveillance videos without domain knowledge about semantic object classes that may appear in the scene .	0	used-for	used-for	1
this paper presents an approach to $localizing functional objects$ in #surveillance videos# without domain knowledge about semantic object classes that may appear in the scene .	0	used-for	used-for	1
this paper presents an approach to localizing functional objects in surveillance videos without $domain knowledge$ about #semantic object classes# that may appear in the scene .	5	feature-of	feature-of	1
a #bayesian framework# is used to probabilistically model : $people 's trajectories and intents$ , constraint map of the scene , and locations of functional objects .	0	used-for	used-for	1
a #bayesian framework# is used to probabilistically model : people 's trajectories and intents , $constraint map of the scene$ , and locations of functional objects .	0	used-for	used-for	1
a #bayesian framework# is used to probabilistically model : people 's trajectories and intents , constraint map of the scene , and $locations of functional objects$ .	0	used-for	used-for	1
a bayesian framework is used to probabilistically model : #people 's trajectories and intents# , $constraint map of the scene$ , and locations of functional objects .	1	conjunction	conjunction	1
a bayesian framework is used to probabilistically model : people 's trajectories and intents , #constraint map of the scene# , and $locations of functional objects$ .	1	conjunction	conjunction	1
a #data-driven markov chain monte carlo -lrb- mcmc -rrb- process# is used for $inference$ .	0	used-for	used-for	1
our evaluation on #videos of public squares and courtyards# demonstrates our effectiveness in $localizing functional objects$ and predicting people 's trajectories in unobserved parts of the video footage .	2	evaluate-for	evaluate-for	1
our evaluation on #videos of public squares and courtyards# demonstrates our effectiveness in localizing functional objects and $predicting people 's trajectories$ in unobserved parts of the video footage .	2	evaluate-for	evaluate-for	1
our evaluation on videos of public squares and courtyards demonstrates our effectiveness in #localizing functional objects# and $predicting people 's trajectories$ in unobserved parts of the video footage .	1	conjunction	conjunction	1
we propose a #process model# for $hierarchical perceptual sound organization$ , which recognizes perceptual sounds included in incoming sound signals .	0	used-for	used-for	1
we propose a process model for hierarchical perceptual sound organization , which recognizes #perceptual sounds# included in $incoming sound signals$ .	4	part-of	part-of	1
we consider $perceptual sound organization$ as a #scene analysis problem# in the auditory domain .	0	used-for	used-for	1
we consider perceptual sound organization as a $scene analysis problem$ in the #auditory domain# .	5	feature-of	feature-of	1
our $model$ consists of multiple #processing modules# and a hypothesis network for quantitative integration of multiple sources of information .	4	part-of	part-of	1
our model consists of multiple #processing modules# and a $hypothesis network$ for quantitative integration of multiple sources of information .	1	conjunction	conjunction	1
our $model$ consists of multiple processing modules and a #hypothesis network# for quantitative integration of multiple sources of information .	4	part-of	part-of	1
on the $hypothesis network$ , individual information is integrated and an optimal #internal model# of perceptual sounds is automatically constructed .	4	part-of	part-of	1
on the hypothesis network , individual information is integrated and an optimal #internal model# of $perceptual sounds$ is automatically constructed .	0	used-for	used-for	1
based on the model , a #music scene analysis system# has been developed for $acoustic signals of ensemble music$ , which recognizes rhythm , chords , and source-separated musical notes .	0	used-for	used-for	1
based on the model , a #music scene analysis system# has been developed for acoustic signals of ensemble music , which recognizes $rhythm$ , chords , and source-separated musical notes .	0	used-for	used-for	1
based on the model , a #music scene analysis system# has been developed for acoustic signals of ensemble music , which recognizes rhythm , $chords$ , and source-separated musical notes .	0	used-for	used-for	1
based on the model , a #music scene analysis system# has been developed for acoustic signals of ensemble music , which recognizes rhythm , chords , and $source-separated musical notes$ .	0	used-for	used-for	1
based on the model , a music scene analysis system has been developed for acoustic signals of ensemble music , which recognizes #rhythm# , $chords$ , and source-separated musical notes .	1	conjunction	conjunction	1
based on the model , a music scene analysis system has been developed for acoustic signals of ensemble music , which recognizes rhythm , #chords# , and $source-separated musical notes$ .	1	conjunction	conjunction	1
experimental results show that our $method$ has permitted autonomous , stable and effective #information integration# to construct the internal model of hierarchical perceptual sounds .	5	feature-of	feature-of	1
experimental results show that our method has permitted autonomous , stable and effective #information integration# to construct the $internal model$ of hierarchical perceptual sounds .	0	used-for	used-for	1
experimental results show that our method has permitted autonomous , stable and effective information integration to construct the #internal model# of $hierarchical perceptual sounds$ .	0	used-for	used-for	1
we directly investigate a subject of much recent debate : do #word sense disambigation models# help $statistical machine translation quality$ ?	0	used-for	used-for	1
using a state-of-the-art #chinese word sense disambiguation model# to choose $translation candidates$ for a typical ibm statistical mt system , we find that word sense disambiguation does not yield significantly better translation quality than the statistical machine translation system alone .	0	used-for	used-for	1
using a state-of-the-art chinese word sense disambiguation model to choose #translation candidates# for a typical $ibm statistical mt system$ , we find that word sense disambiguation does not yield significantly better translation quality than the statistical machine translation system alone .	0	used-for	used-for	1
using a state-of-the-art chinese word sense disambiguation model to choose translation candidates for a typical ibm statistical mt system , we find that #word sense disambiguation# does not yield significantly better translation quality than the $statistical machine translation system$ alone .	6	compare	compare	1
using a state-of-the-art chinese word sense disambiguation model to choose translation candidates for a typical ibm statistical mt system , we find that $word sense disambiguation$ does not yield significantly better #translation quality# than the statistical machine translation system alone .	2	evaluate-for	evaluate-for	1
using a state-of-the-art chinese word sense disambiguation model to choose translation candidates for a typical ibm statistical mt system , we find that word sense disambiguation does not yield significantly better #translation quality# than the $statistical machine translation system$ alone .	2	evaluate-for	evaluate-for	1
#image sequence processing techniques# are used to study $exchange , growth , and transport processes$ and to tackle key questions in environmental physics and biology .	0	used-for	used-for	1
image sequence processing techniques are used to study exchange , growth , and transport processes and to tackle key questions in #environmental physics# and $biology$ .	1	conjunction	conjunction	1
these applications require high #accuracy# for the $estimation of the motion field$ since the most interesting parameters of the dynamical processes studied are contained in first-order derivatives of the motion field or in dynamical changes of the moving objects .	2	evaluate-for	evaluate-for	1
these $applications$ require high accuracy for the #estimation of the motion field# since the most interesting parameters of the dynamical processes studied are contained in first-order derivatives of the motion field or in dynamical changes of the moving objects .	0	used-for	used-for	1
these applications require high accuracy for the estimation of the motion field since the most interesting parameters of the dynamical processes studied are contained in #first-order derivatives of the motion field# or in $dynamical changes of the moving objects$ .	1	conjunction	conjunction	1
a $tensor method$ tuned with carefully optimized #derivative filters# yields reliable and dense displacement vector fields -lrb- dvf -rrb- with an accuracy of up to a few hundredth pixels/frame for real-world images .	0	used-for	used-for	1
a tensor method tuned with carefully optimized derivative filters yields reliable and dense $displacement vector fields -lrb- dvf -rrb-$ with an accuracy of up to a few hundredth #pixels/frame# for real-world images .	2	evaluate-for	evaluate-for	1
a tensor method tuned with carefully optimized derivative filters yields reliable and dense displacement vector fields -lrb- dvf -rrb- with an accuracy of up to a few hundredth $pixels/frame$ for #real-world images# .	0	used-for	used-for	1
the #accuracy# of the $tensor method$ is verified with computer-generated sequences and a calibrated image sequence .	2	evaluate-for	evaluate-for	1
the accuracy of the $tensor method$ is verified with #computer-generated sequences# and a calibrated image sequence .	2	evaluate-for	evaluate-for	1
the accuracy of the tensor method is verified with #computer-generated sequences# and a $calibrated image sequence$ .	1	conjunction	conjunction	1
the accuracy of the $tensor method$ is verified with computer-generated sequences and a #calibrated image sequence# .	2	evaluate-for	evaluate-for	1
with the improvements in #accuracy# the $motion estimation$ is now rather limited by imperfections in the ccd sensors , especially the spatial nonuni-formity in the responsivity .	2	evaluate-for	evaluate-for	1
with the improvements in accuracy the $motion estimation$ is now rather limited by imperfections in the #ccd sensors# , especially the spatial nonuni-formity in the responsivity .	0	used-for	used-for	1
with the improvements in accuracy the motion estimation is now rather limited by imperfections in the ccd sensors , especially the #spatial nonuni-formity# in the $responsivity$ .	5	feature-of	feature-of	1
with the improvements in accuracy the motion estimation is now rather limited by imperfections in the $ccd sensors$ , especially the spatial nonuni-formity in the #responsivity# .	5	feature-of	feature-of	1
the application of the #techniques# to the $analysis of plant growth$ , to ocean surface microturbulence in ir image sequences , and to sediment transport is demonstrated .	0	used-for	used-for	1
the application of the #techniques# to the analysis of plant growth , to $ocean surface microturbulence in ir image sequences$ , and to sediment transport is demonstrated .	0	used-for	used-for	1
the application of the #techniques# to the analysis of plant growth , to ocean surface microturbulence in ir image sequences , and to $sediment transport$ is demonstrated .	0	used-for	used-for	1
the application of the techniques to the #analysis of plant growth# , to $ocean surface microturbulence in ir image sequences$ , and to sediment transport is demonstrated .	1	conjunction	conjunction	1
the application of the techniques to the analysis of plant growth , to #ocean surface microturbulence in ir image sequences# , and to $sediment transport$ is demonstrated .	1	conjunction	conjunction	1
we present a #czech-english statistical machine translation system# which performs $tree-to-tree translation of dependency structures$ .	0	used-for	used-for	1
the only $bilingual resource$ required is a #sentence-aligned parallel corpus# .	0	used-for	used-for	1
we also refer to an evaluation method and plan to compare our #system# 's output with a $benchmark system$ .	6	compare	compare	1
this paper describes the understanding process of the $spatial descriptions$ in #japanese# .	5	feature-of	feature-of	1
to reconstruct the model , the authors extract the qualitative spatial constraints from the text , and represent them as the $numerical constraints$ on the #spatial attributes of the entities# .	0	used-for	used-for	1
such #context information# is therefore important to characterize the $intrinsic representation of a video frame$ .	0	used-for	used-for	1
in this paper , we present a novel #approach# to learn the $deep video representation$ by exploring both local and holistic contexts .	0	used-for	used-for	1
in this paper , we present a novel $approach$ to learn the deep video representation by exploring both #local and holistic contexts# .	0	used-for	used-for	1
specifically , we propose a #triplet sampling mechanism# to encode the $local temporal relationship of adjacent frames$ based on their deep representations .	0	used-for	used-for	1
specifically , we propose a $triplet sampling mechanism$ to encode the local temporal relationship of adjacent frames based on their #deep representations# .	0	used-for	used-for	1
in addition , we incorporate the #graph structure of the video# , as a $priori$ , to holistically preserve the inherent correlations among video frames .	0	used-for	used-for	1
our $approach$ is fully unsupervised and trained in an #end-to-end deep convolutional neu-ral network architecture# .	0	used-for	used-for	1
by extensive experiments , we show that our #learned representation# can significantly boost several video recognition tasks -lrb- retrieval , classification , and highlight detection -rrb- over traditional $video representations$ .	6	compare	compare	1
by extensive experiments , we show that our $learned representation$ can significantly boost several #video recognition tasks# -lrb- retrieval , classification , and highlight detection -rrb- over traditional video representations .	2	evaluate-for	evaluate-for	1
by extensive experiments , we show that our learned representation can significantly boost several #video recognition tasks# -lrb- retrieval , classification , and highlight detection -rrb- over traditional $video representations$ .	2	evaluate-for	evaluate-for	1
by extensive experiments , we show that our learned representation can significantly boost several $video recognition tasks$ -lrb- #retrieval# , classification , and highlight detection -rrb- over traditional video representations .	3	hyponym-of	hyponym-of	1
by extensive experiments , we show that our learned representation can significantly boost several video recognition tasks -lrb- #retrieval# , $classification$ , and highlight detection -rrb- over traditional video representations .	1	conjunction	conjunction	1
by extensive experiments , we show that our learned representation can significantly boost several $video recognition tasks$ -lrb- retrieval , #classification# , and highlight detection -rrb- over traditional video representations .	3	hyponym-of	hyponym-of	1
by extensive experiments , we show that our learned representation can significantly boost several video recognition tasks -lrb- retrieval , #classification# , and $highlight detection$ -rrb- over traditional video representations .	1	conjunction	conjunction	1
by extensive experiments , we show that our learned representation can significantly boost several $video recognition tasks$ -lrb- retrieval , classification , and #highlight detection# -rrb- over traditional video representations .	3	hyponym-of	hyponym-of	1
for $mobile speech application$ , #speaker doa estimation accuracy# , interference robustness and compact physical size are three key factors .	5	feature-of	feature-of	1
for mobile speech application , #speaker doa estimation accuracy# , $interference robustness$ and compact physical size are three key factors .	1	conjunction	conjunction	1
for $mobile speech application$ , speaker doa estimation accuracy , #interference robustness# and compact physical size are three key factors .	5	feature-of	feature-of	1
for mobile speech application , speaker doa estimation accuracy , #interference robustness# and $compact physical size$ are three key factors .	1	conjunction	conjunction	1
for $mobile speech application$ , speaker doa estimation accuracy , interference robustness and #compact physical size# are three key factors .	5	feature-of	feature-of	1
#it# is achieved by deriving the inter-sensor data ratio model of an avs in bispectrum domain -lrb- bisdr -rrb- and exploring the $favorable properties$ of bispectrum , such as zero value of gaussian process and different distribution of speech and nsi .	0	used-for	used-for	1
it is achieved by deriving the #inter-sensor data ratio model# of an $avs$ in bispectrum domain -lrb- bisdr -rrb- and exploring the favorable properties of bispectrum , such as zero value of gaussian process and different distribution of speech and nsi .	0	used-for	used-for	1
it is achieved by deriving the inter-sensor data ratio model of an $avs$ in #bispectrum domain -lrb- bisdr -rrb-# and exploring the favorable properties of bispectrum , such as zero value of gaussian process and different distribution of speech and nsi .	0	used-for	used-for	1
it is achieved by deriving the inter-sensor data ratio model of an avs in bispectrum domain -lrb- bisdr -rrb- and exploring the $favorable properties$ of bispectrum , such as #zero value of gaussian process# and different distribution of speech and nsi .	3	hyponym-of	hyponym-of	1
it is achieved by deriving the inter-sensor data ratio model of an avs in bispectrum domain -lrb- bisdr -rrb- and exploring the favorable properties of bispectrum , such as #zero value of gaussian process# and different $distribution of speech and nsi$ .	1	conjunction	conjunction	1
it is achieved by deriving the inter-sensor data ratio model of an avs in bispectrum domain -lrb- bisdr -rrb- and exploring the $favorable properties$ of bispectrum , such as zero value of gaussian process and different #distribution of speech and nsi# .	3	hyponym-of	hyponym-of	1
specifically , a reliable #bispectrum mask# is generated to guarantee that the $speaker doa cues$ , derived from bisdr , are robust to nsi in terms of speech sparsity and large bispectrum amplitude of the captured signals .	0	used-for	used-for	1
specifically , a reliable bispectrum mask is generated to guarantee that the $speaker doa cues$ , derived from #bisdr# , are robust to nsi in terms of speech sparsity and large bispectrum amplitude of the captured signals .	0	used-for	used-for	1
intensive experiments demonstrate an improved performance of our proposed #algorithm# under various $nsi conditions$ even when sir is smaller than 0db .	0	used-for	used-for	1
in this paper , we want to show how the #morphological component# of an existing $nlp-system for dutch -lrb- dutch medical language processor - dmlp -rrb-$ has been extended in order to produce output that is compatible with the language independent modules of the lsp-mlp system -lrb- linguistic string project - medical language processor -rrb- of the new york university .	4	part-of	part-of	1
in this paper , we want to show how the morphological component of an existing nlp-system for dutch -lrb- dutch medical language processor - dmlp -rrb- has been extended in order to produce output that is compatible with the #language independent modules# of the $lsp-mlp system -lrb- linguistic string project - medical language processor -rrb-$ of the new york university .	4	part-of	part-of	1
the $former$ can take advantage of the language independent developments of the #latter# , while focusing on idiosyncrasies for dutch .	0	used-for	used-for	1
the former can take advantage of the language independent developments of the latter , while focusing on $idiosyncrasies$ for #dutch# .	0	used-for	used-for	1
this general strategy will be illustrated by a practical application , namely the highlighting of #relevant information# in a $patient discharge summary -lrb- pds -rrb-$ by means of modern hypertext mark-up language -lrb- html -rrb- technology .	4	part-of	part-of	1
this general strategy will be illustrated by a practical application , namely the $highlighting of relevant information$ in a patient discharge summary -lrb- pds -rrb- by means of modern #hypertext mark-up language -lrb- html -rrb- technology# .	0	used-for	used-for	1
such an #application# can be of use for $medical administrative purposes$ in a hospital environment .	0	used-for	used-for	1
$criterionsm online essay evaluation service$ includes a capability that labels sentences in student writing with #essay-based discourse elements# -lrb- e.g. , thesis statements -rrb- .	4	part-of	part-of	1
criterionsm online essay evaluation service includes a capability that labels sentences in student writing with $essay-based discourse elements$ -lrb- e.g. , #thesis statements# -rrb- .	3	hyponym-of	hyponym-of	1
we describe a new #system# that enhances $criterion 's capability$ , by evaluating multiple aspects of coherence in essays .	0	used-for	used-for	1
we describe a new $system$ that enhances criterion 's capability , by evaluating multiple aspects of #coherence in essays# .	2	evaluate-for	evaluate-for	1
this #system# identifies $features$ of sentences based on semantic similarity measures and discourse structure .	0	used-for	used-for	1
this system identifies $features$ of sentences based on #semantic similarity measures# and discourse structure .	0	used-for	used-for	1
this system identifies $features$ of sentences based on semantic similarity measures and #discourse structure# .	0	used-for	used-for	1
this system identifies features of sentences based on $semantic similarity measures$ and #discourse structure# .	1	conjunction	conjunction	1
a $support vector machine$ uses these #features# to capture breakdowns in coherence due to relatedness to the essay question and relatedness between discourse elements .	0	used-for	used-for	1
a support vector machine uses these #features# to capture $breakdowns in coherence$ due to relatedness to the essay question and relatedness between discourse elements .	0	used-for	used-for	1
$intra-sentential quality$ is evaluated with #rule-based heuristics# .	2	evaluate-for	evaluate-for	1
results indicate that the #system# yields higher performance than a $baseline$ on all three aspects .	6	compare	compare	1
this paper presents an #algorithm# for $labeling curvilinear structure$ at multiple scales in line drawings and edge images symbolic curve-element tokens residing in a spatially-indexed and scale-indexed data structure denote circular arcs fit to image data .	0	used-for	used-for	1
this paper presents an algorithm for $labeling curvilinear structure$ at multiple scales in #line drawings# and edge images symbolic curve-element tokens residing in a spatially-indexed and scale-indexed data structure denote circular arcs fit to image data .	5	feature-of	feature-of	1
this paper presents an algorithm for labeling curvilinear structure at multiple scales in #line drawings# and $edge images$ symbolic curve-element tokens residing in a spatially-indexed and scale-indexed data structure denote circular arcs fit to image data .	1	conjunction	conjunction	1
this paper presents an algorithm for $labeling curvilinear structure$ at multiple scales in line drawings and #edge images# symbolic curve-element tokens residing in a spatially-indexed and scale-indexed data structure denote circular arcs fit to image data .	5	feature-of	feature-of	1
this paper presents an algorithm for labeling curvilinear structure at multiple scales in line drawings and edge images symbolic #curve-element tokens# residing in a $spatially-indexed and scale-indexed data structure$ denote circular arcs fit to image data .	4	part-of	part-of	1
#recognition of proper nouns# in japanese text has been studied as a part of the more general problem of $morphological analysis$ in japanese text processing -lrb- -lsb- 1 -rsb- -lsb- 2 -rsb- -rrb- .	4	part-of	part-of	1
recognition of #proper nouns# in $japanese text$ has been studied as a part of the more general problem of morphological analysis in japanese text processing -lrb- -lsb- 1 -rsb- -lsb- 2 -rsb- -rrb- .	4	part-of	part-of	1
recognition of proper nouns in japanese text has been studied as a part of the more general problem of #morphological analysis# in $japanese text processing$ -lrb- -lsb- 1 -rsb- -lsb- 2 -rsb- -rrb- .	0	used-for	used-for	1
$it$ has also been studied in the framework of #japanese information extraction# -lrb- -lsb- 3 -rsb- -rrb- in recent years .	0	used-for	used-for	1
our #approach# to the $multi-lingual evaluation task -lrb- met -rrb-$ for japanese text is to consider the given task as a morphological analysis problem in japanese .	0	used-for	used-for	1
our approach to the #multi-lingual evaluation task -lrb- met -rrb-# for $japanese text$ is to consider the given task as a morphological analysis problem in japanese .	0	used-for	used-for	1
our approach to the multi-lingual evaluation task -lrb- met -rrb- for japanese text is to consider the given $task$ as a #morphological analysis problem# in japanese .	0	used-for	used-for	1
our approach to the multi-lingual evaluation task -lrb- met -rrb- for japanese text is to consider the given task as a $morphological analysis problem$ in #japanese# .	0	used-for	used-for	1
our #morphological analyzer# has done all the necessary work for the $recognition and classification of proper names , numerical and temporal expressions$ , i.e. named entity -lrb- ne -rrb- items in the japanese text .	0	used-for	used-for	1
our morphological analyzer has done all the necessary work for the recognition and classification of $proper names , numerical and temporal expressions$ , i.e. #named entity -lrb- ne -rrb- items# in the japanese text .	3	hyponym-of	hyponym-of	1
our morphological analyzer has done all the necessary work for the recognition and classification of proper names , numerical and temporal expressions , i.e. #named entity -lrb- ne -rrb- items# in the $japanese text$ .	4	part-of	part-of	1
#amorph# recognizes $ne items$ in two stages : dictionary lookup and rule application .	0	used-for	used-for	1
$amorph$ recognizes ne items in two stages : #dictionary lookup# and rule application .	4	part-of	part-of	1
amorph recognizes ne items in two stages : #dictionary lookup# and $rule application$ .	1	conjunction	conjunction	1
$amorph$ recognizes ne items in two stages : dictionary lookup and #rule application# .	4	part-of	part-of	1
first , $it$ uses several kinds of #dictionaries# to segment and tag japanese character strings .	0	used-for	used-for	1
first , it uses several kinds of #dictionaries# to segment and tag $japanese character strings$ .	0	used-for	used-for	1
second , based on the information resulting from the #dictionary lookup stage# , a set of $rules$ is applied to the segmented strings in order to identify ne items .	0	used-for	used-for	1
second , based on the information resulting from the dictionary lookup stage , a set of #rules# is applied to the segmented strings in order to identify $ne items$ .	0	used-for	used-for	1
we propose to incorporate a #priori geometric constraints# in a $3 -- d stereo reconstruction scheme$ to cope with the many cases where image information alone is not sufficient to accurately recover 3 -- d shape .	4	part-of	part-of	1
we propose to incorporate a priori geometric constraints in a 3 -- d stereo reconstruction scheme to cope with the many cases where #image information# alone is not sufficient to accurately recover $3 -- d shape$ .	0	used-for	used-for	1
our $approach$ is based on the #iterative deformation of a 3 -- d surface mesh# to minimize an objective function .	0	used-for	used-for	1
our approach is based on the #iterative deformation of a 3 -- d surface mesh# to minimize an $objective function$ .	0	used-for	used-for	1
we show that combining #anisotropic meshing# with a $non-quadratic approach$ to regularization enables us to obtain satisfactory reconstruction results using triangulations with few vertices .	1	conjunction	conjunction	1
we show that combining #anisotropic meshing# with a non-quadratic approach to regularization enables us to obtain satisfactory $reconstruction$ results using triangulations with few vertices .	0	used-for	used-for	1
we show that combining anisotropic meshing with a #non-quadratic approach# to $regularization$ enables us to obtain satisfactory reconstruction results using triangulations with few vertices .	0	used-for	used-for	1
we show that combining anisotropic meshing with a #non-quadratic approach# to regularization enables us to obtain satisfactory $reconstruction$ results using triangulations with few vertices .	0	used-for	used-for	1
we show that combining anisotropic meshing with a non-quadratic approach to regularization enables us to obtain satisfactory $reconstruction$ results using #triangulations# with few vertices .	0	used-for	used-for	1
#structural or numerical constraints# can then be added locally to the $reconstruction process$ through a constrained optimization scheme .	0	used-for	used-for	1
$structural or numerical constraints$ can then be added locally to the reconstruction process through a #constrained optimization scheme# .	0	used-for	used-for	1
#they# improve the $reconstruction$ results and enforce their consistency with a priori knowledge about object shape .	0	used-for	used-for	1
they improve the reconstruction results and enforce their consistency with a $priori knowledge$ about #object shape# .	5	feature-of	feature-of	1
the strong description and modeling properties of differential features make #them# useful tools that can be efficiently used as constraints for $3 -- d reconstruction$ .	0	used-for	used-for	1
it is based on a #weakly supervised dependency parser# that can model $speech syntax$ without relying on any annotated training corpus .	0	used-for	used-for	1
labeled data is replaced by a few #hand-crafted rules# that encode basic $syntactic knowledge$ .	0	used-for	used-for	1
#bayesian inference# then samples the $rules$ , disambiguating and combining them to create complex tree structures that maximize a discriminative model 's posterior on a target unlabeled corpus .	0	used-for	used-for	1
bayesian inference then samples the rules , disambiguating and combining #them# to create $complex tree structures$ that maximize a discriminative model 's posterior on a target unlabeled corpus .	0	used-for	used-for	1
bayesian inference then samples the rules , disambiguating and combining them to create #complex tree structures# that maximize a $discriminative model 's posterior$ on a target unlabeled corpus .	0	used-for	used-for	1
bayesian inference then samples the rules , disambiguating and combining them to create complex tree structures that maximize a $discriminative model 's posterior$ on a target #unlabeled corpus# .	0	used-for	used-for	1
this #posterior# encodes $sparse se-lectional preferences$ between a head word and its dependents .	0	used-for	used-for	1
the $model$ is evaluated on #english and czech newspaper texts# , and is then validated on french broadcast news transcriptions .	2	evaluate-for	evaluate-for	1
the $model$ is evaluated on english and czech newspaper texts , and is then validated on #french broadcast news transcriptions# .	2	evaluate-for	evaluate-for	1
#listen-communicate-show -lrb- lcs -rrb-# is a new paradigm for $human interaction with data sources$ .	0	used-for	used-for	1
we integrate a $spoken language understanding system$ with #intelligent mobile agents# that mediate between users and information sources .	4	part-of	part-of	1
we have built and will demonstrate an application of this #approach# called $lcs-marine$ .	0	used-for	used-for	1
a #domain independent model# is proposed for the $automated interpretation of nominal compounds$ in english .	0	used-for	used-for	1
a domain independent model is proposed for the automated interpretation of $nominal compounds$ in #english# .	5	feature-of	feature-of	1
this #model# is meant to account for $productive rules of interpretation$ which are inferred from the morpho-syntactic and semantic characteristics of the nominal constituents .	0	used-for	used-for	1
this model is meant to account for $productive rules of interpretation$ which are inferred from the #morpho-syntactic and semantic characteristics# of the nominal constituents .	0	used-for	used-for	1
this model is meant to account for productive rules of interpretation which are inferred from the #morpho-syntactic and semantic characteristics# of the $nominal constituents$ .	5	feature-of	feature-of	1
in particular , we make extensive use of pustejovsky 's principles concerning the $predicative information$ associated with #nominals# .	5	feature-of	feature-of	1
we argue that it is necessary to draw a line between #generalizable semantic principles# and $domain-specific semantic information$ .	6	compare	compare	1
we explain this distinction and we show how this #model# may be applied to the $interpretation of compounds$ in real texts , provided that complementary semantic information are retrieved .	0	used-for	used-for	1
we present a new #method# for $detecting interest points$ using histogram information .	0	used-for	used-for	1
we present a new method for $detecting interest points$ using #histogram information# .	0	used-for	used-for	1
unlike existing $interest point detectors$ , which measure #pixel-wise differences in image intensity# , our detectors incorporate histogram-based representations , and thus can find image regions that present a distinct distribution in the neighborhood .	2	evaluate-for	evaluate-for	1
unlike existing interest point detectors , which measure pixel-wise differences in image intensity , our $detectors$ incorporate #histogram-based representations# , and thus can find image regions that present a distinct distribution in the neighborhood .	4	part-of	part-of	1
the proposed #detectors# are able to capture $large-scale structures$ and distinctive textured patterns , and exhibit strong invariance to rotation , illumination variation , and blur .	0	used-for	used-for	1
the proposed #detectors# are able to capture large-scale structures and $distinctive textured patterns$ , and exhibit strong invariance to rotation , illumination variation , and blur .	0	used-for	used-for	1
the proposed #detectors# are able to capture large-scale structures and distinctive textured patterns , and exhibit strong invariance to $rotation$ , illumination variation , and blur .	0	used-for	used-for	1
the proposed #detectors# are able to capture large-scale structures and distinctive textured patterns , and exhibit strong invariance to rotation , $illumination variation$ , and blur .	0	used-for	used-for	1
the proposed #detectors# are able to capture large-scale structures and distinctive textured patterns , and exhibit strong invariance to rotation , illumination variation , and $blur$ .	0	used-for	used-for	1
the proposed detectors are able to capture #large-scale structures# and $distinctive textured patterns$ , and exhibit strong invariance to rotation , illumination variation , and blur .	1	conjunction	conjunction	1
the proposed detectors are able to capture large-scale structures and distinctive textured patterns , and exhibit strong invariance to #rotation# , $illumination variation$ , and blur .	1	conjunction	conjunction	1
the proposed detectors are able to capture large-scale structures and distinctive textured patterns , and exhibit strong invariance to rotation , #illumination variation# , and $blur$ .	1	conjunction	conjunction	1
the experimental results show that the proposed #histogram-based interest point detectors# perform particularly well for the tasks of $matching textured scenes$ under blur and illumination changes , in terms of repeatability and distinctiveness .	0	used-for	used-for	1
the experimental results show that the proposed $histogram-based interest point detectors$ perform particularly well for the tasks of matching textured scenes under blur and illumination changes , in terms of #repeatability# and distinctiveness .	2	evaluate-for	evaluate-for	1
the experimental results show that the proposed histogram-based interest point detectors perform particularly well for the tasks of matching textured scenes under blur and illumination changes , in terms of #repeatability# and $distinctiveness$ .	1	conjunction	conjunction	1
the experimental results show that the proposed $histogram-based interest point detectors$ perform particularly well for the tasks of matching textured scenes under blur and illumination changes , in terms of repeatability and #distinctiveness# .	2	evaluate-for	evaluate-for	1
an extension of our #method# to $space-time interest point detection$ for action classification is also presented .	0	used-for	used-for	1
an extension of our method to #space-time interest point detection# for $action classification$ is also presented .	0	used-for	used-for	1
we have implemented a $restricted domain parser$ called #plume# .	3	hyponym-of	hyponym-of	1
building on previous work at carnegie-mellon university e.g. -lsb- 4 , 5 , 8 -rsb- , #plume 's approach# to $parsing$ is based on semantic caseframe instantiation .	0	used-for	used-for	1
building on previous work at carnegie-mellon university e.g. -lsb- 4 , 5 , 8 -rsb- , $plume 's approach$ to parsing is based on #semantic caseframe instantiation# .	0	used-for	used-for	1
this has the advantages of efficiency on grammatical input , and $robustness$ in the face of #ungrammatical input# .	5	feature-of	feature-of	1
while #plume# is well adapted to simple $declarative and imperative utterances$ , it handles passives , relative clauses and interrogatives in an ad hoc manner leading to patchy syntactic coverage .	0	used-for	used-for	1
while plume is well adapted to simple declarative and imperative utterances , #it# handles $passives$ , relative clauses and interrogatives in an ad hoc manner leading to patchy syntactic coverage .	0	used-for	used-for	1
while plume is well adapted to simple declarative and imperative utterances , #it# handles passives , $relative clauses$ and interrogatives in an ad hoc manner leading to patchy syntactic coverage .	0	used-for	used-for	1
while plume is well adapted to simple declarative and imperative utterances , #it# handles passives , relative clauses and $interrogatives$ in an ad hoc manner leading to patchy syntactic coverage .	0	used-for	used-for	1
while plume is well adapted to simple declarative and imperative utterances , it handles #passives# , $relative clauses$ and interrogatives in an ad hoc manner leading to patchy syntactic coverage .	1	conjunction	conjunction	1
while plume is well adapted to simple declarative and imperative utterances , it handles passives , #relative clauses# and $interrogatives$ in an ad hoc manner leading to patchy syntactic coverage .	1	conjunction	conjunction	1
this paper outlines plume as it currently exists and describes our detailed design for extending #plume# to handle $passives$ , relative clauses , and interrogatives in a general manner .	0	used-for	used-for	1
this paper outlines plume as it currently exists and describes our detailed design for extending #plume# to handle passives , $relative clauses$ , and interrogatives in a general manner .	0	used-for	used-for	1
this paper outlines plume as it currently exists and describes our detailed design for extending #plume# to handle passives , relative clauses , and $interrogatives$ in a general manner .	0	used-for	used-for	1
this paper outlines plume as it currently exists and describes our detailed design for extending plume to handle #passives# , $relative clauses$ , and interrogatives in a general manner .	1	conjunction	conjunction	1
this paper outlines plume as it currently exists and describes our detailed design for extending plume to handle passives , #relative clauses# , and $interrogatives$ in a general manner .	1	conjunction	conjunction	1
in this paper , we present an #unlexicalized parser# for $german$ which employs smoothing and suffix analysis to achieve a labelled bracket f-score of 76.2 , higher than previously reported results on the negra corpus .	0	used-for	used-for	1
in this paper , we present an $unlexicalized parser$ for german which employs #smoothing# and suffix analysis to achieve a labelled bracket f-score of 76.2 , higher than previously reported results on the negra corpus .	0	used-for	used-for	1
in this paper , we present an unlexicalized parser for german which employs #smoothing# and $suffix analysis$ to achieve a labelled bracket f-score of 76.2 , higher than previously reported results on the negra corpus .	1	conjunction	conjunction	1
in this paper , we present an $unlexicalized parser$ for german which employs smoothing and #suffix analysis# to achieve a labelled bracket f-score of 76.2 , higher than previously reported results on the negra corpus .	0	used-for	used-for	1
in this paper , we present an $unlexicalized parser$ for german which employs smoothing and suffix analysis to achieve a #labelled bracket f-score# of 76.2 , higher than previously reported results on the negra corpus .	2	evaluate-for	evaluate-for	1
in this paper , we present an $unlexicalized parser$ for german which employs smoothing and suffix analysis to achieve a labelled bracket f-score of 76.2 , higher than previously reported results on the #negra corpus# .	2	evaluate-for	evaluate-for	1
in addition to the high #accuracy# of the $model$ , the use of smoothing in an unlexicalized parser allows us to better examine the interplay between smoothing and parsing results .	2	evaluate-for	evaluate-for	1
in addition to the high accuracy of the model , the use of #smoothing# in an $unlexicalized parser$ allows us to better examine the interplay between smoothing and parsing results .	0	used-for	used-for	1
this paper presents an #unsupervised learning approach# to disambiguate various $relations between named entities$ by use of various lexical and syntactic features from the contexts .	0	used-for	used-for	1
this paper presents an $unsupervised learning approach$ to disambiguate various relations between named entities by use of various #lexical and syntactic features# from the contexts .	0	used-for	used-for	1
#it# works by calculating eigenvectors of an adjacency graph 's laplacian to recover a $submanifold$ of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors .	0	used-for	used-for	1
$it$ works by calculating #eigenvectors# of an adjacency graph 's laplacian to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors .	0	used-for	used-for	1
it works by calculating $eigenvectors$ of an #adjacency graph 's laplacian# to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors .	5	feature-of	feature-of	1
it works by calculating eigenvectors of an adjacency graph 's laplacian to recover a $submanifold$ of data from a #high dimensionality space# and then performing cluster number estimation on the eigenvectors .	0	used-for	used-for	1
$it$ works by calculating eigenvectors of an adjacency graph 's laplacian to recover a submanifold of data from a high dimensionality space and then performing #cluster number estimation# on the eigenvectors .	0	used-for	used-for	1
it works by calculating eigenvectors of an adjacency graph 's laplacian to recover a submanifold of data from a high dimensionality space and then performing #cluster number estimation# on the $eigenvectors$ .	0	used-for	used-for	1
experiment results on #ace corpora# show that this $spectral clustering based approach$ outperforms the other clustering methods .	2	evaluate-for	evaluate-for	1
experiment results on #ace corpora# show that this spectral clustering based approach outperforms the other $clustering methods$ .	2	evaluate-for	evaluate-for	1
experiment results on ace corpora show that this #spectral clustering based approach# outperforms the other $clustering methods$ .	6	compare	compare	1
this paper proposes a generic mathematical formalism for the combination of various $structures$ : #strings# , trees , dags , graphs , and products of them .	3	hyponym-of	hyponym-of	1
this paper proposes a generic mathematical formalism for the combination of various structures : #strings# , $trees$ , dags , graphs , and products of them .	1	conjunction	conjunction	1
this paper proposes a generic mathematical formalism for the combination of various $structures$ : strings , #trees# , dags , graphs , and products of them .	3	hyponym-of	hyponym-of	1
this paper proposes a generic mathematical formalism for the combination of various structures : strings , #trees# , $dags$ , graphs , and products of them .	1	conjunction	conjunction	1
this paper proposes a generic mathematical formalism for the combination of various $structures$ : strings , trees , #dags# , graphs , and products of them .	3	hyponym-of	hyponym-of	1
this paper proposes a generic mathematical formalism for the combination of various structures : strings , trees , #dags# , $graphs$ , and products of them .	1	conjunction	conjunction	1
this paper proposes a generic mathematical formalism for the combination of various $structures$ : strings , trees , dags , #graphs# , and products of them .	3	hyponym-of	hyponym-of	1
this #formalism# is both elementary and powerful enough to strongly simulate many $grammar formalisms$ , such as rewriting systems , dependency grammars , tag , hpsg and lfg .	0	used-for	used-for	1
this formalism is both elementary and powerful enough to strongly simulate many $grammar formalisms$ , such as #rewriting systems# , dependency grammars , tag , hpsg and lfg .	3	hyponym-of	hyponym-of	1
this formalism is both elementary and powerful enough to strongly simulate many grammar formalisms , such as #rewriting systems# , $dependency grammars$ , tag , hpsg and lfg .	1	conjunction	conjunction	1
this formalism is both elementary and powerful enough to strongly simulate many $grammar formalisms$ , such as rewriting systems , #dependency grammars# , tag , hpsg and lfg .	3	hyponym-of	hyponym-of	1
this formalism is both elementary and powerful enough to strongly simulate many grammar formalisms , such as rewriting systems , #dependency grammars# , $tag$ , hpsg and lfg .	1	conjunction	conjunction	1
this formalism is both elementary and powerful enough to strongly simulate many $grammar formalisms$ , such as rewriting systems , dependency grammars , #tag# , hpsg and lfg .	3	hyponym-of	hyponym-of	1
this formalism is both elementary and powerful enough to strongly simulate many grammar formalisms , such as rewriting systems , dependency grammars , #tag# , $hpsg$ and lfg .	1	conjunction	conjunction	1
this formalism is both elementary and powerful enough to strongly simulate many $grammar formalisms$ , such as rewriting systems , dependency grammars , tag , #hpsg# and lfg .	3	hyponym-of	hyponym-of	1
this formalism is both elementary and powerful enough to strongly simulate many grammar formalisms , such as rewriting systems , dependency grammars , tag , #hpsg# and $lfg$ .	1	conjunction	conjunction	1
this formalism is both elementary and powerful enough to strongly simulate many $grammar formalisms$ , such as rewriting systems , dependency grammars , tag , hpsg and #lfg# .	3	hyponym-of	hyponym-of	1
a #mixed-signal paradigm# is presented for $high-resolution parallel inner-product computation$ in very high dimensions , suitable for efficient implementation of kernels in image processing .	0	used-for	used-for	1
a #mixed-signal paradigm# is presented for high-resolution parallel inner-product computation in very high dimensions , suitable for efficient implementation of $kernels$ in image processing .	0	used-for	used-for	1
a mixed-signal paradigm is presented for high-resolution parallel inner-product computation in very high dimensions , suitable for efficient implementation of #kernels# in $image processing$ .	0	used-for	used-for	1
at the core of the $externally digital architecture$ is a #high-density , low-power analog array# performing binary-binary partial matrix-vector multiplication .	4	part-of	part-of	1
at the core of the externally digital architecture is a $high-density , low-power analog array$ performing #binary-binary partial matrix-vector multiplication# .	0	used-for	used-for	1
full digital resolution is maintained even with low-resolution analog-to-digital conversion , owing to #random statistics# in the $analog summation of binary products$ .	4	part-of	part-of	1
a #random modulation scheme# produces $near-bernoulli statistics$ even for highly correlated inputs .	0	used-for	used-for	1
a $random modulation scheme$ produces near-bernoulli statistics even for #highly correlated inputs# .	0	used-for	used-for	1
the $approach$ is validated with #real image data# , and with experimental results from a cid/dram analog array prototype in 0.5 cents m cmos .	2	evaluate-for	evaluate-for	1
in this paper we specialize the #projective unifocal , bifo-cal , and trifocal tensors# to the $affine case$ , and show how the tensors obtained relate to the registered tensors encountered in previous work .	0	used-for	used-for	1
finally , we show how the $estimation of the tensors$ from #point correspondences# is achieved through factorization , and discuss the estimation from line correspondences .	0	used-for	used-for	1
finally , we show how the estimation of the $tensors$ from point correspondences is achieved through #factorization# , and discuss the estimation from line correspondences .	0	used-for	used-for	1
finally , we show how the estimation of the tensors from point correspondences is achieved through factorization , and discuss the $estimation$ from #line correspondences# .	0	used-for	used-for	1
we propose a #corpus-based method# -lrb- biber ,1993 ; nagao ,1993 ; smadja ,1993 -rrb- which generates $noun classifier associations -lrb- nca -rrb-$ to overcome the problems in classifier assignment and semantic construction of noun phrase .	0	used-for	used-for	1
we propose a #corpus-based method# -lrb- biber ,1993 ; nagao ,1993 ; smadja ,1993 -rrb- which generates noun classifier associations -lrb- nca -rrb- to overcome the problems in $classifier assignment$ and semantic construction of noun phrase .	0	used-for	used-for	1
we propose a #corpus-based method# -lrb- biber ,1993 ; nagao ,1993 ; smadja ,1993 -rrb- which generates noun classifier associations -lrb- nca -rrb- to overcome the problems in classifier assignment and $semantic construction of noun phrase$ .	0	used-for	used-for	1
we propose a corpus-based method -lrb- biber ,1993 ; nagao ,1993 ; smadja ,1993 -rrb- which generates #noun classifier associations -lrb- nca -rrb-# to overcome the problems in $classifier assignment$ and semantic construction of noun phrase .	0	used-for	used-for	1
we propose a corpus-based method -lrb- biber ,1993 ; nagao ,1993 ; smadja ,1993 -rrb- which generates #noun classifier associations -lrb- nca -rrb-# to overcome the problems in classifier assignment and $semantic construction of noun phrase$ .	0	used-for	used-for	1
we propose a corpus-based method -lrb- biber ,1993 ; nagao ,1993 ; smadja ,1993 -rrb- which generates noun classifier associations -lrb- nca -rrb- to overcome the problems in #classifier assignment# and $semantic construction of noun phrase$ .	1	conjunction	conjunction	1
the $nca$ is created statistically from a large corpus and recomposed under #concept hierarchy constraints# and frequency of occurrences .	0	used-for	used-for	1
the $nca$ is created statistically from a large corpus and recomposed under concept hierarchy constraints and #frequency of occurrences# .	0	used-for	used-for	1
the $perception of transparent objects$ from #images# is known to be a very hard problem in vision .	0	used-for	used-for	1
we show how $features$ that are imaged through a transparent object behave differently from #those# that are rigidly attached to the scene .	6	compare	compare	1
we present a novel #model-based approach# to recover the $shapes and the poses of transparent objects$ from known motion .	0	used-for	used-for	1
we present a novel model-based approach to recover the $shapes and the poses of transparent objects$ from #known motion# .	0	used-for	used-for	1
the objects can be complex in that $they$ may be composed of #multiple layers# with different refractive indices .	4	part-of	part-of	1
the objects can be complex in that they may be composed of $multiple layers$ with different #refractive indices# .	5	feature-of	feature-of	1
we have applied #it# to $real scenes$ that include transparent objects and recovered the shapes of the objects with high accuracy .	0	used-for	used-for	1
we have applied #it# to real scenes that include transparent objects and recovered the $shapes of the objects$ with high accuracy .	0	used-for	used-for	1
we have applied it to $real scenes$ that include #transparent objects# and recovered the shapes of the objects with high accuracy .	4	part-of	part-of	1
we have applied it to real scenes that include transparent objects and recovered the $shapes of the objects$ with high #accuracy# .	2	evaluate-for	evaluate-for	1
we propose a novel #probabilistic framework# for learning $visual models of 3d object categories$ by combining appearance information and geometric constraints .	0	used-for	used-for	1
we propose a novel $probabilistic framework$ for learning visual models of 3d object categories by combining #appearance information# and geometric constraints .	0	used-for	used-for	1
we propose a novel probabilistic framework for learning visual models of 3d object categories by combining #appearance information# and $geometric constraints$ .	1	conjunction	conjunction	1
we propose a novel $probabilistic framework$ for learning visual models of 3d object categories by combining appearance information and #geometric constraints# .	0	used-for	used-for	1
a #generative framework# is used for learning a $model$ that captures the relative position of parts within each of the discretized viewpoints .	0	used-for	used-for	1
contrary to most of the existing $mixture of viewpoints models$ , our #model# establishes explicit correspondences of parts across different viewpoints of the object class .	6	compare	compare	1
given a new #image# , $detection$ and classification are achieved by determining the position and viewpoint of the model that maximize recognition scores of the candidate objects .	0	used-for	used-for	1
given a new #image# , detection and $classification$ are achieved by determining the position and viewpoint of the model that maximize recognition scores of the candidate objects .	0	used-for	used-for	1
given a new image , #detection# and $classification$ are achieved by determining the position and viewpoint of the model that maximize recognition scores of the candidate objects .	1	conjunction	conjunction	1
given a new image , $detection$ and classification are achieved by determining the #position# and viewpoint of the model that maximize recognition scores of the candidate objects .	0	used-for	used-for	1
given a new image , detection and $classification$ are achieved by determining the #position# and viewpoint of the model that maximize recognition scores of the candidate objects .	0	used-for	used-for	1
given a new image , detection and classification are achieved by determining the #position# and $viewpoint$ of the model that maximize recognition scores of the candidate objects .	1	conjunction	conjunction	1
given a new image , $detection$ and classification are achieved by determining the position and #viewpoint# of the model that maximize recognition scores of the candidate objects .	0	used-for	used-for	1
given a new image , detection and $classification$ are achieved by determining the position and #viewpoint# of the model that maximize recognition scores of the candidate objects .	0	used-for	used-for	1
our approach is among the first to propose a #generative proba-bilistic framework# for $3d object categorization$ .	0	used-for	used-for	1
we test our #algorithm# on the $detection task$ and the viewpoint classification task by using '' car '' category from both the savarese et al. 2007 and pascal voc 2006 datasets .	0	used-for	used-for	1
we test our #algorithm# on the detection task and the $viewpoint classification task$ by using '' car '' category from both the savarese et al. 2007 and pascal voc 2006 datasets .	0	used-for	used-for	1
we test our algorithm on the #detection task# and the $viewpoint classification task$ by using '' car '' category from both the savarese et al. 2007 and pascal voc 2006 datasets .	1	conjunction	conjunction	1
we test our $algorithm$ on the detection task and the viewpoint classification task by using '' car '' category from both the savarese et al. 2007 and #pascal voc 2006 datasets# .	2	evaluate-for	evaluate-for	1
we show promising results in both the $detection and viewpoint classification tasks$ on these two challenging #datasets# .	2	evaluate-for	evaluate-for	1
we present an application of #ambiguity packing and stochastic disambiguation techniques# for $lexical-functional grammars -lrb- lfg -rrb-$ to the domain of sentence condensation .	0	used-for	used-for	1
we present an application of #ambiguity packing and stochastic disambiguation techniques# for lexical-functional grammars -lrb- lfg -rrb- to the domain of $sentence condensation$ .	0	used-for	used-for	1
our $system$ incorporates a #linguistic parser/generator# for lfg , a transfer component for parse reduction operating on packed parse forests , and a maximum-entropy model for stochastic output selection .	4	part-of	part-of	1
our system incorporates a #linguistic parser/generator# for $lfg$ , a transfer component for parse reduction operating on packed parse forests , and a maximum-entropy model for stochastic output selection .	0	used-for	used-for	1
our system incorporates a #linguistic parser/generator# for lfg , a $transfer component$ for parse reduction operating on packed parse forests , and a maximum-entropy model for stochastic output selection .	1	conjunction	conjunction	1
our $system$ incorporates a linguistic parser/generator for lfg , a #transfer component# for parse reduction operating on packed parse forests , and a maximum-entropy model for stochastic output selection .	4	part-of	part-of	1
our system incorporates a linguistic parser/generator for lfg , a #transfer component# for $parse reduction$ operating on packed parse forests , and a maximum-entropy model for stochastic output selection .	0	used-for	used-for	1
our system incorporates a linguistic parser/generator for lfg , a #transfer component# for parse reduction operating on packed parse forests , and a $maximum-entropy model$ for stochastic output selection .	1	conjunction	conjunction	1
our system incorporates a linguistic parser/generator for lfg , a transfer component for $parse reduction$ operating on #packed parse forests# , and a maximum-entropy model for stochastic output selection .	0	used-for	used-for	1
our $system$ incorporates a linguistic parser/generator for lfg , a transfer component for parse reduction operating on packed parse forests , and a #maximum-entropy model# for stochastic output selection .	4	part-of	part-of	1
our system incorporates a linguistic parser/generator for lfg , a transfer component for parse reduction operating on packed parse forests , and a #maximum-entropy model# for $stochastic output selection$ .	0	used-for	used-for	1
furthermore , we propose the use of standard #parser evaluation methods# for automatically evaluating the $summarization quality$ of sentence condensation systems .	2	evaluate-for	evaluate-for	1
furthermore , we propose the use of standard parser evaluation methods for automatically evaluating the #summarization quality# of $sentence condensation systems$ .	2	evaluate-for	evaluate-for	1
an experimental evaluation of #summarization quality# shows a close correlation between the $automatic parse-based evaluation$ and a manual evaluation of generated strings .	2	evaluate-for	evaluate-for	1
an experimental evaluation of summarization quality shows a close correlation between the #automatic parse-based evaluation# and a $manual evaluation$ of generated strings .	6	compare	compare	1
overall #summarization quality# of the proposed $system$ is state-of-the-art , with guaranteed grammaticality of the system output due to the use of a constraint-based parser/generator .	2	evaluate-for	evaluate-for	1
overall summarization quality of the proposed $system$ is state-of-the-art , with guaranteed #grammaticality# of the system output due to the use of a constraint-based parser/generator .	2	evaluate-for	evaluate-for	1
overall summarization quality of the proposed $system$ is state-of-the-art , with guaranteed grammaticality of the system output due to the use of a #constraint-based parser/generator# .	0	used-for	used-for	1
the #robust principal component analysis -lrb- robust pca -rrb- problem# has been considered in many $machine learning applications$ , where the goal is to decompose the data matrix to a low rank part plus a sparse residual .	0	used-for	used-for	1
the robust principal component analysis -lrb- robust pca -rrb- problem has been considered in many machine learning applications , where the goal is to decompose the $data matrix$ to a #low rank part# plus a sparse residual .	4	part-of	part-of	1
the robust principal component analysis -lrb- robust pca -rrb- problem has been considered in many machine learning applications , where the goal is to decompose the data matrix to a #low rank part# plus a $sparse residual$ .	1	conjunction	conjunction	1
the robust principal component analysis -lrb- robust pca -rrb- problem has been considered in many machine learning applications , where the goal is to decompose the $data matrix$ to a low rank part plus a #sparse residual# .	4	part-of	part-of	1
while current $approaches$ are developed by only considering the #low rank plus sparse structure# , in many applications , side information of row and/or column entities may also be given , and it is still unclear to what extent could such information help robust pca .	0	used-for	used-for	1
while current approaches are developed by only considering the low rank plus sparse structure , in many applications , side information of row and/or column entities may also be given , and it is still unclear to what extent could such #information# help $robust pca$ .	0	used-for	used-for	1
thus , in this paper , we study the problem of $robust pca$ with #side information# , where both prior structure and features of entities are exploited for recovery .	0	used-for	used-for	1
thus , in this paper , we study the problem of robust pca with side information , where both #prior structure# and $features of entities$ are exploited for recovery .	1	conjunction	conjunction	1
thus , in this paper , we study the problem of robust pca with side information , where both #prior structure# and features of entities are exploited for $recovery$ .	0	used-for	used-for	1
thus , in this paper , we study the problem of robust pca with side information , where both prior structure and #features of entities# are exploited for $recovery$ .	0	used-for	used-for	1
we propose a #convex problem# to incorporate $side information$ in robust pca and show that the low rank matrix can be exactly recovered via the proposed method under certain conditions .	0	used-for	used-for	1
we propose a convex problem to incorporate #side information# in $robust pca$ and show that the low rank matrix can be exactly recovered via the proposed method under certain conditions .	4	part-of	part-of	1
we propose a convex problem to incorporate side information in robust pca and show that the $low rank matrix$ can be exactly recovered via the proposed #method# under certain conditions .	0	used-for	used-for	1
in particular , our guarantee suggests that a substantial amount of $low rank matrices$ , which can not be recovered by standard robust pca , become re-coverable by our proposed #method# .	0	used-for	used-for	1
the result theoretically justifies the effectiveness of #features# in $robust pca$ .	5	feature-of	feature-of	1
in addition , we conduct synthetic experiments as well as a real application on #noisy image classification# to show that our $method$ also improves the performance in practice by exploiting side information .	2	evaluate-for	evaluate-for	1
in addition , we conduct synthetic experiments as well as a real application on noisy image classification to show that our $method$ also improves the performance in practice by exploiting #side information# .	0	used-for	used-for	1
this paper presents necessary and sufficient conditions for the use of #demonstrative expressions# in $english$ and discusses implications for current discourse processing algorithms .	5	feature-of	feature-of	1
this paper presents necessary and sufficient conditions for the use of demonstrative expressions in english and discusses #implications# for current $discourse processing algorithms$ .	0	used-for	used-for	1
this research is part of a larger study of #anaphoric expressions# , the results of which will be incorporated into a $natural language generation system$ .	0	used-for	used-for	1
using the #iemocap database# , $discrete -lrb- categorical -rrb- and continuous -lrb- attribute -rrb- emotional assessments$ evaluated by the actors and na ¨ ıve listeners are compared .	0	used-for	used-for	1
the problem of $blind separation of underdetermined instantaneous mixtures of independent signals$ is addressed through a #method# relying on nonstationarity of the original signals .	0	used-for	used-for	1
the problem of blind separation of underdetermined instantaneous mixtures of independent signals is addressed through a $method$ relying on #nonstationarity# of the original signals .	0	used-for	used-for	1
in comparison with previous works , in this paper it is assumed that the $signals$ are not i.i.d. in each epoch , but obey a #first-order autoregressive model# .	0	used-for	used-for	1
this #model# was shown to be more appropriate for $blind separation of natural speech signals .$	0	used-for	used-for	1
a $separation method$ is proposed that is nearly statistically efficient -lrb- approaching the corresponding #cramér-rao lower bound -rrb-# , if the separated signals obey the assumed model .	5	feature-of	feature-of	1
in the case of $natural speech signals$ , the #method# is shown to have separation accuracy better than the state-of-the-art methods .	0	used-for	used-for	1
in the case of natural speech signals , the #method# is shown to have separation accuracy better than the state-of-the-art $methods$ .	6	compare	compare	1
in the case of natural speech signals , the $method$ is shown to have #separation accuracy# better than the state-of-the-art methods .	2	evaluate-for	evaluate-for	1
in the case of natural speech signals , the method is shown to have #separation accuracy# better than the state-of-the-art $methods$ .	2	evaluate-for	evaluate-for	1
in the case of $natural speech signals$ , the method is shown to have separation accuracy better than the state-of-the-art #methods# .	0	used-for	used-for	1
this paper proposes to use a #convolution kernel over parse trees# to model $syntactic structure information$ for relation extraction .	0	used-for	used-for	1
this paper proposes to use a convolution kernel over parse trees to model #syntactic structure information# for $relation extraction$ .	0	used-for	used-for	1
our study reveals that the #syntactic structure features# embedded in a $parse tree$ are very effective for relation extraction and these features can be well captured by the convolution tree kernel .	5	feature-of	feature-of	1
our study reveals that the #syntactic structure features# embedded in a parse tree are very effective for $relation extraction$ and these features can be well captured by the convolution tree kernel .	0	used-for	used-for	1
our study reveals that the syntactic structure features embedded in a parse tree are very effective for relation extraction and these $features$ can be well captured by the #convolution tree kernel# .	0	used-for	used-for	1
evaluation on the #ace 2003 corpus# shows that the $convolution kernel over parse trees$ can achieve comparable performance with the previous best-reported feature-based methods on the 24 ace relation subtypes .	2	evaluate-for	evaluate-for	1
evaluation on the ace 2003 corpus shows that the $convolution kernel over parse trees$ can achieve comparable performance with the previous best-reported #feature-based methods# on the 24 ace relation subtypes .	6	compare	compare	1
it also shows that our #method# significantly outperforms the previous two $dependency tree kernels$ on the 5 ace relation major types .	6	compare	compare	1
this paper presents the results of automatically inducing a #combinatory categorial grammar -lrb- ccg -rrb- lexicon# from a $turkish dependency treebank$ .	4	part-of	part-of	1
the fact that #turkish# is an $agglutinating free word order language$ presents a challenge for language theories .	3	hyponym-of	hyponym-of	1
we explored possible ways to obtain a #compact lexicon# , consistent with ccg principles , from a $treebank$ which is an order of magnitude smaller than penn wsj .	4	part-of	part-of	1
we explored possible ways to obtain a compact lexicon , consistent with ccg principles , from a #treebank# which is an order of magnitude smaller than $penn wsj$ .	6	compare	compare	1
while #sentence extraction# as an approach to $summarization$ has been shown to work in documents of certain genres , because of the conversational nature of email communication where utterances are made in relation to one made previously , sentence extraction may not capture the necessary segments of dialogue that would make a summary coherent .	0	used-for	used-for	1
in this paper , we present our work on the #detection of question-answer pairs# in an email conversation for the task of $email summarization$ .	0	used-for	used-for	1
in this paper , we present our work on the $detection of question-answer pairs$ in an #email conversation# for the task of email summarization .	0	used-for	used-for	1
we show that various #features# based on the structure of email-threads can be used to improve upon $lexical similarity$ of discourse segments for question-answer pairing .	0	used-for	used-for	1
we show that various #features# based on the structure of email-threads can be used to improve upon lexical similarity of discourse segments for $question-answer pairing$ .	0	used-for	used-for	1
we show that various $features$ based on the #structure of email-threads# can be used to improve upon lexical similarity of discourse segments for question-answer pairing .	0	used-for	used-for	1
we show that various features based on the structure of email-threads can be used to improve upon #lexical similarity# of $discourse segments$ for question-answer pairing .	5	feature-of	feature-of	1
specifically , we show how to incorporate a simple #prior on the distribution of natural images# into $support vector machines$ .	0	used-for	used-for	1
#svms# are known to be robust to $overfitting$ ; however , a few training examples usually do not represent well the structure of the class .	0	used-for	used-for	1
our experiments on #real data sets# show that the resulting $detector$ is more robust to the choice of training examples , and substantially improves both linear and kernel svm when trained on 10 positive and 10 negative examples .	2	evaluate-for	evaluate-for	1
our experiments on real data sets show that the resulting #detector# is more robust to the choice of training examples , and substantially improves both $linear and kernel svm$ when trained on 10 positive and 10 negative examples .	6	compare	compare	1
although the study of clustering is centered around an intuitively compelling goal , it has been very difficult to develop a #unified framework# for $reasoning$ about it at a technical level , and profoundly diverse approaches to clustering abound in the research community .	0	used-for	used-for	1
relaxations of these properties expose some of the interesting -lrb- and unavoidable -rrb- trade-offs at work in $well-studied clustering techniques$ such as #single-linkage# , sum-of-pairs , k-means , and k-median .	3	hyponym-of	hyponym-of	1
relaxations of these properties expose some of the interesting -lrb- and unavoidable -rrb- trade-offs at work in well-studied clustering techniques such as #single-linkage# , $sum-of-pairs$ , k-means , and k-median .	1	conjunction	conjunction	1
relaxations of these properties expose some of the interesting -lrb- and unavoidable -rrb- trade-offs at work in $well-studied clustering techniques$ such as single-linkage , #sum-of-pairs# , k-means , and k-median .	3	hyponym-of	hyponym-of	1
relaxations of these properties expose some of the interesting -lrb- and unavoidable -rrb- trade-offs at work in well-studied clustering techniques such as single-linkage , #sum-of-pairs# , $k-means$ , and k-median .	1	conjunction	conjunction	1
relaxations of these properties expose some of the interesting -lrb- and unavoidable -rrb- trade-offs at work in $well-studied clustering techniques$ such as single-linkage , sum-of-pairs , #k-means# , and k-median .	3	hyponym-of	hyponym-of	1
relaxations of these properties expose some of the interesting -lrb- and unavoidable -rrb- trade-offs at work in well-studied clustering techniques such as single-linkage , sum-of-pairs , #k-means# , and $k-median$ .	1	conjunction	conjunction	1
relaxations of these properties expose some of the interesting -lrb- and unavoidable -rrb- trade-offs at work in $well-studied clustering techniques$ such as single-linkage , sum-of-pairs , k-means , and #k-median# .	3	hyponym-of	hyponym-of	1
with $relevant approach$ , we identify important contents by #pagerank algorithm# on the event map constructed from documents .	0	used-for	used-for	1
with relevant approach , we identify important contents by $pagerank algorithm$ on the #event map# constructed from documents .	0	used-for	used-for	1
with relevant approach , we identify important contents by pagerank algorithm on the $event map$ constructed from #documents# .	0	used-for	used-for	1
we present a #scanning method# that recovers $dense sub-pixel camera-projector correspondence$ without requiring any photometric calibration nor preliminary knowledge of their relative geometry .	0	used-for	used-for	1
$subpixel accuracy$ is achieved by considering several #zero-crossings# defined by the difference between pairs of unstructured patterns .	0	used-for	used-for	1
we use $gray-level band-pass white noise patterns$ that increase #robustness# to indirect lighting and scene discontinuities .	2	evaluate-for	evaluate-for	1
we use gray-level band-pass white noise patterns that increase $robustness$ to #indirect lighting# and scene discontinuities .	5	feature-of	feature-of	1
we use gray-level band-pass white noise patterns that increase robustness to #indirect lighting# and $scene discontinuities$ .	1	conjunction	conjunction	1
we use gray-level band-pass white noise patterns that increase $robustness$ to indirect lighting and #scene discontinuities# .	5	feature-of	feature-of	1
simulated and experimental results show that our #method# recovers $scene geometry$ with high subpixel precision , and that it can handle many challenges of active reconstruction systems .	0	used-for	used-for	1
simulated and experimental results show that our method recovers $scene geometry$ with high #subpixel precision# , and that it can handle many challenges of active reconstruction systems .	5	feature-of	feature-of	1
simulated and experimental results show that our method recovers scene geometry with high subpixel precision , and that #it# can handle many challenges of $active reconstruction systems$ .	0	used-for	used-for	1
we compare our results to $state of the art methods$ such as #mi-cro phase shifting# and modulated phase shifting .	3	hyponym-of	hyponym-of	1
we compare our results to state of the art methods such as #mi-cro phase shifting# and $modulated phase shifting$ .	1	conjunction	conjunction	1
we compare our results to $state of the art methods$ such as mi-cro phase shifting and #modulated phase shifting# .	3	hyponym-of	hyponym-of	1
this paper describes a novel #system# for $acquiring adjectival subcategorization frames -lrb- scfs -rrb-$ and associated frequency information from english corpus data .	0	used-for	used-for	1
the $system$ incorporates a #decision-tree classifier# for 30 scf types which tests for the presence of grammatical relations -lrb- grs -rrb- in the output of a robust statistical parser .	4	part-of	part-of	1
the system incorporates a #decision-tree classifier# for 30 scf types which tests for the presence of $grammatical relations -lrb- grs -rrb-$ in the output of a robust statistical parser .	0	used-for	used-for	1
$it$ uses a powerful #pattern-matching language# to classify grs into frames hierarchically in a way that mirrors inheritance-based lexica .	0	used-for	used-for	1
it uses a powerful #pattern-matching language# to classify $grs$ into frames hierarchically in a way that mirrors inheritance-based lexica .	0	used-for	used-for	1
the experiments show that the $system$ is able to detect scf types with 70 % #precision# and 66 % recall rate .	2	evaluate-for	evaluate-for	1
the experiments show that the system is able to detect scf types with 70 % #precision# and 66 % $recall$ rate .	1	conjunction	conjunction	1
the experiments show that the $system$ is able to detect scf types with 70 % precision and 66 % #recall# rate .	2	evaluate-for	evaluate-for	1
a new #tool# for $linguistic annotation of scfs$ in corpus data is also introduced which can considerably alleviate the process of obtaining training and test data for subcategorization acquisition .	0	used-for	used-for	1
a new tool for linguistic annotation of scfs in corpus data is also introduced which can considerably alleviate the process of obtaining #training and test data# for $subcategorization acquisition$ .	0	used-for	used-for	1
#machine transliteration/back-transliteration# plays an important role in many $multilingual speech and language applications$ .	0	used-for	used-for	1
in this paper , a novel #framework# for $machine transliteration/backtransliteration$ that allows us to carry out direct orthographical mapping -lrb- dom -rrb- between two different languages is presented .	0	used-for	used-for	1
in this paper , a novel framework for #machine transliteration/backtransliteration# that allows us to carry out $direct orthographical mapping -lrb- dom -rrb-$ between two different languages is presented .	0	used-for	used-for	1
under this #framework# , a $joint source-channel transliteration model$ , also called n-gram transliteration model -lrb- n-gram tm -rrb- , is further proposed to model the transliteration process .	0	used-for	used-for	1
under this framework , a joint source-channel transliteration model , also called #n-gram transliteration model -lrb- n-gram tm -rrb-# , is further proposed to model the $transliteration process$ .	0	used-for	used-for	1
we evaluate the proposed $methods$ through several #transliteration/backtransliteration# experiments for english/chinese and english/japanese language pairs .	2	evaluate-for	evaluate-for	1
we evaluate the proposed methods through several #transliteration/backtransliteration# experiments for $english/chinese and english/japanese language pairs$ .	0	used-for	used-for	1
our study reveals that the proposed $method$ not only reduces an extensive system development effort but also improves the #transliteration accuracy# significantly .	2	evaluate-for	evaluate-for	1
a #bio-inspired model# for an $analog programmable array processor -lrb- apap -rrb-$ , based on studies on the vertebrate retina , has permitted the realization of complex programmable spatio-temporal dynamics in vlsi .	0	used-for	used-for	1
a #bio-inspired model# for an analog programmable array processor -lrb- apap -rrb- , based on studies on the vertebrate retina , has permitted the realization of $complex programmable spatio-temporal dynamics$ in vlsi .	0	used-for	used-for	1
a $bio-inspired model$ for an analog programmable array processor -lrb- apap -rrb- , based on studies on the #vertebrate retina# , has permitted the realization of complex programmable spatio-temporal dynamics in vlsi .	0	used-for	used-for	1
a bio-inspired model for an analog programmable array processor -lrb- apap -rrb- , based on studies on the vertebrate retina , has permitted the realization of #complex programmable spatio-temporal dynamics# in $vlsi$ .	5	feature-of	feature-of	1
this model mimics the way in which $images$ are processed in the #visual pathway# , rendering a feasible alternative for the implementation of early vision applications in standard technologies .	0	used-for	used-for	1
#computing power per area# and $power consumption$ is amongst the highest reported for a single chip .	1	conjunction	conjunction	1
another problem with $determiners$ is their inherent #ambiguity# .	5	feature-of	feature-of	1
in this paper we propose a #logical formalism# , which , among other things , is suitable for representing $determiners$ without forcing a particular interpretation when their meaning is still not clear .	0	used-for	used-for	1
we investigate the #verbal and nonverbal means# for $grounding$ , and propose a design for embodied conversational agents that relies on both kinds of signals to establish common ground in human-computer interaction .	0	used-for	used-for	1
we investigate the verbal and nonverbal means for grounding , and propose a #design# for $embodied conversational agents$ that relies on both kinds of signals to establish common ground in human-computer interaction .	0	used-for	used-for	1
we investigate the verbal and nonverbal means for grounding , and propose a design for embodied conversational agents that relies on both kinds of signals to establish #common ground# in $human-computer interaction$ .	0	used-for	used-for	1
we analyzed #eye gaze# , $head nods$ and attentional focus in the context of a direction-giving task .	1	conjunction	conjunction	1
we analyzed #eye gaze# , head nods and attentional focus in the context of a $direction-giving task$ .	4	part-of	part-of	1
we analyzed eye gaze , #head nods# and $attentional focus$ in the context of a direction-giving task .	1	conjunction	conjunction	1
we analyzed eye gaze , #head nods# and attentional focus in the context of a $direction-giving task$ .	4	part-of	part-of	1
we analyzed eye gaze , head nods and #attentional focus# in the context of a $direction-giving task$ .	4	part-of	part-of	1
based on these results , we present an $eca$ that uses #verbal and nonverbal grounding acts# to update dialogue state .	0	used-for	used-for	1
based on these results , we present an eca that uses #verbal and nonverbal grounding acts# to update $dialogue state$ .	0	used-for	used-for	1
#sentence boundary detection# in speech is important for enriching $speech recognition output$ , making it easier for humans to read and downstream modules to process .	0	used-for	used-for	1
$sentence boundary detection$ in #speech# is important for enriching speech recognition output , making it easier for humans to read and downstream modules to process .	0	used-for	used-for	1
in previous work , we have developed #hidden markov model -lrb- hmm -rrb- and maximum entropy -lrb- maxent -rrb- classifiers# that integrate textual and prosodic knowledge sources for $detecting sentence boundaries$ .	0	used-for	used-for	1
in previous work , we have developed $hidden markov model -lrb- hmm -rrb- and maximum entropy -lrb- maxent -rrb- classifiers$ that integrate #textual and prosodic knowledge sources# for detecting sentence boundaries .	0	used-for	used-for	1
in this paper , we evaluate the use of a #conditional random field -lrb- crf -rrb-# for this $task$ and relate results with this model to our prior work .	0	used-for	used-for	1
we evaluate across two #corpora# -lrb- conversational telephone speech and broadcast news speech -rrb- on both $human transcriptions$ and speech recognition output .	2	evaluate-for	evaluate-for	1
we evaluate across two #corpora# -lrb- conversational telephone speech and broadcast news speech -rrb- on both human transcriptions and $speech recognition output$ .	2	evaluate-for	evaluate-for	1
we evaluate across two $corpora$ -lrb- #conversational telephone speech# and broadcast news speech -rrb- on both human transcriptions and speech recognition output .	3	hyponym-of	hyponym-of	1
we evaluate across two corpora -lrb- #conversational telephone speech# and $broadcast news speech$ -rrb- on both human transcriptions and speech recognition output .	1	conjunction	conjunction	1
we evaluate across two $corpora$ -lrb- conversational telephone speech and #broadcast news speech# -rrb- on both human transcriptions and speech recognition output .	3	hyponym-of	hyponym-of	1
we evaluate across two corpora -lrb- conversational telephone speech and broadcast news speech -rrb- on both #human transcriptions# and $speech recognition output$ .	1	conjunction	conjunction	1
in general , our #crf model# yields a lower error rate than the $hmm and max-ent models$ on the nist sentence boundary detection task in speech , although it is interesting to note that the best results are achieved by three-way voting among the classifiers .	6	compare	compare	1
in general , our $crf model$ yields a lower #error rate# than the hmm and max-ent models on the nist sentence boundary detection task in speech , although it is interesting to note that the best results are achieved by three-way voting among the classifiers .	2	evaluate-for	evaluate-for	1
in general , our crf model yields a lower #error rate# than the $hmm and max-ent models$ on the nist sentence boundary detection task in speech , although it is interesting to note that the best results are achieved by three-way voting among the classifiers .	2	evaluate-for	evaluate-for	1
in general , our $crf model$ yields a lower error rate than the hmm and max-ent models on the #nist sentence boundary detection task# in speech , although it is interesting to note that the best results are achieved by three-way voting among the classifiers .	2	evaluate-for	evaluate-for	1
in general , our crf model yields a lower error rate than the $hmm and max-ent models$ on the #nist sentence boundary detection task# in speech , although it is interesting to note that the best results are achieved by three-way voting among the classifiers .	2	evaluate-for	evaluate-for	1
in general , our crf model yields a lower error rate than the hmm and max-ent models on the $nist sentence boundary detection task$ in #speech# , although it is interesting to note that the best results are achieved by three-way voting among the classifiers .	5	feature-of	feature-of	1
in general , our crf model yields a lower error rate than the hmm and max-ent models on the nist sentence boundary detection task in speech , although it is interesting to note that the best results are achieved by $three-way voting$ among the #classifiers# .	0	used-for	used-for	1
this probably occurs because each #model# has different strengths and weaknesses for modeling the $knowledge sources$ .	0	used-for	used-for	1
we propose a novel #approach# to associate objects across multiple ptz cameras that can be used to perform $camera handoff in wide-area surveillance scenarios$ .	0	used-for	used-for	1
while previous $approaches$ relied on #geometric , appearance , or correlation-based information# for establishing correspondences between static cameras , they each have well-known limitations and are not extendable to wide-area settings with ptz cameras .	0	used-for	used-for	1
towards this goal , we also propose a novel $multiple instance learning -lrb- mil -rrb- formulation$ for the problem based on the #logistic softmax function of covariance-based region features# within a map estimation framework .	0	used-for	used-for	1
towards this goal , we also propose a novel $multiple instance learning -lrb- mil -rrb- formulation$ for the problem based on the logistic softmax function of covariance-based region features within a #map estimation framework# .	0	used-for	used-for	1
we demonstrate our #approach# with multiple ptz camera sequences in typical $outdoor surveillance settings$ and show a comparison with state-of-the-art approaches .	0	used-for	used-for	1
we demonstrate our #approach# with multiple ptz camera sequences in typical outdoor surveillance settings and show a comparison with $state-of-the-art approaches$ .	6	compare	compare	1
we demonstrate our $approach$ with #multiple ptz camera sequences# in typical outdoor surveillance settings and show a comparison with state-of-the-art approaches .	0	used-for	used-for	1
this paper solves a #specialized regression problem# to obtain $sampling probabilities$ for records in databases .	0	used-for	used-for	1
this paper solves a specialized regression problem to obtain #sampling probabilities# for $records$ in databases .	0	used-for	used-for	1
this paper solves a specialized regression problem to obtain sampling probabilities for #records# in $databases$ .	4	part-of	part-of	1
the goal is to sample a small set of $records$ over which evaluating #aggregate queries# can be done both efficiently and accurately .	2	evaluate-for	evaluate-for	1
we provide a #principled and provable solution# for this $problem$ ; it is parameterless and requires no data insights .	0	used-for	used-for	1
moreover , a $cost zero solution$ always exists and can only be excluded by #hard budget constraints# .	0	used-for	used-for	1
our extensive experimental results significantly improve over both #uniform sampling# and standard $stratified sampling$ which are de-facto the industry standards .	1	conjunction	conjunction	1
we consider the problem of computing the kullback-leibler distance , also called the relative entropy , between a #probabilistic context-free grammar# and a $probabilistic finite automaton$ .	6	compare	compare	1
we show that there is a #closed-form -lrb- analytical -rrb- solution# for one part of the $kullback-leibler distance$ , viz the cross-entropy .	0	used-for	used-for	1
we show that there is a #closed-form -lrb- analytical -rrb- solution# for one part of the kullback-leibler distance , viz the $cross-entropy$ .	0	used-for	used-for	1
we show that there is a closed-form -lrb- analytical -rrb- solution for one part of the $kullback-leibler distance$ , viz the #cross-entropy# .	4	part-of	part-of	1
we discuss several applications of the result to the problem of #distributional approximation# of $probabilistic context-free grammars$ by means of probabilistic finite automata .	5	feature-of	feature-of	1
we discuss several applications of the result to the problem of $distributional approximation$ of probabilistic context-free grammars by means of #probabilistic finite automata# .	0	used-for	used-for	1
in spite of over two decades of intense research , #illumination# and $pose invariance$ remain prohibitively challenging aspects of face recognition for most practical applications .	1	conjunction	conjunction	1
in spite of over two decades of intense research , #illumination# and pose invariance remain prohibitively challenging aspects of $face recognition$ for most practical applications .	4	part-of	part-of	1
in spite of over two decades of intense research , illumination and #pose invariance# remain prohibitively challenging aspects of $face recognition$ for most practical applications .	4	part-of	part-of	1
the objective of this work is to recognize faces using video sequences both for training and recognition input , in a realistic , unconstrained setup in which #lighting# , $pose$ and user motion pattern have a wide variability and face images are of low resolution .	1	conjunction	conjunction	1
the objective of this work is to recognize faces using video sequences both for training and recognition input , in a realistic , unconstrained setup in which lighting , #pose# and $user motion pattern$ have a wide variability and face images are of low resolution .	1	conjunction	conjunction	1
the objective of this work is to recognize faces using video sequences both for training and recognition input , in a realistic , unconstrained setup in which lighting , pose and user motion pattern have a wide variability and $face images$ are of low #resolution# .	5	feature-of	feature-of	1
in particular there are three areas of novelty : -lrb- i -rrb- we show how a #photometric model# of $image formation$ can be combined with a statistical model of generic face appearance variation , learnt offline , to generalize in the presence of extreme illumination changes ; -lrb- ii -rrb- we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve invariance to unseen head poses ; and -lrb- iii -rrb- we introduce an accurate video sequence '' reillumination '' algorithm to achieve robustness to face motion patterns in video .	0	used-for	used-for	1
in particular there are three areas of novelty : -lrb- i -rrb- we show how a #photometric model# of image formation can be combined with a $statistical model$ of generic face appearance variation , learnt offline , to generalize in the presence of extreme illumination changes ; -lrb- ii -rrb- we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve invariance to unseen head poses ; and -lrb- iii -rrb- we introduce an accurate video sequence '' reillumination '' algorithm to achieve robustness to face motion patterns in video .	1	conjunction	conjunction	1
in particular there are three areas of novelty : -lrb- i -rrb- we show how a photometric model of image formation can be combined with a #statistical model# of $generic face appearance variation$ , learnt offline , to generalize in the presence of extreme illumination changes ; -lrb- ii -rrb- we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve invariance to unseen head poses ; and -lrb- iii -rrb- we introduce an accurate video sequence '' reillumination '' algorithm to achieve robustness to face motion patterns in video .	0	used-for	used-for	1
in particular there are three areas of novelty : -lrb- i -rrb- we show how a photometric model of image formation can be combined with a #statistical model# of generic face appearance variation , learnt offline , to generalize in the presence of $extreme illumination changes$ ; -lrb- ii -rrb- we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve invariance to unseen head poses ; and -lrb- iii -rrb- we introduce an accurate video sequence '' reillumination '' algorithm to achieve robustness to face motion patterns in video .	0	used-for	used-for	1
in particular there are three areas of novelty : -lrb- i -rrb- we show how a photometric model of image formation can be combined with a statistical model of generic face appearance variation , learnt offline , to generalize in the presence of extreme illumination changes ; -lrb- ii -rrb- we use the #smoothness# of $geodesically local appearance manifold structure$ and a robust same-identity likelihood to achieve invariance to unseen head poses ; and -lrb- iii -rrb- we introduce an accurate video sequence '' reillumination '' algorithm to achieve robustness to face motion patterns in video .	5	feature-of	feature-of	1
in particular there are three areas of novelty : -lrb- i -rrb- we show how a photometric model of image formation can be combined with a statistical model of generic face appearance variation , learnt offline , to generalize in the presence of extreme illumination changes ; -lrb- ii -rrb- we use the smoothness of #geodesically local appearance manifold structure# and a $robust same-identity likelihood$ to achieve invariance to unseen head poses ; and -lrb- iii -rrb- we introduce an accurate video sequence '' reillumination '' algorithm to achieve robustness to face motion patterns in video .	1	conjunction	conjunction	1
in particular there are three areas of novelty : -lrb- i -rrb- we show how a photometric model of image formation can be combined with a statistical model of generic face appearance variation , learnt offline , to generalize in the presence of extreme illumination changes ; -lrb- ii -rrb- we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve invariance to unseen head poses ; and -lrb- iii -rrb- we introduce an accurate $video sequence '' reillumination '' algorithm$ to achieve #robustness# to face motion patterns in video .	2	evaluate-for	evaluate-for	1
in particular there are three areas of novelty : -lrb- i -rrb- we show how a photometric model of image formation can be combined with a statistical model of generic face appearance variation , learnt offline , to generalize in the presence of extreme illumination changes ; -lrb- ii -rrb- we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve invariance to unseen head poses ; and -lrb- iii -rrb- we introduce an accurate video sequence '' reillumination '' algorithm to achieve $robustness$ to #face motion patterns# in video .	5	feature-of	feature-of	1
in particular there are three areas of novelty : -lrb- i -rrb- we show how a photometric model of image formation can be combined with a statistical model of generic face appearance variation , learnt offline , to generalize in the presence of extreme illumination changes ; -lrb- ii -rrb- we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve invariance to unseen head poses ; and -lrb- iii -rrb- we introduce an accurate video sequence '' reillumination '' algorithm to achieve robustness to #face motion patterns# in $video$ .	4	part-of	part-of	1
we describe a $fully automatic recognition system$ based on the proposed #method# and an extensive evaluation on 171 individuals and over 1300 video sequences with extreme illumination , pose and head motion variation .	0	used-for	used-for	1
we describe a $fully automatic recognition system$ based on the proposed method and an extensive evaluation on 171 individuals and over 1300 #video sequences# with extreme illumination , pose and head motion variation .	2	evaluate-for	evaluate-for	1
we describe a fully automatic recognition system based on the proposed method and an extensive evaluation on 171 individuals and over 1300 $video sequences$ with extreme #illumination# , pose and head motion variation .	5	feature-of	feature-of	1
we describe a fully automatic recognition system based on the proposed method and an extensive evaluation on 171 individuals and over 1300 video sequences with extreme #illumination# , $pose$ and head motion variation .	1	conjunction	conjunction	1
we describe a fully automatic recognition system based on the proposed method and an extensive evaluation on 171 individuals and over 1300 $video sequences$ with extreme illumination , #pose# and head motion variation .	5	feature-of	feature-of	1
we describe a fully automatic recognition system based on the proposed method and an extensive evaluation on 171 individuals and over 1300 video sequences with extreme illumination , #pose# and $head motion variation$ .	1	conjunction	conjunction	1
we describe a fully automatic recognition system based on the proposed method and an extensive evaluation on 171 individuals and over 1300 $video sequences$ with extreme illumination , pose and #head motion variation# .	5	feature-of	feature-of	1
on this challenging #data set# our $system$ consistently demonstrated a nearly perfect recognition rate -lrb- over 99.7 % on all three databases -rrb- , significantly out-performing state-of-the-art commercial software and methods from the literature .	2	evaluate-for	evaluate-for	1
on this challenging data set our #system# consistently demonstrated a nearly perfect recognition rate -lrb- over 99.7 % on all three databases -rrb- , significantly out-performing state-of-the-art $commercial software$ and methods from the literature .	6	compare	compare	1
on this challenging data set our #system# consistently demonstrated a nearly perfect recognition rate -lrb- over 99.7 % on all three databases -rrb- , significantly out-performing state-of-the-art commercial software and $methods$ from the literature .	6	compare	compare	1
on this challenging data set our $system$ consistently demonstrated a nearly perfect #recognition rate# -lrb- over 99.7 % on all three databases -rrb- , significantly out-performing state-of-the-art commercial software and methods from the literature .	2	evaluate-for	evaluate-for	1
on this challenging data set our system consistently demonstrated a nearly perfect recognition rate -lrb- over 99.7 % on all three databases -rrb- , significantly out-performing state-of-the-art #commercial software# and $methods$ from the literature .	1	conjunction	conjunction	1
we present #minimum bayes-risk -lrb- mbr -rrb- decoding# for $statistical machine translation$ .	0	used-for	used-for	1
this statistical approach aims to minimize expected loss of translation errors under #loss functions# that measure $translation$ performance .	2	evaluate-for	evaluate-for	1
we describe a hierarchy of $loss functions$ that incorporate different levels of #linguistic information# from word strings , word-to-word alignments from an mt system , and syntactic structure from parse-trees of source and target language sentences .	0	used-for	used-for	1
we describe a hierarchy of $loss functions$ that incorporate different levels of linguistic information from word strings , #word-to-word alignments# from an mt system , and syntactic structure from parse-trees of source and target language sentences .	0	used-for	used-for	1
we describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings , #word-to-word alignments# from an $mt system$ , and syntactic structure from parse-trees of source and target language sentences .	4	part-of	part-of	1
we describe a hierarchy of $loss functions$ that incorporate different levels of linguistic information from word strings , word-to-word alignments from an mt system , and #syntactic structure# from parse-trees of source and target language sentences .	0	used-for	used-for	1
we describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings , word-to-word alignments from an mt system , and $syntactic structure$ from #parse-trees# of source and target language sentences .	4	part-of	part-of	1
we report the performance of the #mbr decoders# on a $chinese-to-english translation task$ .	0	used-for	used-for	1
our results show that #mbr decoding# can be used to tune $statistical mt$ performance for specific loss functions .	0	used-for	used-for	1
our results show that #mbr decoding# can be used to tune statistical mt performance for specific $loss functions$ .	0	used-for	used-for	1
this paper presents a critical discussion of the various #approaches# that have been used in the $evaluation of natural language systems$ .	0	used-for	used-for	1
we conclude that previous #approaches# have neglected to evaluate $systems$ in the context of their use , e.g. solving a task requiring data retrieval .	2	evaluate-for	evaluate-for	1
we conclude that previous approaches have neglected to evaluate #systems# in the context of their use , e.g. solving a $task$ requiring data retrieval .	0	used-for	used-for	1
we conclude that previous approaches have neglected to evaluate systems in the context of their use , e.g. solving a $task$ requiring #data retrieval# .	4	part-of	part-of	1
in the second half of the paper , we report a laboratory study using the #wizard of oz technique# to identify $nl requirements$ for carrying out this task .	0	used-for	used-for	1
in the second half of the paper , we report a laboratory study using the #wizard of oz technique# to identify nl requirements for carrying out this $task$ .	0	used-for	used-for	1
we evaluate the demands that #task dialogues# collected using this technique , place upon a $prototype natural language system$ .	0	used-for	used-for	1
we evaluate the demands that $task dialogues$ collected using this #technique# , place upon a prototype natural language system .	0	used-for	used-for	1
we present results on $addressee identification in four-participants face-to-face meetings$ using #bayesian network# and naive bayes classifiers .	0	used-for	used-for	1
we present results on $addressee identification in four-participants face-to-face meetings$ using bayesian network and #naive bayes classifiers# .	0	used-for	used-for	1
we present results on addressee identification in four-participants face-to-face meetings using $bayesian network$ and #naive bayes classifiers# .	1	conjunction	conjunction	1
first , we investigate how well the $addressee of a dialogue act$ can be predicted based on #gaze# , utterance and conversational context features .	0	used-for	used-for	1
first , we investigate how well the addressee of a dialogue act can be predicted based on #gaze# , $utterance$ and conversational context features .	1	conjunction	conjunction	1
first , we investigate how well the $addressee of a dialogue act$ can be predicted based on gaze , #utterance# and conversational context features .	0	used-for	used-for	1
first , we investigate how well the addressee of a dialogue act can be predicted based on gaze , #utterance# and $conversational context features$ .	1	conjunction	conjunction	1
first , we investigate how well the $addressee of a dialogue act$ can be predicted based on gaze , utterance and #conversational context features# .	0	used-for	used-for	1
both $classifiers$ perform the best when #conversational context# and utterance features are combined with speaker 's gaze information .	0	used-for	used-for	1
both classifiers perform the best when #conversational context# and $utterance features$ are combined with speaker 's gaze information .	1	conjunction	conjunction	1
both $classifiers$ perform the best when conversational context and #utterance features# are combined with speaker 's gaze information .	0	used-for	used-for	1
both $classifiers$ perform the best when conversational context and utterance features are combined with #speaker 's gaze information# .	0	used-for	used-for	1
both classifiers perform the best when conversational context and $utterance features$ are combined with #speaker 's gaze information# .	1	conjunction	conjunction	1
towards deep analysis of $compositional classes of paraphrases$ , we have examined a #class-oriented framework# for collecting paraphrase examples , in which sentential paraphrases are collected for each paraphrase class separately by means of automatic candidate generation and manual judgement .	0	used-for	used-for	1
towards deep analysis of compositional classes of paraphrases , we have examined a #class-oriented framework# for collecting $paraphrase examples$ , in which sentential paraphrases are collected for each paraphrase class separately by means of automatic candidate generation and manual judgement .	0	used-for	used-for	1
towards deep analysis of compositional classes of paraphrases , we have examined a class-oriented framework for collecting paraphrase examples , in which $sentential paraphrases$ are collected for each paraphrase class separately by means of #automatic candidate generation# and manual judgement .	0	used-for	used-for	1
towards deep analysis of compositional classes of paraphrases , we have examined a class-oriented framework for collecting paraphrase examples , in which sentential paraphrases are collected for each paraphrase class separately by means of #automatic candidate generation# and $manual judgement$ .	1	conjunction	conjunction	1
towards deep analysis of compositional classes of paraphrases , we have examined a class-oriented framework for collecting paraphrase examples , in which $sentential paraphrases$ are collected for each paraphrase class separately by means of automatic candidate generation and #manual judgement# .	0	used-for	used-for	1
the purpose of this research is to test the efficacy of applying #automated evaluation techniques# , originally devised for the $evaluation of human language learners$ , to the output of machine translation -lrb- mt -rrb- systems .	0	used-for	used-for	1
we believe that these #evaluation techniques# will provide information about both the $human language learning process$ , the translation process and the development of machine translation systems .	0	used-for	used-for	1
we believe that these #evaluation techniques# will provide information about both the human language learning process , the $translation process$ and the development of machine translation systems .	0	used-for	used-for	1
we believe that these #evaluation techniques# will provide information about both the human language learning process , the translation process and the development of $machine translation systems$ .	0	used-for	used-for	1
we believe that these evaluation techniques will provide information about both the #human language learning process# , the $translation process$ and the development of machine translation systems .	1	conjunction	conjunction	1
we believe that these evaluation techniques will provide information about both the human language learning process , the #translation process# and the development of $machine translation systems$ .	1	conjunction	conjunction	1
a #language learning# experiment showed that $assessors$ can differentiate native from non-native language essays in less than 100 words .	2	evaluate-for	evaluate-for	1
some of the extracts were $expert human translations$ , others were #machine translation outputs# .	1	conjunction	conjunction	1
the subjects were given three minutes per extract to determine whether they believed the sample output to be an #expert human translation# or a $machine translation$ .	6	compare	compare	1
this paper presents a #machine learning approach# to $bare slice disambiguation$ in dialogue .	0	used-for	used-for	1
this paper presents a machine learning approach to $bare slice disambiguation$ in #dialogue# .	0	used-for	used-for	1
we extract a set of $heuristic principles$ from a #corpus-based sample# and formulate them as probabilistic horn clauses .	0	used-for	used-for	1
we extract a set of $heuristic principles$ from a corpus-based sample and formulate them as #probabilistic horn clauses# .	5	feature-of	feature-of	1
we then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset , and run two different machine learning algorithms : #slipper# , a $rule-based learning algorithm$ , and timbl , a memory-based system .	3	hyponym-of	hyponym-of	1
we then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset , and run two different $machine learning algorithms$ : slipper , a #rule-based learning algorithm# , and timbl , a memory-based system .	4	part-of	part-of	1
we then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset , and run two different machine learning algorithms : slipper , a #rule-based learning algorithm# , and timbl , a $memory-based system$ .	6	compare	compare	1
we then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset , and run two different machine learning algorithms : slipper , a rule-based learning algorithm , and #timbl# , a $memory-based system$ .	3	hyponym-of	hyponym-of	1
we then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset , and run two different $machine learning algorithms$ : slipper , a rule-based learning algorithm , and timbl , a #memory-based system# .	4	part-of	part-of	1
the results show that the #features# in terms of which we formulate our $heuristic principles$ have significant predictive power , and that rules that closely resemble our horn clauses can be learnt automatically from these features .	5	feature-of	feature-of	1
we suggest a new goal and #evaluation criterion# for $word similarity measures$ .	0	used-for	used-for	1
the new criterion -- #meaning-entailing substitutability# -- fits the needs of $semantic-oriented nlp applications$ and can be evaluated directly -lrb- independent of an application -rrb- at a good level of human agreement .	0	used-for	used-for	1
the new criterion -- $meaning-entailing substitutability$ -- fits the needs of semantic-oriented nlp applications and can be evaluated directly -lrb- independent of an application -rrb- at a good level of #human agreement# .	2	evaluate-for	evaluate-for	1
motivated by this #semantic criterion# we analyze the empirical quality of $distributional word feature vectors$ and its impact on word similarity results , proposing an objective measure for evaluating feature vector quality .	2	evaluate-for	evaluate-for	1
motivated by this semantic criterion we analyze the empirical quality of #distributional word feature vectors# and its impact on $word similarity$ results , proposing an objective measure for evaluating feature vector quality .	0	used-for	used-for	1
motivated by this semantic criterion we analyze the empirical quality of distributional word feature vectors and its impact on word similarity results , proposing an objective #measure# for evaluating $feature vector quality$ .	2	evaluate-for	evaluate-for	1
finally , a novel #feature weighting and selection function# is presented , which yields superior $feature vectors$ and better word similarity performance .	0	used-for	used-for	1
finally , a novel #feature weighting and selection function# is presented , which yields superior feature vectors and better $word similarity$ performance .	0	used-for	used-for	1
finally , a novel feature weighting and selection function is presented , which yields superior #feature vectors# and better $word similarity$ performance .	1	conjunction	conjunction	1
this phenomenon causes many image processing techniques to fail as they assume the presence of only one layer at each examined site e.g. #motion estimation# and $object recognition$ .	1	conjunction	conjunction	1
this work presents an automated #technique# for $detecting reflections in image sequences$ by analyzing motion trajectories of feature points .	0	used-for	used-for	1
this work presents an automated $technique$ for detecting reflections in image sequences by analyzing #motion trajectories# of feature points .	0	used-for	used-for	1
this work presents an automated technique for detecting reflections in image sequences by analyzing $motion trajectories$ of #feature points# .	5	feature-of	feature-of	1
#it# models $reflection$ as regions containing two different layers moving over each other .	0	used-for	used-for	1
we present a strong $detector$ based on combining a set of weak #detectors# .	0	used-for	used-for	1
we use novel #priors# , generate $sparse and dense detection maps$ and our results show high detection rate with rejection to pathological motion and occlusion .	0	used-for	used-for	1
we use novel priors , generate sparse and dense detection maps and our results show high detection rate with rejection to #pathological motion# and $occlusion$ .	1	conjunction	conjunction	1
this paper considers the problem of $reconstructing the motion of a 3d articulated tree$ from #2d point correspondences# subject to some temporal prior .	0	used-for	used-for	1
hitherto , $smooth motion$ has been encouraged using a #trajectory basis# , yielding a hard combinatorial problem with time complexity growing exponentially in the number of frames .	0	used-for	used-for	1
hitherto , smooth motion has been encouraged using a trajectory basis , yielding a $hard combinatorial problem$ with #time complexity# growing exponentially in the number of frames .	2	evaluate-for	evaluate-for	1
#branch and bound strategies# have previously attempted to curb this $complexity$ whilst maintaining global optimality .	0	used-for	used-for	1
$branch and bound strategies$ have previously attempted to curb this complexity whilst maintaining #global optimality# .	5	feature-of	feature-of	1
however , #they# provide no guarantee of being more efficient than $exhaustive search$ .	6	compare	compare	1
extension to #affine projection# enables $reconstruction$ without estimating cameras .	0	used-for	used-for	1
#topical blog post retrieval# is the task of $ranking blog posts$ with respect to their relevance for a given topic .	3	hyponym-of	hyponym-of	1
topical blog post retrieval is the task of ranking $blog posts$ with respect to their #relevance# for a given topic .	5	feature-of	feature-of	1
to improve $topical blog post retrieval$ we incorporate #textual credibility indicators# in the retrieval process .	0	used-for	used-for	1
to improve topical blog post retrieval we incorporate #textual credibility indicators# in the $retrieval process$ .	4	part-of	part-of	1
we describe how to estimate these indicators and how to integrate #them# into a $retrieval approach$ based on language models .	4	part-of	part-of	1
we describe how to estimate these indicators and how to integrate $them$ into a retrieval approach based on #language models# .	0	used-for	used-for	1
experiments on the #trec blog track test set# show that both groups of $credibility indicators$ significantly improve retrieval effectiveness ; the best performance is achieved when combining them .	2	evaluate-for	evaluate-for	1
experiments on the trec blog track test set show that both groups of $credibility indicators$ significantly improve #retrieval effectiveness# ; the best performance is achieved when combining them .	2	evaluate-for	evaluate-for	1
we investigate the problem of learning to predict moves in the $board game of go$ from #game records of expert players# .	0	used-for	used-for	1
this #distribution# has numerous applications in $computer go$ , including serving as an efficient stand-alone go player .	0	used-for	used-for	1
#it# would also be effective as a $move selector$ and move sorter for game tree search and as a training tool for go players .	0	used-for	used-for	1
#it# would also be effective as a move selector and $move sorter$ for game tree search and as a training tool for go players .	0	used-for	used-for	1
#it# would also be effective as a move selector and move sorter for game tree search and as a $training tool$ for go players .	0	used-for	used-for	1
it would also be effective as a #move selector# and $move sorter$ for game tree search and as a training tool for go players .	1	conjunction	conjunction	1
it would also be effective as a #move selector# and move sorter for $game tree search$ and as a training tool for go players .	0	used-for	used-for	1
it would also be effective as a move selector and #move sorter# for $game tree search$ and as a training tool for go players .	0	used-for	used-for	1
it would also be effective as a move selector and move sorter for game tree search and as a #training tool# for $go players$ .	0	used-for	used-for	1
our $method$ has two major components : a -rrb- a #pattern extraction scheme# for efficiently harvesting patterns of given size and shape from expert game records and b -rrb- a bayesian learning algorithm -lrb- in two variants -rrb- that learns a distribution over the values of a move given a board position based on the local pattern context .	4	part-of	part-of	1
our method has two major components : a -rrb- a #pattern extraction scheme# for efficiently harvesting patterns of given size and shape from expert game records and b -rrb- a $bayesian learning algorithm$ -lrb- in two variants -rrb- that learns a distribution over the values of a move given a board position based on the local pattern context .	1	conjunction	conjunction	1
our $method$ has two major components : a -rrb- a pattern extraction scheme for efficiently harvesting patterns of given size and shape from expert game records and b -rrb- a #bayesian learning algorithm# -lrb- in two variants -rrb- that learns a distribution over the values of a move given a board position based on the local pattern context .	4	part-of	part-of	1
the $system$ is trained on 181,000 #expert games# and shows excellent prediction performance as indicated by its ability to perfectly predict the moves made by professional go players in 34 % of test positions .	0	used-for	used-for	1
we present a novel #approach# for $automatically acquiring english topic signatures$ .	0	used-for	used-for	1
#topic signatures# can be useful in a number of $natural language processing -lrb- nlp -rrb- applications$ , such as word sense disambiguation -lrb- wsd -rrb- and text summarisation .	0	used-for	used-for	1
#topic signatures# can be useful in a number of natural language processing -lrb- nlp -rrb- applications , such as $word sense disambiguation -lrb- wsd -rrb-$ and text summarisation .	0	used-for	used-for	1
#topic signatures# can be useful in a number of natural language processing -lrb- nlp -rrb- applications , such as word sense disambiguation -lrb- wsd -rrb- and $text summarisation$ .	0	used-for	used-for	1
topic signatures can be useful in a number of $natural language processing -lrb- nlp -rrb- applications$ , such as #word sense disambiguation -lrb- wsd -rrb-# and text summarisation .	3	hyponym-of	hyponym-of	1
topic signatures can be useful in a number of natural language processing -lrb- nlp -rrb- applications , such as #word sense disambiguation -lrb- wsd -rrb-# and $text summarisation$ .	1	conjunction	conjunction	1
topic signatures can be useful in a number of $natural language processing -lrb- nlp -rrb- applications$ , such as word sense disambiguation -lrb- wsd -rrb- and #text summarisation# .	3	hyponym-of	hyponym-of	1
our method takes advantage of the different way in which word senses are lexicalised in english and chinese , and also exploits the large amount of #chinese text# available in $corpora$ and on the web .	4	part-of	part-of	1
our method takes advantage of the different way in which word senses are lexicalised in english and chinese , and also exploits the large amount of #chinese text# available in corpora and on the $web$ .	4	part-of	part-of	1
our method takes advantage of the different way in which word senses are lexicalised in english and chinese , and also exploits the large amount of chinese text available in #corpora# and on the $web$ .	1	conjunction	conjunction	1
we evaluated the $topic signatures$ on a #wsd task# , where we trained a second-order vector cooccurrence algorithm on standard wsd datasets , with promising results .	2	evaluate-for	evaluate-for	1
we evaluated the topic signatures on a wsd task , where we trained a $second-order vector cooccurrence algorithm$ on standard #wsd datasets# , with promising results .	0	used-for	used-for	1
#joint matrix triangularization# is often used for estimating the $joint eigenstructure$ of a set m of matrices , with applications in signal processing and machine learning .	0	used-for	used-for	1
joint matrix triangularization is often used for estimating the #joint eigenstructure# of a set m of matrices , with applications in $signal processing$ and machine learning .	0	used-for	used-for	1
joint matrix triangularization is often used for estimating the #joint eigenstructure# of a set m of matrices , with applications in signal processing and $machine learning$ .	0	used-for	used-for	1
joint matrix triangularization is often used for estimating the joint eigenstructure of a set m of matrices , with applications in #signal processing# and $machine learning$ .	1	conjunction	conjunction	1
our main result is a first-order upper bound on the distance between any #approximate joint triangularizer# of the matrices in m ' and any $exact joint triangularizer$ of the matrices in m .	1	conjunction	conjunction	1
to our knowledge , this is the first a #posteriori bound# for $joint matrix decomposition$ .	0	used-for	used-for	1
the #psycholinguistic literature# provides evidence for $syntactic priming$ , i.e. , the tendency to repeat structures .	0	used-for	used-for	1
this paper describes a #method# for incorporating $priming$ into an incremental probabilistic parser .	0	used-for	used-for	1
this paper describes a method for incorporating #priming# into an $incremental probabilistic parser$ .	0	used-for	used-for	1
these models simulate the reading time advantage for #parallel structures# found in $human data$ , and also yield a small increase in overall parsing accuracy .	4	part-of	part-of	1
#learned confidence measures# gain increasing importance for $outlier removal$ and quality improvement in stereo vision .	0	used-for	used-for	1
#learned confidence measures# gain increasing importance for outlier removal and $quality improvement$ in stereo vision .	0	used-for	used-for	1
learned confidence measures gain increasing importance for #outlier removal# and $quality improvement$ in stereo vision .	1	conjunction	conjunction	1
learned confidence measures gain increasing importance for #outlier removal# and quality improvement in $stereo vision$ .	4	part-of	part-of	1
learned confidence measures gain increasing importance for outlier removal and #quality improvement# in $stereo vision$ .	4	part-of	part-of	1
however , acquiring the necessary training data is typically a tedious and time consuming $task$ that involves #manual interaction# , active sensing devices and/or synthetic scenes .	0	used-for	used-for	1
however , acquiring the necessary training data is typically a tedious and time consuming task that involves #manual interaction# , $active sensing devices$ and/or synthetic scenes .	1	conjunction	conjunction	1
however , acquiring the necessary training data is typically a tedious and time consuming $task$ that involves manual interaction , #active sensing devices# and/or synthetic scenes .	0	used-for	used-for	1
however , acquiring the necessary training data is typically a tedious and time consuming task that involves manual interaction , #active sensing devices# and/or $synthetic scenes$ .	1	conjunction	conjunction	1
however , acquiring the necessary training data is typically a tedious and time consuming $task$ that involves manual interaction , active sensing devices and/or #synthetic scenes# .	0	used-for	used-for	1
the key idea of our $approach$ is to use different #view points# for reasoning about contradictions and consistencies between multiple depth maps generated with the same stereo algorithm .	0	used-for	used-for	1
among other experiments , we demonstrate the potential of our #approach# by boosting the performance of three $learned confidence measures$ on the kitti2012 dataset by simply training them on a vast amount of automatically generated training data rather than a limited amount of laser ground truth data .	0	used-for	used-for	1
among other experiments , we demonstrate the potential of our approach by boosting the performance of three $learned confidence measures$ on the #kitti2012 dataset# by simply training them on a vast amount of automatically generated training data rather than a limited amount of laser ground truth data .	2	evaluate-for	evaluate-for	1
among other experiments , we demonstrate the potential of our approach by boosting the performance of three learned confidence measures on the kitti2012 dataset by simply training $them$ on a vast amount of #automatically generated training data# rather than a limited amount of laser ground truth data .	0	used-for	used-for	1
among other experiments , we demonstrate the potential of our approach by boosting the performance of three learned confidence measures on the kitti2012 dataset by simply training them on a vast amount of $automatically generated training data$ rather than a limited amount of #laser ground truth data# .	6	compare	compare	1
an important area of #learning in autonomous agents# is the ability to learn $domain-speciic models of actions$ to be used by planning systems .	0	used-for	used-for	1
an important area of learning in autonomous agents is the ability to learn $domain-speciic models of actions$ to be used by #planning systems# .	0	used-for	used-for	1
these $methods$ diier from previous work in the area in two ways : the use of an #action model formalism# which is better suited to the needs of a re-active agent , and successful implementation of noise-handling mechanisms .	0	used-for	used-for	1
these methods diier from previous work in the area in two ways : the use of an #action model formalism# which is better suited to the needs of a $re-active agent$ , and successful implementation of noise-handling mechanisms .	0	used-for	used-for	1
these $methods$ diier from previous work in the area in two ways : the use of an action model formalism which is better suited to the needs of a re-active agent , and successful implementation of #noise-handling mechanisms# .	0	used-for	used-for	1
training instances are generated from experience and observation , and a variant of #golem# is used to learn $action models$ from these instances .	0	used-for	used-for	1
the $integrated learning system$ has been experimentally validated in #simulated construction# and ooce domains .	2	evaluate-for	evaluate-for	1
the integrated learning system has been experimentally validated in #simulated construction# and $ooce domains$ .	1	conjunction	conjunction	1
the $integrated learning system$ has been experimentally validated in simulated construction and #ooce domains# .	2	evaluate-for	evaluate-for	1
this paper describes #ferret# , an $interactive question-answering -lrb- q/a -rrb- system$ designed to address the challenges of integrating automatic q/a applications into real-world environments .	3	hyponym-of	hyponym-of	1
this paper describes #ferret# , an interactive question-answering -lrb- q/a -rrb- system designed to address the challenges of $integrating automatic q/a applications into real-world environments$ .	0	used-for	used-for	1
$ferret$ utilizes a novel #approach# to q/a known as predictive questioning which attempts to identify the questions -lrb- and answers -rrb- that users need by analyzing how a user interacts with a system while gathering information related to a particular scenario .	0	used-for	used-for	1
ferret utilizes a novel #approach# to $q/a$ known as predictive questioning which attempts to identify the questions -lrb- and answers -rrb- that users need by analyzing how a user interacts with a system while gathering information related to a particular scenario .	0	used-for	used-for	1
in order to build robust $automatic abstracting systems$ , there is a need for better #training resources# than are currently available .	0	used-for	used-for	1
in this paper , we introduce an #annotation scheme# for $scientific articles$ which can be used to build such a resource in a consistent way .	0	used-for	used-for	1
in this paper , we introduce an #annotation scheme# for scientific articles which can be used to build such a $resource$ in a consistent way .	0	used-for	used-for	1
the seven categories of the $scheme$ are based on #rhetorical moves of argumentation# .	0	used-for	used-for	1
the $automated segmentation$ of #images# into semantically meaningful parts requires shape information since low-level feature analysis alone often fails to reach this goal .	0	used-for	used-for	1
we introduce a novel #method# of $shape constrained image segmentation$ which is based on mixtures of feature distributions for color and texture as well as probabilistic shape knowledge .	0	used-for	used-for	1
we introduce a novel $method$ of shape constrained image segmentation which is based on #mixtures of feature distributions# for color and texture as well as probabilistic shape knowledge .	0	used-for	used-for	1
we introduce a novel method of shape constrained image segmentation which is based on #mixtures of feature distributions# for $color$ and texture as well as probabilistic shape knowledge .	0	used-for	used-for	1
we introduce a novel method of shape constrained image segmentation which is based on #mixtures of feature distributions# for color and $texture$ as well as probabilistic shape knowledge .	0	used-for	used-for	1
we introduce a novel method of shape constrained image segmentation which is based on #mixtures of feature distributions# for color and texture as well as $probabilistic shape knowledge$ .	0	used-for	used-for	1
we introduce a novel method of shape constrained image segmentation which is based on mixtures of feature distributions for #color# and $texture$ as well as probabilistic shape knowledge .	1	conjunction	conjunction	1
we introduce a novel method of shape constrained image segmentation which is based on mixtures of feature distributions for color and #texture# as well as $probabilistic shape knowledge$ .	1	conjunction	conjunction	1
the combined #approach# is formulated in the framework of bayesian statistics to account for the $robust-ness requirement in image understanding$ .	0	used-for	used-for	1
the combined $approach$ is formulated in the framework of #bayesian statistics# to account for the robust-ness requirement in image understanding .	0	used-for	used-for	1
the goal of this work is the enrichment of $human-machine interactions$ in a #natural language environment# .	5	feature-of	feature-of	1
this paper highlights a particular class of $miscommunication$ -- #reference problems# -- by describing a case study and techniques for avoiding failures of reference .	3	hyponym-of	hyponym-of	1
this paper highlights a particular class of miscommunication -- reference problems -- by describing a case study and #techniques# for avoiding $failures of reference$ .	0	used-for	used-for	1
this paper examines the benefits of #system combination# for $unsupervised wsd$ .	0	used-for	used-for	1
we investigate several #voting - and arbiter-based combination strategies# over a diverse pool of $unsupervised wsd systems$ .	0	used-for	used-for	1
our $combination methods$ rely on #predominant senses# which are derived automatically from raw text .	0	used-for	used-for	1
our combination methods rely on $predominant senses$ which are derived automatically from #raw text# .	0	used-for	used-for	1
experiments using the #semcor and senseval-3 data sets# demonstrate that our $ensembles$ yield significantly better results when compared with state-of-the-art .	2	evaluate-for	evaluate-for	1
experiments using the #semcor and senseval-3 data sets# demonstrate that our ensembles yield significantly better results when compared with $state-of-the-art$ .	2	evaluate-for	evaluate-for	1
the applicability of many current $information extraction techniques$ is severely limited by the need for #supervised training data# .	0	used-for	used-for	1
we demonstrate that for certain $field structured extraction tasks$ , such as #classified advertisements# and bibliographic citations , small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion .	3	hyponym-of	hyponym-of	1
we demonstrate that for certain field structured extraction tasks , such as #classified advertisements# and $bibliographic citations$ , small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion .	1	conjunction	conjunction	1
we demonstrate that for certain $field structured extraction tasks$ , such as classified advertisements and #bibliographic citations# , small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion .	3	hyponym-of	hyponym-of	1
we demonstrate that for certain $field structured extraction tasks$ , such as classified advertisements and bibliographic citations , small amounts of #prior knowledge# can be used to learn effective models in a primarily unsupervised fashion .	0	used-for	used-for	1
although #hidden markov models -lrb- hmms -rrb-# provide a suitable $generative model$ for field structured text , general unsupervised hmm learning fails to learn useful structure in either of our domains .	0	used-for	used-for	1
although hidden markov models -lrb- hmms -rrb- provide a suitable #generative model# for $field structured text$ , general unsupervised hmm learning fails to learn useful structure in either of our domains .	0	used-for	used-for	1
in both domains , we found that #unsupervised methods# can attain accuracies with 400 unlabeled examples comparable to those attained by $supervised methods$ on 50 labeled examples , and that semi-supervised methods can make good use of small amounts of labeled data .	6	compare	compare	1
in both domains , we found that $unsupervised methods$ can attain #accuracies# with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples , and that semi-supervised methods can make good use of small amounts of labeled data .	2	evaluate-for	evaluate-for	1
in both domains , we found that unsupervised methods can attain #accuracies# with 400 unlabeled examples comparable to those attained by $supervised methods$ on 50 labeled examples , and that semi-supervised methods can make good use of small amounts of labeled data .	2	evaluate-for	evaluate-for	1
in both domains , we found that $unsupervised methods$ can attain accuracies with 400 #unlabeled examples# comparable to those attained by supervised methods on 50 labeled examples , and that semi-supervised methods can make good use of small amounts of labeled data .	0	used-for	used-for	1
in both domains , we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by $supervised methods$ on 50 #labeled examples# , and that semi-supervised methods can make good use of small amounts of labeled data .	0	used-for	used-for	1
in both domains , we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples , and that $semi-supervised methods$ can make good use of small amounts of #labeled data# .	0	used-for	used-for	1
this paper gives an overall account of a prototype $natural language question answering system$ , called #chat-80# .	3	hyponym-of	hyponym-of	1
the $system$ is implemented entirely in #prolog# , a programming language based on logic .	0	used-for	used-for	1
the system is implemented entirely in #prolog# , a $programming language$ based on logic .	3	hyponym-of	hyponym-of	1
the system is implemented entirely in prolog , a $programming language$ based on #logic# .	0	used-for	used-for	1
with the aid of a $logic-based grammar formalism$ called #extraposition grammars# , chat-80 translates english questions into the prolog subset of logic .	3	hyponym-of	hyponym-of	1
with the aid of a logic-based grammar formalism called #extraposition grammars# , $chat-80$ translates english questions into the prolog subset of logic .	0	used-for	used-for	1
the resulting $logical expression$ is then transformed by a #planning algorithm# into efficient prolog , cf. query optimisation in a relational database .	0	used-for	used-for	1
the resulting logical expression is then transformed by a planning algorithm into efficient prolog , cf. $query optimisation$ in a #relational database# .	0	used-for	used-for	1
$human action recognition$ from #well-segmented 3d skeleton data# has been intensively studied and attracting an increasing attention .	0	used-for	used-for	1
#online action detection# goes one step further and is more challenging , which identifies the $action type$ and localizes the action positions on the fly from the untrimmed stream .	0	used-for	used-for	1
#online action detection# goes one step further and is more challenging , which identifies the action type and localizes the $action positions$ on the fly from the untrimmed stream .	0	used-for	used-for	1
online action detection goes one step further and is more challenging , which identifies the #action type# and localizes the $action positions$ on the fly from the untrimmed stream .	1	conjunction	conjunction	1
$online action detection$ goes one step further and is more challenging , which identifies the action type and localizes the action positions on the fly from the #untrimmed stream# .	0	used-for	used-for	1
in this paper , we study the problem of $online action detection$ from the #streaming skeleton data# .	0	used-for	used-for	1
we propose a #multi-task end-to-end joint classification-regression recurrent neural network# to better explore the $action type$ and temporal localiza-tion information .	0	used-for	used-for	1
we propose a #multi-task end-to-end joint classification-regression recurrent neural network# to better explore the action type and $temporal localiza-tion information$ .	0	used-for	used-for	1
we propose a multi-task end-to-end joint classification-regression recurrent neural network to better explore the #action type# and $temporal localiza-tion information$ .	1	conjunction	conjunction	1
by employing a #joint classification and regression optimization objective# , this $network$ is capable of automatically localizing the start and end points of actions more accurately .	0	used-for	used-for	1
specifically , by leveraging the merits of the #deep long short-term memory -lrb- lstm -rrb- subnetwork# , the proposed $model$ automatically captures the complex long-range temporal dynamics , which naturally avoids the typical sliding window design and thus ensures high computational efficiency .	0	used-for	used-for	1
specifically , by leveraging the merits of the deep long short-term memory -lrb- lstm -rrb- subnetwork , the proposed $model$ automatically captures the complex #long-range temporal dynamics# , which naturally avoids the typical sliding window design and thus ensures high computational efficiency .	5	feature-of	feature-of	1
to evaluate our proposed $model$ , we build a large #streaming video dataset# with annotations .	2	evaluate-for	evaluate-for	1
experimental results on our #dataset# and the public $g3d dataset$ both demonstrate very promising performance of our scheme .	1	conjunction	conjunction	1
the task of #machine translation -lrb- mt -rrb- evaluation# is closely related to the task of $sentence-level semantic equivalence classification$ .	1	conjunction	conjunction	1
this paper investigates the utility of applying standard #mt evaluation methods# -lrb- bleu , nist , wer and per -rrb- to building $classifiers$ to predict semantic equivalence and entailment .	0	used-for	used-for	1
this paper investigates the utility of applying standard $mt evaluation methods$ -lrb- #bleu# , nist , wer and per -rrb- to building classifiers to predict semantic equivalence and entailment .	3	hyponym-of	hyponym-of	1
this paper investigates the utility of applying standard mt evaluation methods -lrb- #bleu# , $nist$ , wer and per -rrb- to building classifiers to predict semantic equivalence and entailment .	1	conjunction	conjunction	1
this paper investigates the utility of applying standard $mt evaluation methods$ -lrb- bleu , #nist# , wer and per -rrb- to building classifiers to predict semantic equivalence and entailment .	3	hyponym-of	hyponym-of	1
this paper investigates the utility of applying standard mt evaluation methods -lrb- bleu , #nist# , $wer$ and per -rrb- to building classifiers to predict semantic equivalence and entailment .	1	conjunction	conjunction	1
this paper investigates the utility of applying standard $mt evaluation methods$ -lrb- bleu , nist , #wer# and per -rrb- to building classifiers to predict semantic equivalence and entailment .	3	hyponym-of	hyponym-of	1
this paper investigates the utility of applying standard mt evaluation methods -lrb- bleu , nist , #wer# and $per$ -rrb- to building classifiers to predict semantic equivalence and entailment .	1	conjunction	conjunction	1
this paper investigates the utility of applying standard $mt evaluation methods$ -lrb- bleu , nist , wer and #per# -rrb- to building classifiers to predict semantic equivalence and entailment .	3	hyponym-of	hyponym-of	1
this paper investigates the utility of applying standard mt evaluation methods -lrb- bleu , nist , wer and per -rrb- to building #classifiers# to predict $semantic equivalence$ and entailment .	0	used-for	used-for	1
this paper investigates the utility of applying standard mt evaluation methods -lrb- bleu , nist , wer and per -rrb- to building #classifiers# to predict semantic equivalence and $entailment$ .	0	used-for	used-for	1
this paper investigates the utility of applying standard mt evaluation methods -lrb- bleu , nist , wer and per -rrb- to building classifiers to predict #semantic equivalence# and $entailment$ .	1	conjunction	conjunction	1
we also introduce a novel $classification method$ based on #per# which leverages part of speech information of the words contributing to the word matches and non-matches in the sentence .	0	used-for	used-for	1
we also introduce a novel classification method based on #per# which leverages $part of speech information$ of the words contributing to the word matches and non-matches in the sentence .	0	used-for	used-for	1
we also introduce a novel classification method based on per which leverages #part of speech information# of the words contributing to the $word matches and non-matches$ in the sentence .	0	used-for	used-for	1
our results show that #mt evaluation techniques# are able to produce useful $features$ for paraphrase classification and to a lesser extent entailment .	0	used-for	used-for	1
our results show that #mt evaluation techniques# are able to produce useful features for $paraphrase classification$ and to a lesser extent entailment .	0	used-for	used-for	1
our results show that #mt evaluation techniques# are able to produce useful features for paraphrase classification and to a lesser extent $entailment$ .	0	used-for	used-for	1
our results show that mt evaluation techniques are able to produce useful features for #paraphrase classification# and to a lesser extent $entailment$ .	1	conjunction	conjunction	1
our #technique# gives a substantial improvement in paraphrase classification accuracy over all of the other $models$ used in the experiments .	6	compare	compare	1
our $technique$ gives a substantial improvement in #paraphrase classification accuracy# over all of the other models used in the experiments .	2	evaluate-for	evaluate-for	1
our technique gives a substantial improvement in #paraphrase classification accuracy# over all of the other $models$ used in the experiments .	2	evaluate-for	evaluate-for	1
given an object model and a black-box measure of similarity between the model and candidate targets , we consider $visual object tracking$ as a #numerical optimization problem# .	0	used-for	used-for	1
during normal tracking conditions when the object is visible from frame to frame , #local optimization# is used to track the $local mode of the similarity measure$ in a parameter space of translation , rotation and scale .	0	used-for	used-for	1
during normal tracking conditions when the object is visible from frame to frame , local optimization is used to track the $local mode of the similarity measure$ in a #parameter space of translation , rotation and scale# .	0	used-for	used-for	1
however , when the object becomes partially or totally occluded , such local tracking is prone to failure , especially when common $prediction techniques$ like the #kalman filter# do not provide a good estimate of object parameters in future frames .	4	part-of	part-of	1
to recover from these inevitable tracking failures , we consider $object detection$ as a #global optimization problem# and solve it via adaptive simulated annealing -lrb- asa -rrb- , a method that avoids becoming trapped at local modes and is much faster than exhaustive search .	0	used-for	used-for	1
to recover from these inevitable tracking failures , we consider object detection as a global optimization problem and solve $it$ via #adaptive simulated annealing -lrb- asa -rrb-# , a method that avoids becoming trapped at local modes and is much faster than exhaustive search .	0	used-for	used-for	1
to recover from these inevitable tracking failures , we consider object detection as a global optimization problem and solve it via adaptive simulated annealing -lrb- asa -rrb- , a #method# that avoids becoming trapped at local modes and is much faster than $exhaustive search$ .	6	compare	compare	1
as a $monte carlo approach$ , #asa# stochastically samples the parameter space , in contrast to local deterministic search .	3	hyponym-of	hyponym-of	1
as a monte carlo approach , #asa# stochastically samples the parameter space , in contrast to $local deterministic search$ .	6	compare	compare	1
we apply #cluster analysis# on the $sampled parameter space$ to redetect the object and renew the local tracker .	0	used-for	used-for	1
we apply #cluster analysis# on the sampled parameter space to redetect the object and renew the $local tracker$ .	0	used-for	used-for	1
our $numerical hybrid local and global mode-seeking tracker$ is validated on challenging #airborne videos# with heavy occlusion and large camera motions .	2	evaluate-for	evaluate-for	1
our numerical hybrid local and global mode-seeking tracker is validated on challenging $airborne videos$ with #heavy occlusion# and large camera motions .	5	feature-of	feature-of	1
our numerical hybrid local and global mode-seeking tracker is validated on challenging airborne videos with #heavy occlusion# and large $camera motions$ .	1	conjunction	conjunction	1
our numerical hybrid local and global mode-seeking tracker is validated on challenging $airborne videos$ with heavy occlusion and large #camera motions# .	5	feature-of	feature-of	1
our $approach$ outperforms #state-of-the-art trackers# on the vivid benchmark datasets .	6	compare	compare	1
our $approach$ outperforms state-of-the-art trackers on the #vivid benchmark datasets# .	2	evaluate-for	evaluate-for	1
our approach outperforms $state-of-the-art trackers$ on the #vivid benchmark datasets# .	2	evaluate-for	evaluate-for	1
#techniques# for $automatically training modules$ of a natural language generator have recently been proposed , but a fundamental concern is whether the quality of utterances produced with trainable components can compete with hand-crafted template-based or rule-based approaches .	0	used-for	used-for	1
techniques for #automatically training modules# of a $natural language generator$ have recently been proposed , but a fundamental concern is whether the quality of utterances produced with trainable components can compete with hand-crafted template-based or rule-based approaches .	4	part-of	part-of	1
techniques for automatically training modules of a natural language generator have recently been proposed , but a fundamental concern is whether the quality of #utterances# produced with $trainable components$ can compete with hand-crafted template-based or rule-based approaches .	2	evaluate-for	evaluate-for	1
techniques for automatically training modules of a natural language generator have recently been proposed , but a fundamental concern is whether the quality of #utterances# produced with trainable components can compete with $hand-crafted template-based or rule-based approaches$ .	2	evaluate-for	evaluate-for	1
techniques for automatically training modules of a natural language generator have recently been proposed , but a fundamental concern is whether the quality of utterances produced with #trainable components# can compete with $hand-crafted template-based or rule-based approaches$ .	6	compare	compare	1
in this paper we experimentally evaluate a #trainable sentence planner# for a $spoken dialogue system$ by eliciting subjective human judgments .	0	used-for	used-for	1
in this paper we experimentally evaluate a $trainable sentence planner$ for a spoken dialogue system by eliciting #subjective human judgments# .	2	evaluate-for	evaluate-for	1
in order to perform an exhaustive comparison , we also evaluate a #hand-crafted template-based generation component# , two $rule-based sentence planners$ , and two baseline sentence planners .	1	conjunction	conjunction	1
in order to perform an exhaustive comparison , we also evaluate a hand-crafted template-based generation component , two #rule-based sentence planners# , and two $baseline sentence planners$ .	1	conjunction	conjunction	1
we show that the #trainable sentence planner# performs better than the $rule-based systems$ and the baselines , and as well as the hand-crafted system .	6	compare	compare	1
we show that the #trainable sentence planner# performs better than the rule-based systems and the $baselines$ , and as well as the hand-crafted system .	6	compare	compare	1
we show that the #trainable sentence planner# performs better than the rule-based systems and the baselines , and as well as the $hand-crafted system$ .	6	compare	compare	1
we show that the trainable sentence planner performs better than the #rule-based systems# and the $baselines$ , and as well as the hand-crafted system .	1	conjunction	conjunction	1
we show that the trainable sentence planner performs better than the rule-based systems and the #baselines# , and as well as the $hand-crafted system$ .	1	conjunction	conjunction	1
a new #algorithm# is proposed for $novel view generation$ in one-to-one teleconferencing applications .	0	used-for	used-for	1
a new algorithm is proposed for #novel view generation# in $one-to-one teleconferencing applications$ .	0	used-for	used-for	1
given the $video streams$ acquired by two #cameras# placed on either side of a computer monitor , the proposed algorithm synthesises images from a virtual camera in arbitrary position -lrb- typically located within the monitor -rrb- to facilitate eye contact .	0	used-for	used-for	1
given the video streams acquired by two cameras placed on either side of a computer monitor , the proposed #algorithm# synthesises images from a virtual camera in arbitrary position -lrb- typically located within the monitor -rrb- to facilitate $eye contact$ .	0	used-for	used-for	1
given the video streams acquired by two cameras placed on either side of a computer monitor , the proposed algorithm synthesises $images$ from a #virtual camera# in arbitrary position -lrb- typically located within the monitor -rrb- to facilitate eye contact .	0	used-for	used-for	1
given the video streams acquired by two cameras placed on either side of a computer monitor , the proposed algorithm synthesises images from a $virtual camera$ in #arbitrary position# -lrb- typically located within the monitor -rrb- to facilitate eye contact .	5	feature-of	feature-of	1
our #technique# is based on an improved , dynamic-programming , stereo algorithm for efficient $novel-view generation$ .	0	used-for	used-for	1
our $technique$ is based on an improved , #dynamic-programming , stereo algorithm# for efficient novel-view generation .	0	used-for	used-for	1
the two main contributions of this paper are : i -rrb- a new type of #three-plane graph# for $dense-stereo dynamic-programming$ , that encourages correct occlusion labeling ; ii -rrb- a compact geometric derivation for novel-view synthesis by direct projection of the minimum-cost surface .	0	used-for	used-for	1
the two main contributions of this paper are : i -rrb- a new type of three-plane graph for #dense-stereo dynamic-programming# , that encourages correct $occlusion labeling$ ; ii -rrb- a compact geometric derivation for novel-view synthesis by direct projection of the minimum-cost surface .	0	used-for	used-for	1
the two main contributions of this paper are : i -rrb- a new type of three-plane graph for dense-stereo dynamic-programming , that encourages correct occlusion labeling ; ii -rrb- a #compact geometric derivation# for $novel-view synthesis$ by direct projection of the minimum-cost surface .	0	used-for	used-for	1
the two main contributions of this paper are : i -rrb- a new type of three-plane graph for dense-stereo dynamic-programming , that encourages correct occlusion labeling ; ii -rrb- a $compact geometric derivation$ for novel-view synthesis by #direct projection of the minimum-cost surface# .	0	used-for	used-for	1
furthermore , this paper presents a novel #algorithm# for the $temporal maintenance of a background model$ to enhance the rendering of occlusions and reduce temporal artefacts -lrb- flicker -rrb- ; and a cost aggregation algorithm that acts directly on our three-dimensional matching cost space .	0	used-for	used-for	1
furthermore , this paper presents a novel #algorithm# for the temporal maintenance of a background model to enhance the $rendering of occlusions$ and reduce temporal artefacts -lrb- flicker -rrb- ; and a cost aggregation algorithm that acts directly on our three-dimensional matching cost space .	0	used-for	used-for	1
furthermore , this paper presents a novel #algorithm# for the temporal maintenance of a background model to enhance the rendering of occlusions and reduce $temporal artefacts -lrb- flicker -rrb-$ ; and a cost aggregation algorithm that acts directly on our three-dimensional matching cost space .	0	used-for	used-for	1
furthermore , this paper presents a novel $algorithm$ for the temporal maintenance of a background model to enhance the rendering of occlusions and reduce temporal artefacts -lrb- flicker -rrb- ; and a #cost aggregation algorithm# that acts directly on our three-dimensional matching cost space .	1	conjunction	conjunction	1
furthermore , this paper presents a novel algorithm for the temporal maintenance of a background model to enhance the rendering of occlusions and reduce temporal artefacts -lrb- flicker -rrb- ; and a #cost aggregation algorithm# that acts directly on our $three-dimensional matching cost space$ .	0	used-for	used-for	1
examples are given that demonstrate the #robustness# of the new $algorithm$ to spatial and temporal artefacts for long stereo video streams .	2	evaluate-for	evaluate-for	1
examples are given that demonstrate the robustness of the new #algorithm# to $spatial and temporal artefacts$ for long stereo video streams .	0	used-for	used-for	1
examples are given that demonstrate the robustness of the new algorithm to #spatial and temporal artefacts# for $long stereo video streams$ .	0	used-for	used-for	1
we further demonstrate $synthesis$ from a freely #translating virtual camera# .	0	used-for	used-for	1
to a large extent , these statistics reflect #semantic constraints# and thus are used to disambiguate $anaphora references$ and syntactic ambiguities .	0	used-for	used-for	1
to a large extent , these statistics reflect #semantic constraints# and thus are used to disambiguate anaphora references and $syntactic ambiguities$ .	0	used-for	used-for	1
to a large extent , these statistics reflect semantic constraints and thus are used to disambiguate #anaphora references# and $syntactic ambiguities$ .	1	conjunction	conjunction	1
the results of the experiment show that in most of the cases the #cooccurrence statistics# indeed reflect the semantic constraints and thus provide a basis for a useful $disambiguation tool$ .	0	used-for	used-for	1
we present a novel #method# for $discovering parallel sentences$ in comparable , non-parallel corpora .	0	used-for	used-for	1
we present a novel method for $discovering parallel sentences$ in #comparable , non-parallel corpora# .	0	used-for	used-for	1
using this #approach# , we extract $parallel data$ from large chinese , arabic , and english non-parallel newspaper corpora .	0	used-for	used-for	1
using this approach , we extract #parallel data# from large $chinese , arabic , and english non-parallel newspaper corpora$ .	4	part-of	part-of	1
we evaluate the quality of the extracted data by showing that #it# improves the performance of a state-of-the-art $statistical machine translation system$ .	0	used-for	used-for	1
we also show that a good-quality $mt system$ can be built from scratch by starting with a very small #parallel corpus# -lrb- 100,000 words -rrb- and exploiting a large non-parallel corpus .	0	used-for	used-for	1
we also show that a good-quality mt system can be built from scratch by starting with a very small #parallel corpus# -lrb- 100,000 words -rrb- and exploiting a large $non-parallel corpus$ .	1	conjunction	conjunction	1
we also show that a good-quality $mt system$ can be built from scratch by starting with a very small parallel corpus -lrb- 100,000 words -rrb- and exploiting a large #non-parallel corpus# .	0	used-for	used-for	1
thus , our $method$ can be applied with great benefit to language pairs for which only #scarce resources# are available .	0	used-for	used-for	1
in this paper , we describe a #search procedure# for $statistical machine translation -lrb- mt -rrb-$ based on dynamic programming -lrb- dp -rrb- .	0	used-for	used-for	1
in this paper , we describe a search procedure for $statistical machine translation -lrb- mt -rrb-$ based on #dynamic programming -lrb- dp -rrb-# .	0	used-for	used-for	1
starting from a dp-based solution to the traveling salesman problem , we present a novel #technique# to restrict the possible word reordering between source and target language in order to achieve an efficient $search algorithm$ .	0	used-for	used-for	1
the experimental tests are carried out on the #verbmobil task# -lrb- german-english , 8000-word vocabulary -rrb- , which is a $limited-domain spoken-language task$ .	3	hyponym-of	hyponym-of	1
a purely functional implementation of $lr-parsers$ is given , together with a simple #correctness proof# .	1	conjunction	conjunction	1
$it$ is presented as a generalization of the #recursive descent parser# .	0	used-for	used-for	1
for non-lr grammars the #time-complexity# of our $parser$ is cubic if the functions that constitute the parser are implemented as memo-functions , i.e. functions that memorize the results of previous invocations .	2	evaluate-for	evaluate-for	1
for $non-lr grammars$ the time-complexity of our #parser# is cubic if the functions that constitute the parser are implemented as memo-functions , i.e. functions that memorize the results of previous invocations .	0	used-for	used-for	1
for non-lr grammars the time-complexity of our parser is cubic if the functions that constitute the $parser$ are implemented as #memo-functions# , i.e. functions that memorize the results of previous invocations .	0	used-for	used-for	1
#memo-functions# also facilitate a simple way to construct a very compact representation of the $parse forest$ .	0	used-for	used-for	1
for $lr -lrb- 0 -rrb- grammars$ , our #algorithm# is closely related to the recursive ascent parsers recently discovered by kruse-man aretz -lsb- 1 -rsb- and roberts -lsb- 2 -rsb- .	0	used-for	used-for	1
for lr -lrb- 0 -rrb- grammars , our #algorithm# is closely related to the $recursive ascent parsers$ recently discovered by kruse-man aretz -lsb- 1 -rsb- and roberts -lsb- 2 -rsb- .	1	conjunction	conjunction	1
extended cf grammars -lrb- $grammars$ with #regular expressions# at the right hand side -rrb- can be parsed with a simple modification of the lr-parser for normal cf grammars .	5	feature-of	feature-of	1
$extended cf grammars$ -lrb- grammars with regular expressions at the right hand side -rrb- can be parsed with a simple modification of the #lr-parser# for normal cf grammars .	0	used-for	used-for	1
extended cf grammars -lrb- grammars with regular expressions at the right hand side -rrb- can be parsed with a simple modification of the #lr-parser# for normal $cf grammars$ .	0	used-for	used-for	1
in this theory , $discourse structure$ is composed of three separate but interrelated #components# : the structure of the sequence of utterances -lrb- called the linguistic structure -rrb- , a structure of purposes -lrb- called the intentional structure -rrb- , and the state of focus of attention -lrb- called the attentional state -rrb- .	4	part-of	part-of	1
in this theory , discourse structure is composed of three separate but interrelated $components$ : the structure of the sequence of utterances -lrb- called the #linguistic structure# -rrb- , a structure of purposes -lrb- called the intentional structure -rrb- , and the state of focus of attention -lrb- called the attentional state -rrb- .	4	part-of	part-of	1
in this theory , discourse structure is composed of three separate but interrelated components : the structure of the sequence of utterances -lrb- called the #linguistic structure# -rrb- , a structure of purposes -lrb- called the $intentional structure$ -rrb- , and the state of focus of attention -lrb- called the attentional state -rrb- .	1	conjunction	conjunction	1
in this theory , discourse structure is composed of three separate but interrelated $components$ : the structure of the sequence of utterances -lrb- called the linguistic structure -rrb- , a structure of purposes -lrb- called the #intentional structure# -rrb- , and the state of focus of attention -lrb- called the attentional state -rrb- .	4	part-of	part-of	1
in this theory , discourse structure is composed of three separate but interrelated components : the structure of the sequence of utterances -lrb- called the linguistic structure -rrb- , a structure of purposes -lrb- called the #intentional structure# -rrb- , and the state of focus of attention -lrb- called the $attentional state$ -rrb- .	1	conjunction	conjunction	1
in this theory , discourse structure is composed of three separate but interrelated $components$ : the structure of the sequence of utterances -lrb- called the linguistic structure -rrb- , a structure of purposes -lrb- called the intentional structure -rrb- , and the state of focus of attention -lrb- called the #attentional state# -rrb- .	4	part-of	part-of	1
the #intentional structure# captures the $discourse-relevant purposes$ , expressed in each of the linguistic segments as well as relationships among them .	0	used-for	used-for	1
the distinction among these components is essential to provide an adequate explanation of such $discourse phenomena$ as #cue phrases# , referring expressions , and interruptions .	3	hyponym-of	hyponym-of	1
the distinction among these components is essential to provide an adequate explanation of such discourse phenomena as #cue phrases# , $referring expressions$ , and interruptions .	1	conjunction	conjunction	1
the distinction among these components is essential to provide an adequate explanation of such $discourse phenomena$ as cue phrases , #referring expressions# , and interruptions .	3	hyponym-of	hyponym-of	1
the distinction among these components is essential to provide an adequate explanation of such discourse phenomena as cue phrases , #referring expressions# , and $interruptions$ .	1	conjunction	conjunction	1
the distinction among these components is essential to provide an adequate explanation of such $discourse phenomena$ as cue phrases , referring expressions , and #interruptions# .	3	hyponym-of	hyponym-of	1
we examine the relationship between the two $grammatical formalisms$ : #tree adjoining grammars# and head grammars .	3	hyponym-of	hyponym-of	1
we examine the relationship between the two grammatical formalisms : #tree adjoining grammars# and $head grammars$ .	6	compare	compare	1
we examine the relationship between the two $grammatical formalisms$ : tree adjoining grammars and #head grammars# .	3	hyponym-of	hyponym-of	1
we then turn to a discussion comparing the #linguistic expressiveness# of the two $formalisms$ .	5	feature-of	feature-of	1
we provide a unified account of $sentence-level and text-level anaphora$ within the framework of a #dependency-based grammar model# .	0	used-for	used-for	1
#criteria# for $anaphora resolution within sentence boundaries$ rephrase major concepts from gb 's binding theory , while those for text-level anaphora incorporate an adapted version of a grosz-sidner-style focus model .	0	used-for	used-for	1
$criteria$ for anaphora resolution within sentence boundaries rephrase major concepts from #gb 's binding theory# , while those for text-level anaphora incorporate an adapted version of a grosz-sidner-style focus model .	0	used-for	used-for	1
criteria for anaphora resolution within sentence boundaries rephrase major concepts from gb 's binding theory , while #those# for $text-level anaphora$ incorporate an adapted version of a grosz-sidner-style focus model .	0	used-for	used-for	1
criteria for anaphora resolution within sentence boundaries rephrase major concepts from gb 's binding theory , while $those$ for text-level anaphora incorporate an adapted version of a #grosz-sidner-style focus model# .	4	part-of	part-of	1
#coedition# of a natural language text and its representation in some interlingual form seems the best and simplest way to share $text revision$ across languages .	0	used-for	used-for	1
$coedition$ of a #natural language text# and its representation in some interlingual form seems the best and simplest way to share text revision across languages .	0	used-for	used-for	1
the modified #graph# is then sent to the $unl-l0 deconverter$ and the result shown .	0	used-for	used-for	1
on the internal side , $liaisons$ are established between elements of the text and the graph by using broadly available #resources# such as a lo-english or better a l0-unl dictionary , a morphosyntactic parser of l0 , and a canonical graph2tree transformation .	0	used-for	used-for	1
on the internal side , liaisons are established between elements of the text and the graph by using broadly available $resources$ such as a #lo-english or better a l0-unl dictionary# , a morphosyntactic parser of l0 , and a canonical graph2tree transformation .	3	hyponym-of	hyponym-of	1
on the internal side , liaisons are established between elements of the text and the graph by using broadly available resources such as a #lo-english or better a l0-unl dictionary# , a $morphosyntactic parser of l0$ , and a canonical graph2tree transformation .	1	conjunction	conjunction	1
on the internal side , liaisons are established between elements of the text and the graph by using broadly available $resources$ such as a lo-english or better a l0-unl dictionary , a #morphosyntactic parser of l0# , and a canonical graph2tree transformation .	3	hyponym-of	hyponym-of	1
on the internal side , liaisons are established between elements of the text and the graph by using broadly available resources such as a lo-english or better a l0-unl dictionary , a #morphosyntactic parser of l0# , and a $canonical graph2tree transformation$ .	1	conjunction	conjunction	1
on the internal side , liaisons are established between elements of the text and the graph by using broadly available $resources$ such as a lo-english or better a l0-unl dictionary , a morphosyntactic parser of l0 , and a #canonical graph2tree transformation# .	3	hyponym-of	hyponym-of	1
establishing a `` best '' correspondence between the '' #unl-tree + l0# '' and the '' $ms-l0 structure$ '' , a lattice , may be done using the dictionary and trying to align the tree and the selected trajectory with as few crossing liaisons as possible .	1	conjunction	conjunction	1
establishing a `` best '' correspondence between the '' unl-tree + l0 '' and the '' ms-l0 structure '' , a $lattice$ , may be done using the #dictionary# and trying to align the tree and the selected trajectory with as few crossing liaisons as possible .	0	used-for	used-for	1
a central goal of this research is to merge approaches from #pivot mt# , $interactive mt$ , and multilingual text authoring .	1	conjunction	conjunction	1
a central goal of this research is to merge approaches from pivot mt , #interactive mt# , and $multilingual text authoring$ .	1	conjunction	conjunction	1
we report experiments conducted on a #multilingual corpus# to estimate the number of $analogies$ among the sentences that it contains .	2	evaluate-for	evaluate-for	1
our goal is to learn a $mahalanobis distance$ by minimizing a #loss# defined on the weighted sum of the precision at different ranks .	0	used-for	used-for	1
our goal is to learn a mahalanobis distance by minimizing a loss defined on the #weighted sum# of the $precision$ at different ranks .	5	feature-of	feature-of	1
our core motivation is that minimizing a #weighted rank loss# is a natural criterion for many problems in $computer vision$ such as person re-identification .	0	used-for	used-for	1
our core motivation is that minimizing a #weighted rank loss# is a natural criterion for many problems in computer vision such as $person re-identification$ .	0	used-for	used-for	1
our core motivation is that minimizing a weighted rank loss is a natural criterion for many problems in $computer vision$ such as #person re-identification# .	3	hyponym-of	hyponym-of	1
we propose a novel $metric learning formulation$ called #weighted approximate rank component analysis -lrb- warca -rrb-# .	3	hyponym-of	hyponym-of	1
we then derive a scalable #stochastic gradient descent algorithm# for the resulting $learning problem$ .	0	used-for	used-for	1
we also derive an efficient $non-linear extension of warca$ by using the #kernel trick# .	0	used-for	used-for	1
#kernel space embedding# decouples the training and prediction costs from the data dimension and enables us to plug $inarbitrary distance measures$ which are more natural for the features .	0	used-for	used-for	1
we also address a more general problem of #matrix rank degeneration# & $non-isolated minima$ in the low-rank matrix optimization by using new type of regularizer which approximately enforces the or-thonormality of the learned matrix very efficiently .	1	conjunction	conjunction	1
we also address a more general problem of #matrix rank degeneration# & non-isolated minima in the $low-rank matrix optimization$ by using new type of regularizer which approximately enforces the or-thonormality of the learned matrix very efficiently .	5	feature-of	feature-of	1
we also address a more general problem of matrix rank degeneration & #non-isolated minima# in the $low-rank matrix optimization$ by using new type of regularizer which approximately enforces the or-thonormality of the learned matrix very efficiently .	5	feature-of	feature-of	1
we also address a more general problem of matrix rank degeneration & non-isolated minima in the $low-rank matrix optimization$ by using new type of #regularizer# which approximately enforces the or-thonormality of the learned matrix very efficiently .	0	used-for	used-for	1
we also address a more general problem of matrix rank degeneration & non-isolated minima in the low-rank matrix optimization by using new type of #regularizer# which approximately enforces the $or-thonormality$ of the learned matrix very efficiently .	0	used-for	used-for	1
we also address a more general problem of matrix rank degeneration & non-isolated minima in the low-rank matrix optimization by using new type of regularizer which approximately enforces the #or-thonormality# of the $learned matrix$ very efficiently .	5	feature-of	feature-of	1
we validate this new $method$ on nine standard #person re-identification datasets# including two large scale market-1501 and cuhk03 datasets and show that we improve upon the current state-of-the-art methods on all of them .	2	evaluate-for	evaluate-for	1
we validate this new method on nine standard $person re-identification datasets$ including two large #scale market-1501# and cuhk03 datasets and show that we improve upon the current state-of-the-art methods on all of them .	3	hyponym-of	hyponym-of	1
we validate this new method on nine standard $person re-identification datasets$ including two large scale market-1501 and #cuhk03 datasets# and show that we improve upon the current state-of-the-art methods on all of them .	3	hyponym-of	hyponym-of	1
we validate this new method on nine standard person re-identification datasets including two large $scale market-1501$ and #cuhk03 datasets# and show that we improve upon the current state-of-the-art methods on all of them .	1	conjunction	conjunction	1
in this paper , we discuss $language model adaptation methods$ given a #word list# and a raw corpus .	0	used-for	used-for	1
in this paper , we discuss language model adaptation methods given a #word list# and a $raw corpus$ .	1	conjunction	conjunction	1
in this paper , we discuss $language model adaptation methods$ given a word list and a #raw corpus# .	0	used-for	used-for	1
in this situation , the general #method# is to segment the $raw corpus$ automatically using a word list , correct the output sentences by hand , and build a model from the segmented corpus .	0	used-for	used-for	1
in this situation , the general $method$ is to segment the raw corpus automatically using a #word list# , correct the output sentences by hand , and build a model from the segmented corpus .	0	used-for	used-for	1
in this situation , the general method is to segment the raw corpus automatically using a word list , correct the output sentences by hand , and build a $model$ from the #segmented corpus# .	0	used-for	used-for	1
in the experiments , we used a variety of #methods# for $preparing a segmented corpus$ and compared the language models by their speech recognition accuracies .	0	used-for	used-for	1
in the experiments , we used a variety of methods for preparing a segmented corpus and compared the $language models$ by their #speech recognition accuracies# .	2	evaluate-for	evaluate-for	1
many practical $modeling problems$ involve #discrete data# that are best represented as draws from multinomial or categorical distributions .	0	used-for	used-for	1
many practical $modeling problems$ involve discrete data that are best represented as draws from #multinomial or categorical distributions# .	0	used-for	used-for	1
for example , $nucleotides in a dna sequence$ , children 's names in a given state and year , and text documents are all commonly modeled with #multinomial distributions# .	0	used-for	used-for	1
for example , nucleotides in a dna sequence , children 's names in a given state and year , and $text documents$ are all commonly modeled with #multinomial distributions# .	0	used-for	used-for	1
here , we leverage a #logistic stick-breaking representation# and recent innovations in pólya-gamma augmentation to reformu-late the $multinomial distribution$ in terms of latent variables with jointly gaussian likelihoods , enabling us to take advantage of a host of bayesian inference techniques for gaussian models with minimal overhead .	0	used-for	used-for	1
here , we leverage a logistic stick-breaking representation and recent innovations in #pólya-gamma augmentation# to reformu-late the $multinomial distribution$ in terms of latent variables with jointly gaussian likelihoods , enabling us to take advantage of a host of bayesian inference techniques for gaussian models with minimal overhead .	0	used-for	used-for	1
here , we leverage a logistic stick-breaking representation and recent innovations in pólya-gamma augmentation to reformu-late the $multinomial distribution$ in terms of #latent variables# with jointly gaussian likelihoods , enabling us to take advantage of a host of bayesian inference techniques for gaussian models with minimal overhead .	4	part-of	part-of	1
here , we leverage a logistic stick-breaking representation and recent innovations in pólya-gamma augmentation to reformu-late the multinomial distribution in terms of $latent variables$ with #jointly gaussian likelihoods# , enabling us to take advantage of a host of bayesian inference techniques for gaussian models with minimal overhead .	5	feature-of	feature-of	1
here , we leverage a logistic stick-breaking representation and recent innovations in pólya-gamma augmentation to reformu-late the multinomial distribution in terms of latent variables with jointly gaussian likelihoods , enabling us to take advantage of a host of #bayesian inference techniques# for $gaussian models$ with minimal overhead .	0	used-for	used-for	1
here , we leverage a logistic stick-breaking representation and recent innovations in pólya-gamma augmentation to reformu-late the multinomial distribution in terms of latent variables with jointly gaussian likelihoods , enabling us to take advantage of a host of bayesian inference techniques for $gaussian models$ with #minimal overhead# .	5	feature-of	feature-of	1
#minpran# , a new $robust operator$ , nds good ts in data sets where more than 50 % of the points are outliers .	3	hyponym-of	hyponym-of	1
unlike other #techniques# that handle $large outlier percentages$ , minpran does not rely on a known error bound for the good data .	0	used-for	used-for	1
unlike other #techniques# that handle large outlier percentages , $minpran$ does not rely on a known error bound for the good data .	6	compare	compare	1
based on this , $minpran$ uses #random sampling# to search for the t and the number of inliers to the t that are least likely to have occurred randomly .	0	used-for	used-for	1
$minpran$ 's properties are connrmed experimentally on #synthetic data# and compare favorably to least median of squares .	2	evaluate-for	evaluate-for	1
$minpran$ 's properties are connrmed experimentally on synthetic data and compare favorably to #least median of squares# .	6	compare	compare	1
related work applies #minpran# to $complex range$ and intensity data 23 -rsb- .	0	used-for	used-for	1
related work applies #minpran# to complex range and $intensity data$ 23 -rsb- .	0	used-for	used-for	1
$metagrammatical formalisms$ that combine #context-free phrase structure rules# and metarules -lrb- mps grammars -rrb- allow concise statement of generalizations about the syntax of natural languages .	4	part-of	part-of	1
metagrammatical formalisms that combine #context-free phrase structure rules# and $metarules -lrb- mps grammars -rrb-$ allow concise statement of generalizations about the syntax of natural languages .	1	conjunction	conjunction	1
$metagrammatical formalisms$ that combine context-free phrase structure rules and #metarules -lrb- mps grammars -rrb-# allow concise statement of generalizations about the syntax of natural languages .	4	part-of	part-of	1
we evaluate several proposals for constraining $them$ , basing our assessment on #computational tractability and explanatory adequacy# .	2	evaluate-for	evaluate-for	1
the unique properties of tree-adjoining grammars -lrb- tag -rrb- present a challenge for the application of #tags# beyond the limited confines of syntax , for instance , to the task of $semantic interpretation$ or automatic translation of natural language .	0	used-for	used-for	1
the unique properties of tree-adjoining grammars -lrb- tag -rrb- present a challenge for the application of #tags# beyond the limited confines of syntax , for instance , to the task of semantic interpretation or $automatic translation of natural language$ .	0	used-for	used-for	1
the unique properties of tree-adjoining grammars -lrb- tag -rrb- present a challenge for the application of tags beyond the limited confines of syntax , for instance , to the task of #semantic interpretation# or $automatic translation of natural language$ .	1	conjunction	conjunction	1
the formalism 's intended usage is to relate expressions of natural languages to their associated $semantics$ represented in a #logical form language# , or to their translates in another natural language ; in summary , we intend it to allow tags to be used beyond their role in syntax proper .	0	used-for	used-for	1
the formalism 's intended usage is to relate expressions of natural languages to their associated semantics represented in a logical form language , or to their translates in another natural language ; in summary , we intend it to allow #tags# to be used beyond their role in $syntax proper$ .	0	used-for	used-for	1
a #model-based approach# to $on-line cursive handwriting analysis and recognition$ is presented and evaluated .	0	used-for	used-for	1
in this #model# , $on-line handwriting$ is considered as a modulation of a simple cycloidal pen motion , described by two coupled oscillations with a constant linear drift along the line of the writing .	0	used-for	used-for	1
in this model , #on-line handwriting# is considered as a modulation of a simple $cycloidal pen motion$ , described by two coupled oscillations with a constant linear drift along the line of the writing .	4	part-of	part-of	1
a general procedure for the estimation and quantization of these #cycloidal motion parameters# for $arbitrary handwriting$ is presented .	0	used-for	used-for	1
the result is a #discrete motor control representation# of the $continuous pen motion$ , via the quantized levels of the model parameters .	0	used-for	used-for	1
this #motor control representation# enables successful $word spotting$ and matching of cursive scripts .	0	used-for	used-for	1
this #motor control representation# enables successful word spotting and $matching of cursive scripts$ .	0	used-for	used-for	1
this motor control representation enables successful #word spotting# and $matching of cursive scripts$ .	1	conjunction	conjunction	1
our experiments clearly indicate the potential of this #dynamic representation# for complete $cursive handwriting recognition$ .	0	used-for	used-for	1
in the $object recognition task$ , there exists a di-chotomy between the #categorization of objects# and estimating object pose , where the former necessitates a view-invariant representation , while the latter requires a representation capable of capturing pose information over different categories of objects .	4	part-of	part-of	1
in the object recognition task , there exists a di-chotomy between the #categorization of objects# and $estimating object pose$ , where the former necessitates a view-invariant representation , while the latter requires a representation capable of capturing pose information over different categories of objects .	1	conjunction	conjunction	1
in the $object recognition task$ , there exists a di-chotomy between the categorization of objects and #estimating object pose# , where the former necessitates a view-invariant representation , while the latter requires a representation capable of capturing pose information over different categories of objects .	4	part-of	part-of	1
in the object recognition task , there exists a di-chotomy between the categorization of objects and estimating object pose , where the $former$ necessitates a #view-invariant representation# , while the latter requires a representation capable of capturing pose information over different categories of objects .	0	used-for	used-for	1
in the object recognition task , there exists a di-chotomy between the categorization of objects and estimating object pose , where the former necessitates a view-invariant representation , while the $latter$ requires a #representation# capable of capturing pose information over different categories of objects .	0	used-for	used-for	1
in the object recognition task , there exists a di-chotomy between the categorization of objects and estimating object pose , where the former necessitates a view-invariant representation , while the latter requires a #representation# capable of capturing $pose information$ over different categories of objects .	0	used-for	used-for	1
with the rise of #deep archi-tectures# , the prime focus has been on $object category recognition$ .	0	used-for	used-for	1
in contrast , $object pose estimation$ using these #approaches# has received relatively less attention .	0	used-for	used-for	1
in this work , we study how #convolutional neural networks -lrb- cnn -rrb- architectures# can be adapted to the task of simultaneous $object recognition$ and pose estimation .	0	used-for	used-for	1
in this work , we study how #convolutional neural networks -lrb- cnn -rrb- architectures# can be adapted to the task of simultaneous object recognition and $pose estimation$ .	0	used-for	used-for	1
in this work , we study how convolutional neural networks -lrb- cnn -rrb- architectures can be adapted to the task of simultaneous #object recognition# and $pose estimation$ .	1	conjunction	conjunction	1
we investigate and analyze the #layers# of various $cnn models$ and extensively compare between them with the goal of discovering how the layers of distributed representations within cnns represent object pose information and how this contradicts with object category representations .	4	part-of	part-of	1
we investigate and analyze the layers of various cnn models and extensively compare between them with the goal of discovering how the #layers of distributed representations# within $cnns$ represent object pose information and how this contradicts with object category representations .	4	part-of	part-of	1
we investigate and analyze the layers of various cnn models and extensively compare between them with the goal of discovering how the #layers of distributed representations# within cnns represent $object pose information$ and how this contradicts with object category representations .	0	used-for	used-for	1
we investigate and analyze the layers of various cnn models and extensively compare between them with the goal of discovering how the layers of distributed representations within cnns represent object pose information and how #this# contradicts with $object category representations$ .	6	compare	compare	1
#it# is particularly valuable to $empirical mt research$ .	0	used-for	used-for	1
in this paper , we explore #geometric structures of 3d lines# in ray space for improving $light field triangulation$ and stereo matching .	0	used-for	used-for	1
in this paper , we explore #geometric structures of 3d lines# in ray space for improving light field triangulation and $stereo matching$ .	0	used-for	used-for	1
in this paper , we explore $geometric structures of 3d lines$ in #ray space# for improving light field triangulation and stereo matching .	5	feature-of	feature-of	1
in this paper , we explore geometric structures of 3d lines in ray space for improving #light field triangulation# and $stereo matching$ .	1	conjunction	conjunction	1
such a #triangulation# provides a $piecewise-linear interpolant$ useful for light field super-resolution .	0	used-for	used-for	1
such a triangulation provides a #piecewise-linear interpolant# useful for $light field super-resolution$ .	0	used-for	used-for	1
experiments on #synthetic and real data# show that both our $triangulation and lagc algorithms$ outperform state-of-the-art solutions in accuracy and visual quality .	2	evaluate-for	evaluate-for	1
experiments on #synthetic and real data# show that both our triangulation and lagc algorithms outperform $state-of-the-art solutions$ in accuracy and visual quality .	2	evaluate-for	evaluate-for	1
experiments on synthetic and real data show that both our #triangulation and lagc algorithms# outperform $state-of-the-art solutions$ in accuracy and visual quality .	6	compare	compare	1
experiments on synthetic and real data show that both our $triangulation and lagc algorithms$ outperform state-of-the-art solutions in #accuracy# and visual quality .	2	evaluate-for	evaluate-for	1
experiments on synthetic and real data show that both our triangulation and lagc algorithms outperform $state-of-the-art solutions$ in #accuracy# and visual quality .	2	evaluate-for	evaluate-for	1
experiments on synthetic and real data show that both our $triangulation and lagc algorithms$ outperform state-of-the-art solutions in accuracy and #visual quality# .	2	evaluate-for	evaluate-for	1
experiments on synthetic and real data show that both our triangulation and lagc algorithms outperform $state-of-the-art solutions$ in accuracy and #visual quality# .	2	evaluate-for	evaluate-for	1
this paper presents a $phrase-based statistical machine translation method$ , based on #non-contiguous phrases# , i.e. phrases with gaps .	0	used-for	used-for	1
a #method# for producing such $phrases$ from a word-aligned corpora is proposed .	0	used-for	used-for	1
a $method$ for producing such phrases from a #word-aligned corpora# is proposed .	2	evaluate-for	evaluate-for	1
a #statistical translation model# is also presented that deals such $phrases$ , as well as a training method based on the maximization of translation accuracy , as measured with the nist evaluation metric .	0	used-for	used-for	1
a statistical translation model is also presented that deals such phrases , as well as a $training method$ based on the #maximization of translation accuracy# , as measured with the nist evaluation metric .	0	used-for	used-for	1
a $statistical translation model$ is also presented that deals such phrases , as well as a training method based on the maximization of translation accuracy , as measured with the #nist evaluation metric# .	2	evaluate-for	evaluate-for	1
$translations$ are produced by means of a #beam-search decoder# .	0	used-for	used-for	1
#glosser# is designed to support $reading and learning$ to read in a foreign language .	0	used-for	used-for	1
there are four #language pairs# currently supported by $glosser$ : english-bulgarian , english-estonian , english-hungarian and french-dutch .	0	used-for	used-for	1
there are four $language pairs$ currently supported by glosser : #english-bulgarian# , english-estonian , english-hungarian and french-dutch .	3	hyponym-of	hyponym-of	1
there are four language pairs currently supported by glosser : #english-bulgarian# , $english-estonian$ , english-hungarian and french-dutch .	1	conjunction	conjunction	1
there are four $language pairs$ currently supported by glosser : english-bulgarian , #english-estonian# , english-hungarian and french-dutch .	3	hyponym-of	hyponym-of	1
there are four language pairs currently supported by glosser : english-bulgarian , #english-estonian# , $english-hungarian$ and french-dutch .	1	conjunction	conjunction	1
there are four $language pairs$ currently supported by glosser : english-bulgarian , english-estonian , #english-hungarian# and french-dutch .	3	hyponym-of	hyponym-of	1
there are four language pairs currently supported by glosser : english-bulgarian , english-estonian , #english-hungarian# and $french-dutch$ .	1	conjunction	conjunction	1
there are four $language pairs$ currently supported by glosser : english-bulgarian , english-estonian , english-hungarian and #french-dutch# .	3	hyponym-of	hyponym-of	1
a demonstration -lrb- in unix -rrb- for applied natural language processing emphasizes #components# put to novel technical uses in $intelligent computer-assisted morphological analysis -lrb- icall -rrb-$ , including disambiguated morphological analysis and lemmatized indexing for an aligned bilingual corpus of word examples .	0	used-for	used-for	1
a demonstration -lrb- in unix -rrb- for applied natural language processing emphasizes $components$ put to novel technical uses in intelligent computer-assisted morphological analysis -lrb- icall -rrb- , including #disambiguated morphological analysis# and lemmatized indexing for an aligned bilingual corpus of word examples .	3	hyponym-of	hyponym-of	1
a demonstration -lrb- in unix -rrb- for applied natural language processing emphasizes components put to novel technical uses in intelligent computer-assisted morphological analysis -lrb- icall -rrb- , including #disambiguated morphological analysis# and $lemmatized indexing$ for an aligned bilingual corpus of word examples .	1	conjunction	conjunction	1
a demonstration -lrb- in unix -rrb- for applied natural language processing emphasizes components put to novel technical uses in intelligent computer-assisted morphological analysis -lrb- icall -rrb- , including #disambiguated morphological analysis# and lemmatized indexing for an $aligned bilingual corpus$ of word examples .	0	used-for	used-for	1
a demonstration -lrb- in unix -rrb- for applied natural language processing emphasizes $components$ put to novel technical uses in intelligent computer-assisted morphological analysis -lrb- icall -rrb- , including disambiguated morphological analysis and #lemmatized indexing# for an aligned bilingual corpus of word examples .	3	hyponym-of	hyponym-of	1
a demonstration -lrb- in unix -rrb- for applied natural language processing emphasizes components put to novel technical uses in intelligent computer-assisted morphological analysis -lrb- icall -rrb- , including disambiguated morphological analysis and #lemmatized indexing# for an $aligned bilingual corpus$ of word examples .	0	used-for	used-for	1
we present a new $part-of-speech tagger$ that demonstrates the following ideas : -lrb- i -rrb- explicit use of both preceding and following #tag contexts# via a dependency network representation , -lrb- ii -rrb- broad use of lexical features , including jointly conditioning on multiple consecutive words , -lrb- iii -rrb- effective use of priors in conditional loglinear models , and -lrb- iv -rrb- fine-grained modeling of unknown word features .	0	used-for	used-for	1
we present a new part-of-speech tagger that demonstrates the following ideas : -lrb- i -rrb- explicit use of both preceding and following $tag contexts$ via a #dependency network representation# , -lrb- ii -rrb- broad use of lexical features , including jointly conditioning on multiple consecutive words , -lrb- iii -rrb- effective use of priors in conditional loglinear models , and -lrb- iv -rrb- fine-grained modeling of unknown word features .	0	used-for	used-for	1
we present a new $part-of-speech tagger$ that demonstrates the following ideas : -lrb- i -rrb- explicit use of both preceding and following tag contexts via a dependency network representation , -lrb- ii -rrb- broad use of #lexical features# , including jointly conditioning on multiple consecutive words , -lrb- iii -rrb- effective use of priors in conditional loglinear models , and -lrb- iv -rrb- fine-grained modeling of unknown word features .	0	used-for	used-for	1
we present a new $part-of-speech tagger$ that demonstrates the following ideas : -lrb- i -rrb- explicit use of both preceding and following tag contexts via a dependency network representation , -lrb- ii -rrb- broad use of lexical features , including jointly conditioning on multiple consecutive words , -lrb- iii -rrb- effective use of #priors in conditional loglinear models# , and -lrb- iv -rrb- fine-grained modeling of unknown word features .	0	used-for	used-for	1
we present a new $part-of-speech tagger$ that demonstrates the following ideas : -lrb- i -rrb- explicit use of both preceding and following tag contexts via a dependency network representation , -lrb- ii -rrb- broad use of lexical features , including jointly conditioning on multiple consecutive words , -lrb- iii -rrb- effective use of priors in conditional loglinear models , and -lrb- iv -rrb- #fine-grained modeling of unknown word features# .	0	used-for	used-for	1
using these ideas together , the resulting $tagger$ gives a 97.24 % #accuracy# on the penn treebank wsj , an error reduction of 4.4 % on the best previous single automatically learned tagging result .	2	evaluate-for	evaluate-for	1
using these ideas together , the resulting $tagger$ gives a 97.24 % accuracy on the #penn treebank wsj# , an error reduction of 4.4 % on the best previous single automatically learned tagging result .	2	evaluate-for	evaluate-for	1
using these ideas together , the resulting $tagger$ gives a 97.24 % accuracy on the penn treebank wsj , an #error# reduction of 4.4 % on the best previous single automatically learned tagging result .	2	evaluate-for	evaluate-for	1
owing to these variations , the $pedestrian data$ is distributed as #highly-curved manifolds# in the feature space , despite the current convolutional neural networks -lrb- cnn -rrb- 's capability of feature extraction .	0	used-for	used-for	1
owing to these variations , the pedestrian data is distributed as $highly-curved manifolds$ in the #feature space# , despite the current convolutional neural networks -lrb- cnn -rrb- 's capability of feature extraction .	5	feature-of	feature-of	1
owing to these variations , the pedestrian data is distributed as highly-curved manifolds in the feature space , despite the current #convolutional neural networks -lrb- cnn -rrb-# 's capability of $feature extraction$ .	0	used-for	used-for	1
in practice , the current $deep embedding methods$ use the #euclidean distance# for the training and test .	0	used-for	used-for	1
on the other hand , the $manifold learning methods$ suggest to use the #euclidean distance# in the local range , combining with the graphical relationship between samples , for approximating the geodesic distance .	0	used-for	used-for	1
on the other hand , the manifold learning methods suggest to use the #euclidean distance# in the local range , combining with the $graphical relationship$ between samples , for approximating the geodesic distance .	1	conjunction	conjunction	1
on the other hand , the manifold learning methods suggest to use the #euclidean distance# in the local range , combining with the graphical relationship between samples , for approximating the $geodesic distance$ .	0	used-for	used-for	1
on the other hand , the manifold learning methods suggest to use the $euclidean distance$ in the #local range# , combining with the graphical relationship between samples , for approximating the geodesic distance .	5	feature-of	feature-of	1
on the other hand , the manifold learning methods suggest to use the euclidean distance in the local range , combining with the #graphical relationship# between samples , for approximating the $geodesic distance$ .	0	used-for	used-for	1
from this point of view , selecting suitable positive -lrb- i.e. intra-class -rrb- training samples within a local range is critical for training the cnn embedding , especially when the $data$ has large #intra-class variations# .	5	feature-of	feature-of	1
in this paper , we propose a novel #moderate positive sample mining method# to train $robust cnn$ for person re-identification , dealing with the problem of large variation .	0	used-for	used-for	1
in this paper , we propose a novel moderate positive sample mining method to train #robust cnn# for $person re-identification$ , dealing with the problem of large variation .	0	used-for	used-for	1
in addition , we improve the $learning$ by a #metric weight constraint# , so that the learned metric has a better generalization ability .	0	used-for	used-for	1
in addition , we improve the learning by a metric weight constraint , so that the $learned metric$ has a better #generalization ability# .	5	feature-of	feature-of	1
experiments show that these two strategies are effective in learning #robust deep metrics# for $person re-identification$ , and accordingly our deep model significantly outperforms the state-of-the-art methods on several benchmarks of person re-identification .	0	used-for	used-for	1
experiments show that these two strategies are effective in learning robust deep metrics for person re-identification , and accordingly our #deep model# significantly outperforms the $state-of-the-art methods$ on several benchmarks of person re-identification .	6	compare	compare	1
experiments show that these two strategies are effective in learning robust deep metrics for person re-identification , and accordingly our #deep model# significantly outperforms the state-of-the-art methods on several benchmarks of $person re-identification$ .	0	used-for	used-for	1
experiments show that these two strategies are effective in learning robust deep metrics for person re-identification , and accordingly our deep model significantly outperforms the #state-of-the-art methods# on several benchmarks of $person re-identification$ .	0	used-for	used-for	1
therefore , the study presented in this paper may be useful in inspiring new designs of #deep models# for $person re-identification$ .	0	used-for	used-for	1
#utterance verification -lrb- uv -rrb-# is a critical function of an $automatic speech recognition -lrb- asr -rrb- system$ working on real applications where spontaneous speech , out-of-vocabulary -lrb- oov -rrb- words and acoustic noises are present .	3	hyponym-of	hyponym-of	1
in this paper we present a new uv procedure with two major features : a -rrb- #confidence tests# are applied to $decoded string hypotheses$ obtained from using word and garbage models that represent oov words and noises .	0	used-for	used-for	1
in this paper we present a new uv procedure with two major features : a -rrb- confidence tests are applied to decoded string hypotheses obtained from using word and garbage models that represent $oov words$ and #noises# .	1	conjunction	conjunction	1
thus the #asr system# is designed to deal with what we refer to as $word spotting$ and noise spotting capabilities .	0	used-for	used-for	1
thus the #asr system# is designed to deal with what we refer to as word spotting and $noise spotting capabilities$ .	0	used-for	used-for	1
b -rrb- the $uv procedure$ is based on three different #confidence tests# , two based on acoustic measures and one founded on linguistic information , applied in a hierarchical structure .	0	used-for	used-for	1
b -rrb- the uv procedure is based on three different #confidence tests# , two based on acoustic measures and one founded on linguistic information , applied in a $hierarchical structure$ .	0	used-for	used-for	1
b -rrb- the uv procedure is based on three different $confidence tests$ , #two# based on acoustic measures and one founded on linguistic information , applied in a hierarchical structure .	3	hyponym-of	hyponym-of	1
b -rrb- the uv procedure is based on three different confidence tests , $two$ based on #acoustic measures# and one founded on linguistic information , applied in a hierarchical structure .	0	used-for	used-for	1
b -rrb- the uv procedure is based on three different $confidence tests$ , two based on acoustic measures and #one# founded on linguistic information , applied in a hierarchical structure .	3	hyponym-of	hyponym-of	1
b -rrb- the uv procedure is based on three different confidence tests , two based on acoustic measures and $one$ founded on #linguistic information# , applied in a hierarchical structure .	0	used-for	used-for	1
experimental results from a real $telephone application$ on a #natural number recognition task# show an 50 % reduction in recognition errors with a moderate 12 % rejection rate of correct utterances and a low 1.5 % rate of false acceptance .	5	feature-of	feature-of	1
experimental results from a real telephone application on a $natural number recognition task$ show an 50 % reduction in #recognition errors# with a moderate 12 % rejection rate of correct utterances and a low 1.5 % rate of false acceptance .	2	evaluate-for	evaluate-for	1
a critical step in #encoding sound# for $neuronal processing$ occurs when the analog pressure wave is coded into discrete nerve-action potentials .	0	used-for	used-for	1
a critical step in encoding sound for neuronal processing occurs when the $analog pressure wave$ is coded into #discrete nerve-action potentials# .	0	used-for	used-for	1
recent #pool models# of the $inner hair cell synapse$ do not reproduce the dead time period after an intense stimulus , so we used visual inspection and automatic speech recognition -lrb- asr -rrb- to investigate an offset adaptation -lrb- oa -rrb- model proposed by zhang et al. -lsb- 1 -rsb- .	0	used-for	used-for	1
recent pool models of the inner hair cell synapse do not reproduce the dead time period after an intense stimulus , so we used #visual inspection# and $automatic speech recognition -lrb- asr -rrb-$ to investigate an offset adaptation -lrb- oa -rrb- model proposed by zhang et al. -lsb- 1 -rsb- .	1	conjunction	conjunction	1
recent pool models of the inner hair cell synapse do not reproduce the dead time period after an intense stimulus , so we used #visual inspection# and automatic speech recognition -lrb- asr -rrb- to investigate an $offset adaptation -lrb- oa -rrb- model$ proposed by zhang et al. -lsb- 1 -rsb- .	0	used-for	used-for	1
recent pool models of the inner hair cell synapse do not reproduce the dead time period after an intense stimulus , so we used visual inspection and #automatic speech recognition -lrb- asr -rrb-# to investigate an $offset adaptation -lrb- oa -rrb- model$ proposed by zhang et al. -lsb- 1 -rsb- .	0	used-for	used-for	1
#oa# improved $phase locking in the auditory nerve -lrb- an -rrb-$ and raised asr accuracy for features derived from an fibers -lrb- anfs -rrb- .	0	used-for	used-for	1
#oa# improved phase locking in the auditory nerve -lrb- an -rrb- and raised asr accuracy for $features$ derived from an fibers -lrb- anfs -rrb- .	0	used-for	used-for	1
oa improved phase locking in the auditory nerve -lrb- an -rrb- and raised #asr accuracy# for $features$ derived from an fibers -lrb- anfs -rrb- .	2	evaluate-for	evaluate-for	1
oa improved phase locking in the auditory nerve -lrb- an -rrb- and raised asr accuracy for $features$ derived from #an fibers -lrb- anfs -rrb-# .	0	used-for	used-for	1
we also found that #oa# is crucial for $auditory processing$ by onset neurons -lrb- ons -rrb- in the next neuronal stage , the auditory brainstem .	0	used-for	used-for	1
we also found that $oa$ is crucial for auditory processing by #onset neurons -lrb- ons -rrb-# in the next neuronal stage , the auditory brainstem .	0	used-for	used-for	1
#multi-layer perceptrons -lrb- mlps -rrb-# performed much better than standard $gaussian mixture models -lrb- gmms -rrb-$ for both our anf-based and on-based auditory features .	6	compare	compare	1
#multi-layer perceptrons -lrb- mlps -rrb-# performed much better than standard gaussian mixture models -lrb- gmms -rrb- for both our $anf-based and on-based auditory features$ .	0	used-for	used-for	1
multi-layer perceptrons -lrb- mlps -rrb- performed much better than standard #gaussian mixture models -lrb- gmms -rrb-# for both our $anf-based and on-based auditory features$ .	0	used-for	used-for	1
recent progress in $computer vision$ has been driven by #high-capacity models# trained on large datasets .	0	used-for	used-for	1
recent progress in computer vision has been driven by $high-capacity models$ trained on #large datasets# .	0	used-for	used-for	1
unfortunately , creating $large datasets$ with #pixel-level labels# has been extremely costly due to the amount of human effort required .	5	feature-of	feature-of	1
in this paper , we present an #approach# to rapidly creating $pixel-accurate semantic label maps$ for images extracted from modern computer games .	0	used-for	used-for	1
in this paper , we present an approach to rapidly creating #pixel-accurate semantic label maps# for $images$ extracted from modern computer games .	0	used-for	used-for	1
in this paper , we present an approach to rapidly creating pixel-accurate semantic label maps for #images# extracted from $modern computer games$ .	4	part-of	part-of	1
we propose a novel step toward the $unsupervised seg-mentation of whole objects$ by combining '' hints '' of #partial scene segmentation# offered by multiple soft , binary mattes .	0	used-for	used-for	1
we propose a novel step toward the unsupervised seg-mentation of whole objects by combining '' hints '' of $partial scene segmentation$ offered by multiple #soft , binary mattes# .	0	used-for	used-for	1
these $mattes$ are implied by a set of #hypothesized object boundary fragments# in the scene .	0	used-for	used-for	1
this reflects #contemporary methods# for $unsupervised object discovery$ from groups of images , and it allows us to define intuitive evaluation met-rics for our sets of segmentations based on the accurate and parsimonious delineation of scene objects .	0	used-for	used-for	1
our proposed $approach$ builds on recent advances in #spectral clustering# , image matting , and boundary detection .	0	used-for	used-for	1
our proposed approach builds on recent advances in #spectral clustering# , $image matting$ , and boundary detection .	1	conjunction	conjunction	1
our proposed $approach$ builds on recent advances in spectral clustering , #image matting# , and boundary detection .	0	used-for	used-for	1
our proposed approach builds on recent advances in spectral clustering , #image matting# , and $boundary detection$ .	1	conjunction	conjunction	1
our proposed $approach$ builds on recent advances in spectral clustering , image matting , and #boundary detection# .	0	used-for	used-for	1
#it# is demonstrated qualitatively and quantitatively on a dataset of scenes and is suitable for current work in $unsupervised object discovery$ without top-down knowledge .	0	used-for	used-for	1
$it$ is demonstrated qualitatively and quantitatively on a #dataset of scenes# and is suitable for current work in unsupervised object discovery without top-down knowledge .	2	evaluate-for	evaluate-for	1
#language resource quality# is crucial in $nlp$ .	5	feature-of	feature-of	1
many of the resources used are derived from data created by human beings out of an $nlp$ context , especially regarding #mt# and reference translations .	3	hyponym-of	hyponym-of	1
many of the resources used are derived from data created by human beings out of an nlp context , especially regarding #mt# and $reference translations$ .	1	conjunction	conjunction	1
many of the resources used are derived from data created by human beings out of an $nlp$ context , especially regarding mt and #reference translations# .	3	hyponym-of	hyponym-of	1
indeed , $automatic evaluations$ need #high-quality data# that allow the comparison of both automatic and human translations .	2	evaluate-for	evaluate-for	1
this paper describes the impact of using #different-quality references# on $evaluation$ .	0	used-for	used-for	1
thus , the limitations of the #automatic metrics# used within $mt$ are also discussed in this regard .	2	evaluate-for	evaluate-for	1
this poster paper describes a #full scale two-level morphological description# -lrb- karttunen , 1983 ; koskenniemi , 1983 -rrb- of $turkish word structures$ .	0	used-for	used-for	1
the $description$ has been implemented using the #pc-kimmo environment# -lrb- antworth , 1990 -rrb- and is based on a root word lexicon of about 23,000 roots words .	0	used-for	used-for	1
the $description$ has been implemented using the pc-kimmo environment -lrb- antworth , 1990 -rrb- and is based on a #root word lexicon# of about 23,000 roots words .	0	used-for	used-for	1
#turkish# is an $agglutinative language$ with word structures formed by productive affixations of derivational and inflectional suffixes to root words .	3	hyponym-of	hyponym-of	1
turkish is an $agglutinative language$ with #word structures# formed by productive affixations of derivational and inflectional suffixes to root words .	5	feature-of	feature-of	1
turkish is an agglutinative language with $word structures$ formed by #productive affixations of derivational and inflectional suffixes# to root words .	4	part-of	part-of	1
the $surface realizations of morphological constructions$ are constrained and modified by a number of #phonetic rules# such as vowel harmony .	0	used-for	used-for	1
the surface realizations of morphological constructions are constrained and modified by a number of $phonetic rules$ such as #vowel harmony# .	3	hyponym-of	hyponym-of	1
this paper deals with the problem of generating the #fundamental frequency -lrb- f0 -rrb- contour of speech# from a text input for $text-to-speech synthesis$ .	0	used-for	used-for	1
this paper deals with the problem of generating the $fundamental frequency -lrb- f0 -rrb- contour of speech$ from a #text input# for text-to-speech synthesis .	0	used-for	used-for	1
we have previously introduced a #statistical model# describing the generating process of $speech f0 contours$ , based on the discrete-time version of the fujisaki model .	0	used-for	used-for	1
we have previously introduced a $statistical model$ describing the generating process of speech f0 contours , based on the discrete-time version of the #fujisaki model# .	0	used-for	used-for	1
one #remarkable feature# of this $model$ is that it has allowed us to derive an efficient algorithm based on powerful statistical methods for estimating the fujisaki-model parameters from raw f0 contours .	5	feature-of	feature-of	1
one remarkable feature of this model is that it has allowed us to derive an efficient #algorithm# based on powerful statistical methods for estimating the $fujisaki-model parameters$ from raw f0 contours .	0	used-for	used-for	1
one remarkable feature of this model is that it has allowed us to derive an efficient $algorithm$ based on powerful #statistical methods# for estimating the fujisaki-model parameters from raw f0 contours .	0	used-for	used-for	1
one remarkable feature of this model is that it has allowed us to derive an efficient algorithm based on powerful statistical methods for estimating the $fujisaki-model parameters$ from #raw f0 contours# .	0	used-for	used-for	1
to associate a sequence of the $fujisaki-model parameters$ with a #text input# based on statistical learning , this paper proposes extending this model to a context-dependent one .	0	used-for	used-for	1
to associate a sequence of the $fujisaki-model parameters$ with a text input based on #statistical learning# , this paper proposes extending this model to a context-dependent one .	0	used-for	used-for	1
we further propose a #parameter training algorithm# for the present $model$ based on a decision tree-based context clustering .	0	used-for	used-for	1
we further propose a $parameter training algorithm$ for the present model based on a #decision tree-based context clustering# .	0	used-for	used-for	1
we introduce a #method# to accelerate the $evaluation of object detection cascades$ with the help of a divide-and-conquer procedure in the space of candidate regions .	0	used-for	used-for	1
we introduce a $method$ to accelerate the evaluation of object detection cascades with the help of a #divide-and-conquer procedure# in the space of candidate regions .	0	used-for	used-for	1
we introduce a method to accelerate the evaluation of object detection cascades with the help of a $divide-and-conquer procedure$ in the #space of candidate regions# .	5	feature-of	feature-of	1
compared to the #exhaustive procedure# that thus far is the state-of-the-art for $cascade evaluation$ , the proposed method requires fewer evaluations of the classifier functions , thereby speeding up the search .	0	used-for	used-for	1
compared to the #exhaustive procedure# that thus far is the state-of-the-art for cascade evaluation , the proposed $method$ requires fewer evaluations of the classifier functions , thereby speeding up the search .	6	compare	compare	1
compared to the exhaustive procedure that thus far is the state-of-the-art for cascade evaluation , the proposed #method# requires fewer evaluations of the classifier functions , thereby speeding up the $search$ .	0	used-for	used-for	1
furthermore , we show how the recently developed efficient #subwindow search -lrb- ess -rrb- procedure# -lsb- 11 -rsb- can be integrated into the last stage of our $method$ .	4	part-of	part-of	1
this allows us to use our #method# to act not only as a faster procedure for $cascade evaluation$ , but also as a tool to perform efficient branch-and-bound object detection with nonlinear quality functions , in particular kernel-ized support vector machines .	0	used-for	used-for	1
this allows us to use our #method# to act not only as a faster procedure for cascade evaluation , but also as a tool to perform efficient $branch-and-bound object detection$ with nonlinear quality functions , in particular kernel-ized support vector machines .	0	used-for	used-for	1
this allows us to use our method to act not only as a faster procedure for cascade evaluation , but also as a tool to perform efficient $branch-and-bound object detection$ with #nonlinear quality functions# , in particular kernel-ized support vector machines .	0	used-for	used-for	1
this allows us to use our method to act not only as a faster procedure for cascade evaluation , but also as a tool to perform efficient branch-and-bound object detection with $nonlinear quality functions$ , in particular #kernel-ized support vector machines# .	3	hyponym-of	hyponym-of	1
experiments on the #pascal voc 2006 dataset# show an acceleration of more than 50 % by our $method$ compared to standard cascade evaluation .	2	evaluate-for	evaluate-for	1
experiments on the #pascal voc 2006 dataset# show an acceleration of more than 50 % by our method compared to standard $cascade evaluation$ .	2	evaluate-for	evaluate-for	1
experiments on the pascal voc 2006 dataset show an acceleration of more than 50 % by our $method$ compared to standard #cascade evaluation# .	6	compare	compare	1
#background modeling# is an important component of many $vision systems$ .	4	part-of	part-of	1
when the $scene$ exhibits a #persistent dynamic behavior# in time , such an assumption is violated and detection performance deteriorates .	5	feature-of	feature-of	1
in this paper , we propose a new #method# for the $modeling and subtraction of such scenes$ .	0	used-for	used-for	1
towards the $modeling of the dynamic characteristics$ , #optical flow# is computed and utilized as a feature in a higher dimensional space .	0	used-for	used-for	1
towards the modeling of the dynamic characteristics , #optical flow# is computed and utilized as a $feature$ in a higher dimensional space .	0	used-for	used-for	1
towards the $modeling of the dynamic characteristics$ , optical flow is computed and utilized as a #feature# in a higher dimensional space .	0	used-for	used-for	1
towards the modeling of the dynamic characteristics , optical flow is computed and utilized as a $feature$ in a #higher dimensional space# .	5	feature-of	feature-of	1
inherent #ambiguities# in the $computation of features$ are addressed by using a data-dependent bandwidth for density estimation using kernels .	5	feature-of	feature-of	1
inherent $ambiguities$ in the computation of features are addressed by using a #data-dependent bandwidth# for density estimation using kernels .	0	used-for	used-for	1
inherent ambiguities in the computation of features are addressed by using a #data-dependent bandwidth# for $density estimation$ using kernels .	0	used-for	used-for	1
inherent ambiguities in the computation of features are addressed by using a data-dependent bandwidth for $density estimation$ using #kernels# .	0	used-for	used-for	1
in this paper , we present our approach for using #information extraction annotations# to augment $document retrieval for distillation$ .	0	used-for	used-for	1
this paper presents a novel #representation# for $three-dimensional objects$ in terms of affine-invariant image patches and their spatial relationships .	0	used-for	used-for	1
this paper presents a novel representation for $three-dimensional objects$ in terms of #affine-invariant image patches# and their spatial relationships .	5	feature-of	feature-of	1
this paper presents a novel representation for three-dimensional objects in terms of $affine-invariant image patches$ and their #spatial relationships# .	5	feature-of	feature-of	1
#multi-view constraints# associated with groups of patches are combined with a $normalized representation$ of their appearance to guide matching and reconstruction , allowing the acquisition of true three-dimensional affine and euclidean models from multiple images and their recognition in a single photograph taken from an arbitrary viewpoint .	1	conjunction	conjunction	1
#multi-view constraints# associated with groups of patches are combined with a normalized representation of their appearance to guide $matching$ and reconstruction , allowing the acquisition of true three-dimensional affine and euclidean models from multiple images and their recognition in a single photograph taken from an arbitrary viewpoint .	0	used-for	used-for	1
#multi-view constraints# associated with groups of patches are combined with a normalized representation of their appearance to guide matching and $reconstruction$ , allowing the acquisition of true three-dimensional affine and euclidean models from multiple images and their recognition in a single photograph taken from an arbitrary viewpoint .	0	used-for	used-for	1
multi-view constraints associated with groups of patches are combined with a #normalized representation# of their appearance to guide $matching$ and reconstruction , allowing the acquisition of true three-dimensional affine and euclidean models from multiple images and their recognition in a single photograph taken from an arbitrary viewpoint .	0	used-for	used-for	1
multi-view constraints associated with groups of patches are combined with a #normalized representation# of their appearance to guide matching and $reconstruction$ , allowing the acquisition of true three-dimensional affine and euclidean models from multiple images and their recognition in a single photograph taken from an arbitrary viewpoint .	0	used-for	used-for	1
multi-view constraints associated with groups of patches are combined with a normalized representation of their appearance to guide #matching# and $reconstruction$ , allowing the acquisition of true three-dimensional affine and euclidean models from multiple images and their recognition in a single photograph taken from an arbitrary viewpoint .	1	conjunction	conjunction	1
multi-view constraints associated with groups of patches are combined with a normalized representation of their appearance to guide matching and reconstruction , allowing the $acquisition of true three-dimensional affine and euclidean models$ from multiple #images# and their recognition in a single photograph taken from an arbitrary viewpoint .	0	used-for	used-for	1
the proposed #approach# does not require a separate segmentation stage and is applicable to $cluttered scenes$ .	0	used-for	used-for	1
#fast algorithms# for $nearest neighbor -lrb- nn -rrb- search$ have in large part focused on 2 distance .	0	used-for	used-for	1
here we develop an #approach# for $1 distance$ that begins with an explicit and exactly distance-preserving embedding of the points into 2 2 .	0	used-for	used-for	1
we show how #this# can efficiently be combined with $random-projection based methods$ for 2 nn search , such as locality-sensitive hashing -lrb- lsh -rrb- or random projection trees .	1	conjunction	conjunction	1
we show how this can efficiently be combined with #random-projection based methods# for 2 $nn search$ , such as locality-sensitive hashing -lrb- lsh -rrb- or random projection trees .	0	used-for	used-for	1
we show how this can efficiently be combined with $random-projection based methods$ for 2 nn search , such as #locality-sensitive hashing -lrb- lsh -rrb-# or random projection trees .	3	hyponym-of	hyponym-of	1
we show how this can efficiently be combined with random-projection based methods for 2 nn search , such as #locality-sensitive hashing -lrb- lsh -rrb-# or $random projection trees$ .	1	conjunction	conjunction	1
we show how this can efficiently be combined with $random-projection based methods$ for 2 nn search , such as locality-sensitive hashing -lrb- lsh -rrb- or #random projection trees# .	3	hyponym-of	hyponym-of	1
we rigorously establish the correctness of the methodology and show by experimentation using lsh that #it# is competitive in practice with available $alternatives$ .	6	compare	compare	1
this paper presents an #algorithm# for $computing optical flow , shape , motion , lighting , and albedo$ from an image sequence of a rigidly-moving lambertian object under distant illumination .	0	used-for	used-for	1
this paper presents an $algorithm$ for computing optical flow , shape , motion , lighting , and albedo from an #image sequence# of a rigidly-moving lambertian object under distant illumination .	0	used-for	used-for	1
this paper presents an algorithm for computing optical flow , shape , motion , lighting , and albedo from an $image sequence$ of a #rigidly-moving lambertian object# under distant illumination .	5	feature-of	feature-of	1
this paper presents an algorithm for computing optical flow , shape , motion , lighting , and albedo from an image sequence of a $rigidly-moving lambertian object$ under #distant illumination# .	5	feature-of	feature-of	1
the problem is formulated in a manner that subsumes structure from #motion# , $multi-view stereo$ , and photo-metric stereo as special cases .	1	conjunction	conjunction	1
the problem is formulated in a manner that subsumes structure from motion , #multi-view stereo# , and $photo-metric stereo$ as special cases .	1	conjunction	conjunction	1
the $algorithm$ utilizes both #spatial and temporal intensity variation# as cues : the former constrains flow and the latter constrains surface orientation ; combining both cues enables dense reconstruction of both textured and texture-less surfaces .	0	used-for	used-for	1
the algorithm utilizes both spatial and temporal intensity variation as $cues$ : the #former# constrains flow and the latter constrains surface orientation ; combining both cues enables dense reconstruction of both textured and texture-less surfaces .	3	hyponym-of	hyponym-of	1
the algorithm utilizes both spatial and temporal intensity variation as cues : the #former# constrains $flow$ and the latter constrains surface orientation ; combining both cues enables dense reconstruction of both textured and texture-less surfaces .	0	used-for	used-for	1
the algorithm utilizes both spatial and temporal intensity variation as cues : the #former# constrains flow and the $latter$ constrains surface orientation ; combining both cues enables dense reconstruction of both textured and texture-less surfaces .	1	conjunction	conjunction	1
the algorithm utilizes both spatial and temporal intensity variation as $cues$ : the former constrains flow and the #latter# constrains surface orientation ; combining both cues enables dense reconstruction of both textured and texture-less surfaces .	3	hyponym-of	hyponym-of	1
the algorithm utilizes both spatial and temporal intensity variation as cues : the former constrains flow and the #latter# constrains $surface orientation$ ; combining both cues enables dense reconstruction of both textured and texture-less surfaces .	0	used-for	used-for	1
the algorithm utilizes both spatial and temporal intensity variation as cues : the former constrains flow and the latter constrains surface orientation ; combining both #cues# enables $dense reconstruction of both textured and texture-less surfaces$ .	0	used-for	used-for	1
the $algorithm$ works by iteratively #estimating affine camera parameters , illumination , shape , and albedo# in an alternating fashion .	0	used-for	used-for	1
an #entity-oriented approach# to $restricted-domain parsing$ is proposed .	0	used-for	used-for	1
like semantic grammar , #this# allows easy exploitation of $limited domain semantics$ .	0	used-for	used-for	1
in addition , #it# facilitates $fragmentary recognition$ and the use of multiple parsing strategies , and so is particularly useful for robust recognition of extra-grammatical input .	0	used-for	used-for	1
in addition , #it# facilitates fragmentary recognition and the use of $multiple parsing strategies$ , and so is particularly useful for robust recognition of extra-grammatical input .	0	used-for	used-for	1
in addition , it facilitates fragmentary recognition and the use of #multiple parsing strategies# , and so is particularly useful for robust $recognition of extra-grammatical input$ .	0	used-for	used-for	1
representative samples from an entity-oriented language definition are presented , along with a #control structure# for an $entity-oriented parser$ , some parsing strategies that use the control structure , and worked examples of parses .	0	used-for	used-for	1
representative samples from an entity-oriented language definition are presented , along with a control structure for an entity-oriented parser , some $parsing strategies$ that use the #control structure# , and worked examples of parses .	0	used-for	used-for	1
a $parser$ incorporating the #control structure# and the parsing strategies is currently under implementation .	4	part-of	part-of	1
this paper summarizes the formalism of category cooccurrence restrictions -lrb- ccrs -rrb- and describes two #parsing algorithms# that interpret $it$ .	0	used-for	used-for	1
the use of ccrs leads to $syntactic descriptions$ formulated entirely with #restrictive statements# .	5	feature-of	feature-of	1
the paper shows how conventional #algorithms# for the analysis of context free languages can be adapted to the $ccr formalism$ .	0	used-for	used-for	1
the paper shows how conventional $algorithms$ for the analysis of #context free languages# can be adapted to the ccr formalism .	0	used-for	used-for	1
special attention is given to the part of the parser that checks the fulfillment of #logical well-formedness conditions# on $trees$ .	5	feature-of	feature-of	1
we present a #text mining method# for finding $synonymous expressions$ based on the distributional hypothesis in a set of coherent corpora .	0	used-for	used-for	1
we present a $text mining method$ for finding synonymous expressions based on the #distributional hypothesis# in a set of coherent corpora .	0	used-for	used-for	1
this paper proposes a new methodology to improve the #accuracy# of a $term aggregation system$ using each author 's text as a coherent corpus .	2	evaluate-for	evaluate-for	1
this paper proposes a new $methodology$ to improve the accuracy of a #term aggregation system# using each author 's text as a coherent corpus .	2	evaluate-for	evaluate-for	1
our proposed method improves the #accuracy# of our $term aggregation system$ , showing that our approach is successful .	2	evaluate-for	evaluate-for	1
our proposed $method$ improves the accuracy of our #term aggregation system# , showing that our approach is successful .	2	evaluate-for	evaluate-for	1
in this work , we present a #technique# for $robust estimation$ , which by explicitly incorporating the inherent uncertainty of the estimation procedure , results in a more efficient robust estimation algorithm .	0	used-for	used-for	1
in this work , we present a #technique# for robust estimation , which by explicitly incorporating the inherent uncertainty of the estimation procedure , results in a more $efficient robust estimation algorithm$ .	0	used-for	used-for	1
in this work , we present a $technique$ for robust estimation , which by explicitly incorporating the #inherent uncertainty of the estimation procedure# , results in a more efficient robust estimation algorithm .	0	used-for	used-for	1
the combination of these two #strategies# results in a $robust estimation procedure$ that provides a significant speed-up over existing ransac techniques , while requiring no prior information to guide the sampling process .	0	used-for	used-for	1
the combination of these two strategies results in a $robust estimation procedure$ that provides a significant speed-up over existing #ransac techniques# , while requiring no prior information to guide the sampling process .	6	compare	compare	1
in particular , our #algorithm# requires , on average , 3-10 times fewer samples than standard $ransac$ , which is in close agreement with theoretical predictions .	6	compare	compare	1
the efficiency of the $algorithm$ is demonstrated on a selection of #geometric estimation problems# .	2	evaluate-for	evaluate-for	1
an attempt has been made to use an #augmented transition network# as a procedural $dialog model$ .	3	hyponym-of	hyponym-of	1
the development of such a model appears to be important in several respects : as a $device$ to represent and to use different #dialog schemata# proposed in empirical conversation analysis ; as a device to represent and to use models of verbal interaction ; as a device combining knowledge about dialog schemata and about verbal interaction with knowledge about task-oriented and goal-directed dialogs .	0	used-for	used-for	1
the development of such a model appears to be important in several respects : as a device to represent and to use different #dialog schemata# proposed in empirical $conversation analysis$ ; as a device to represent and to use models of verbal interaction ; as a device combining knowledge about dialog schemata and about verbal interaction with knowledge about task-oriented and goal-directed dialogs .	0	used-for	used-for	1
the development of such a model appears to be important in several respects : as a device to represent and to use different dialog schemata proposed in empirical conversation analysis ; as a $device$ to represent and to use #models# of verbal interaction ; as a device combining knowledge about dialog schemata and about verbal interaction with knowledge about task-oriented and goal-directed dialogs .	0	used-for	used-for	1
the development of such a model appears to be important in several respects : as a device to represent and to use different dialog schemata proposed in empirical conversation analysis ; as a device to represent and to use #models# of $verbal interaction$ ; as a device combining knowledge about dialog schemata and about verbal interaction with knowledge about task-oriented and goal-directed dialogs .	0	used-for	used-for	1
the development of such a model appears to be important in several respects : as a device to represent and to use different dialog schemata proposed in empirical conversation analysis ; as a device to represent and to use models of verbal interaction ; as a device combining knowledge about #dialog schemata# and about $verbal interaction$ with knowledge about task-oriented and goal-directed dialogs .	1	conjunction	conjunction	1
a standard #atn# should be further developed in order to account for the $verbal interactions$ of task-oriented dialogs .	0	used-for	used-for	1
a standard atn should be further developed in order to account for the #verbal interactions# of $task-oriented dialogs$ .	5	feature-of	feature-of	1
we present a practically #unsupervised learning method# to produce $single-snippet answers$ to definition questions in question answering systems that supplement web search engines .	0	used-for	used-for	1
we present a practically unsupervised learning method to produce single-snippet answers to definition questions in #question answering systems# that supplement $web search engines$ .	0	used-for	used-for	1
the #method# exploits $on-line encyclopedias and dictionaries$ to generate automatically an arbitrarily large number of positive and negative definition examples , which are then used to train an svm to separate the two classes .	0	used-for	used-for	1
the method exploits #on-line encyclopedias and dictionaries# to generate automatically an arbitrarily large number of $positive and negative definition examples$ , which are then used to train an svm to separate the two classes .	0	used-for	used-for	1
the method exploits on-line encyclopedias and dictionaries to generate automatically an arbitrarily large number of #positive and negative definition examples# , which are then used to train an $svm$ to separate the two classes .	0	used-for	used-for	1
we show experimentally that the proposed method is viable , that #it# outperforms the $alternative$ of training the system on questions and news articles from trec , and that it helps the search engine handle definition questions significantly better .	6	compare	compare	1
we show experimentally that the proposed method is viable , that it outperforms the alternative of training the $system$ on questions and #news articles# from trec , and that it helps the search engine handle definition questions significantly better .	0	used-for	used-for	1
we show experimentally that the proposed method is viable , that it outperforms the alternative of training the system on questions and #news articles# from $trec$ , and that it helps the search engine handle definition questions significantly better .	4	part-of	part-of	1
we show experimentally that the proposed method is viable , that it outperforms the alternative of training the system on questions and news articles from trec , and that #it# helps the $search engine$ handle definition questions significantly better .	0	used-for	used-for	1
we revisit the $classical decision-theoretic problem of weighted expert voting$ from a #statistical learning perspective# .	0	used-for	used-for	1
in the case of known expert competence levels , we give #sharp error estimates# for the $optimal rule$ .	0	used-for	used-for	1
we analyze a #reweighted version of the kikuchi approximation# for estimating the $log partition function of a product distribution$ defined over a region graph .	0	used-for	used-for	1
we analyze a reweighted version of the kikuchi approximation for estimating the #log partition function of a product distribution# defined over a $region graph$ .	5	feature-of	feature-of	1
we establish sufficient conditions for the #concavity# of our $reweighted objective function$ in terms of weight assignments in the kikuchi expansion , and show that a reweighted version of the sum product algorithm applied to the kikuchi region graph will produce global optima of the kikuchi approximation whenever the algorithm converges .	5	feature-of	feature-of	1
we establish sufficient conditions for the concavity of our reweighted objective function in terms of weight assignments in the kikuchi expansion , and show that a #reweighted version of the sum product algorithm# applied to the $kikuchi region graph$ will produce global optima of the kikuchi approximation whenever the algorithm converges .	0	used-for	used-for	1
we establish sufficient conditions for the concavity of our reweighted objective function in terms of weight assignments in the kikuchi expansion , and show that a reweighted version of the sum product algorithm applied to the kikuchi region graph will produce #global optima# of the $kikuchi approximation$ whenever the algorithm converges .	5	feature-of	feature-of	1
finally , we provide an explicit characterization of the polytope of concavity in terms of the #cycle structure# of the $region graph$ .	5	feature-of	feature-of	1
we apply a #decision tree based approach# to $pronoun resolution$ in spoken dialogue .	0	used-for	used-for	1
we apply a decision tree based approach to #pronoun resolution# in $spoken dialogue$ .	0	used-for	used-for	1
our #system# deals with $pronouns$ with np - and non-np-antecedents .	0	used-for	used-for	1
our system deals with $pronouns$ with #np - and non-np-antecedents# .	0	used-for	used-for	1
we present a set of #features# designed for $pronoun resolution$ in spoken dialogue and determine the most promising features .	0	used-for	used-for	1
we present a set of features designed for #pronoun resolution# in $spoken dialogue$ and determine the most promising features .	0	used-for	used-for	1
we evaluate the $system$ on twenty #switchboard dialogues# and show that it compares well to byron 's -lrb- 2002 -rrb- manually tuned system .	2	evaluate-for	evaluate-for	1
we evaluate the system on twenty switchboard dialogues and show that #it# compares well to $byron 's -lrb- 2002 -rrb- manually tuned system$ .	6	compare	compare	1
we present a new #approach# for building an efficient and robust $classifier$ for the two class problem , that localizes objects that may appear in the image under different orien-tations .	0	used-for	used-for	1
we present a new approach for building an efficient and robust #classifier# for the two $class problem$ , that localizes objects that may appear in the image under different orien-tations .	0	used-for	used-for	1
in contrast to other works that address this problem using multiple classifiers , each one specialized for a specific orientation , we propose a simple two-step $approach$ with an #estimation stage# and a classification stage .	4	part-of	part-of	1
in contrast to other works that address this problem using multiple classifiers , each one specialized for a specific orientation , we propose a simple two-step approach with an #estimation stage# and a $classification stage$ .	1	conjunction	conjunction	1
in contrast to other works that address this problem using multiple classifiers , each one specialized for a specific orientation , we propose a simple two-step $approach$ with an estimation stage and a #classification stage# .	4	part-of	part-of	1
the estimator yields an initial set of potential $object poses$ that are then validated by the #classifier# .	0	used-for	used-for	1
this methodology allows reducing the #time complexity# of the $algorithm$ while classification results remain high .	2	evaluate-for	evaluate-for	1
the $classifier$ we use in both stages is based on a #boosted combination of random ferns# over local histograms of oriented gradients -lrb- hogs -rrb- , which we compute during a pre-processing step .	0	used-for	used-for	1
the classifier we use in both stages is based on a $boosted combination of random ferns$ over #local histograms of oriented gradients -lrb- hogs -rrb-# , which we compute during a pre-processing step .	5	feature-of	feature-of	1
the classifier we use in both stages is based on a boosted combination of random ferns over $local histograms of oriented gradients -lrb- hogs -rrb-$ , which we compute during a #pre-processing step# .	0	used-for	used-for	1
both the use of #supervised learning# and working on the gradient space makes our $approach$ robust while being efficient at run-time .	0	used-for	used-for	1
both the use of supervised learning and working on the #gradient space# makes our $approach$ robust while being efficient at run-time .	0	used-for	used-for	1
we show these properties by thorough testing on standard databases and on a new $database$ made of #motorbikes under planar rotations# , and with challenging conditions such as cluttered backgrounds , changing illumination conditions and partial occlusions .	5	feature-of	feature-of	1
we show these properties by thorough testing on standard databases and on a new $database$ made of motorbikes under planar rotations , and with challenging #conditions# such as cluttered backgrounds , changing illumination conditions and partial occlusions .	5	feature-of	feature-of	1
we show these properties by thorough testing on standard databases and on a new database made of motorbikes under planar rotations , and with challenging $conditions$ such as #cluttered backgrounds# , changing illumination conditions and partial occlusions .	3	hyponym-of	hyponym-of	1
we show these properties by thorough testing on standard databases and on a new database made of motorbikes under planar rotations , and with challenging conditions such as #cluttered backgrounds# , $changing illumination conditions$ and partial occlusions .	1	conjunction	conjunction	1
we show these properties by thorough testing on standard databases and on a new database made of motorbikes under planar rotations , and with challenging $conditions$ such as cluttered backgrounds , #changing illumination conditions# and partial occlusions .	3	hyponym-of	hyponym-of	1
we show these properties by thorough testing on standard databases and on a new database made of motorbikes under planar rotations , and with challenging conditions such as cluttered backgrounds , #changing illumination conditions# and $partial occlusions$ .	1	conjunction	conjunction	1
we show these properties by thorough testing on standard databases and on a new database made of motorbikes under planar rotations , and with challenging $conditions$ such as cluttered backgrounds , changing illumination conditions and #partial occlusions# .	3	hyponym-of	hyponym-of	1
a very simple improved #duration model# has reduced the error rate by about 10 % in both $triphone and semiphone systems$ .	0	used-for	used-for	1
a very simple improved duration model has reduced the #error rate# by about 10 % in both $triphone and semiphone systems$ .	2	evaluate-for	evaluate-for	1
a new $training strategy$ has been tested which , by itself , did not provide useful improvements but suggests that improvements can be obtained by a related #rapid adaptation technique# .	0	used-for	used-for	1
finally , the $recognizer$ has been modified to use #bigram back-off language models# .	0	used-for	used-for	1
the #system# was then transferred from the $rm task$ to the atis csr task and a limited number of development tests performed .	0	used-for	used-for	1
the #system# was then transferred from the rm task to the $atis csr task$ and a limited number of development tests performed .	0	used-for	used-for	1
the system was then transferred from the #rm task# to the $atis csr task$ and a limited number of development tests performed .	1	conjunction	conjunction	1
a new #approach# for $interactive machine translation$ where the author interacts during the creation or the modification of the document is proposed .	0	used-for	used-for	1
this paper presents a new $interactive disambiguation scheme$ based on the #paraphrasing# of a parser 's multiple output .	0	used-for	used-for	1
we describe a novel #approach# to $statistical machine translation$ that combines syntactic information in the source language with recent advances in phrasal translation .	0	used-for	used-for	1
we describe a novel $approach$ to statistical machine translation that combines #syntactic information# in the source language with recent advances in phrasal translation .	4	part-of	part-of	1
we describe a novel approach to statistical machine translation that combines #syntactic information# in the source language with recent advances in $phrasal translation$ .	1	conjunction	conjunction	1
we describe a novel $approach$ to statistical machine translation that combines syntactic information in the source language with recent advances in #phrasal translation# .	4	part-of	part-of	1
this $method$ requires a #source-language dependency parser# , target language word segmentation and an unsupervised word alignment component .	0	used-for	used-for	1
this method requires a #source-language dependency parser# , $target language word segmentation$ and an unsupervised word alignment component .	1	conjunction	conjunction	1
this $method$ requires a source-language dependency parser , #target language word segmentation# and an unsupervised word alignment component .	0	used-for	used-for	1
this method requires a source-language dependency parser , #target language word segmentation# and an $unsupervised word alignment component$ .	1	conjunction	conjunction	1
this $method$ requires a source-language dependency parser , target language word segmentation and an #unsupervised word alignment component# .	0	used-for	used-for	1
we describe an efficient decoder and show that using these #tree-based models# in combination with conventional $smt models$ provides a promising approach that incorporates the power of phrasal smt with the linguistic generality available in a parser .	1	conjunction	conjunction	1
we describe an efficient decoder and show that using these #tree-based models# in combination with conventional smt models provides a promising $approach$ that incorporates the power of phrasal smt with the linguistic generality available in a parser .	0	used-for	used-for	1
we describe an efficient decoder and show that using these tree-based models in combination with conventional #smt models# provides a promising $approach$ that incorporates the power of phrasal smt with the linguistic generality available in a parser .	0	used-for	used-for	1
we describe an efficient decoder and show that using these tree-based models in combination with conventional smt models provides a promising approach that incorporates the power of #phrasal smt# with the $linguistic generality$ available in a parser .	1	conjunction	conjunction	1
we describe an efficient decoder and show that using these tree-based models in combination with conventional smt models provides a promising approach that incorporates the power of #phrasal smt# with the linguistic generality available in a $parser$ .	0	used-for	used-for	1
we describe an efficient decoder and show that using these tree-based models in combination with conventional smt models provides a promising approach that incorporates the power of phrasal smt with the #linguistic generality# available in a $parser$ .	5	feature-of	feature-of	1
$video$ provides not only rich #visual cues# such as motion and appearance , but also much less explored long-range temporal interactions among objects .	5	feature-of	feature-of	1
video provides not only rich $visual cues$ such as #motion# and appearance , but also much less explored long-range temporal interactions among objects .	3	hyponym-of	hyponym-of	1
video provides not only rich visual cues such as #motion# and $appearance$ , but also much less explored long-range temporal interactions among objects .	1	conjunction	conjunction	1
video provides not only rich $visual cues$ such as motion and #appearance# , but also much less explored long-range temporal interactions among objects .	3	hyponym-of	hyponym-of	1
we aim to capture such interactions and to construct a powerful #intermediate-level video representation# for subsequent $recognition$ .	0	used-for	used-for	1
first , we develop an efficient $spatio-temporal video segmentation algorithm$ , which naturally incorporates #long-range motion cues# from the past and future frames in the form of clusters of point tracks with coherent motion .	0	used-for	used-for	1
first , we develop an efficient spatio-temporal video segmentation algorithm , which naturally incorporates $long-range motion cues$ from the past and future frames in the form of #clusters of point tracks# with coherent motion .	0	used-for	used-for	1
second , we devise a new $track clustering cost function$ that includes #occlusion reasoning# , in the form of depth ordering constraints , as well as motion similarity along the tracks .	4	part-of	part-of	1
second , we devise a new track clustering cost function that includes $occlusion reasoning$ , in the form of #depth ordering constraints# , as well as motion similarity along the tracks .	5	feature-of	feature-of	1
second , we devise a new $track clustering cost function$ that includes occlusion reasoning , in the form of depth ordering constraints , as well as #motion similarity# along the tracks .	4	part-of	part-of	1
we evaluate the proposed $approach$ on a challenging set of #video sequences of office scenes# from feature length movies .	2	evaluate-for	evaluate-for	1
in this paper , we introduce #kaze features# , a novel $multiscale 2d feature detection and description algorithm$ in nonlinear scale spaces .	3	hyponym-of	hyponym-of	1
in this paper , we introduce kaze features , a novel $multiscale 2d feature detection and description algorithm$ in #nonlinear scale spaces# .	5	feature-of	feature-of	1
in contrast , we detect and describe $2d features$ in a #nonlinear scale space# by means of nonlinear diffusion filtering .	5	feature-of	feature-of	1
in contrast , we detect and describe $2d features$ in a nonlinear scale space by means of #nonlinear diffusion filtering# .	0	used-for	used-for	1
the $nonlinear scale space$ is built using efficient #additive operator splitting -lrb- aos -rrb- techniques# and variable con-ductance diffusion .	0	used-for	used-for	1
the nonlinear scale space is built using efficient #additive operator splitting -lrb- aos -rrb- techniques# and $variable con-ductance diffusion$ .	1	conjunction	conjunction	1
the $nonlinear scale space$ is built using efficient additive operator splitting -lrb- aos -rrb- techniques and #variable con-ductance diffusion# .	0	used-for	used-for	1
even though our #features# are somewhat more expensive to compute than $surf$ due to the construction of the nonlinear scale space , but comparable to sift , our results reveal a step forward in performance both in detection and description against previous state-of-the-art methods .	6	compare	compare	1
even though our #features# are somewhat more expensive to compute than surf due to the construction of the nonlinear scale space , but comparable to $sift$ , our results reveal a step forward in performance both in detection and description against previous state-of-the-art methods .	6	compare	compare	1
even though our features are somewhat more expensive to compute than surf due to the construction of the nonlinear scale space , but comparable to sift , our #results# reveal a step forward in performance both in detection and description against previous $state-of-the-art methods$ .	6	compare	compare	1
even though our features are somewhat more expensive to compute than surf due to the construction of the nonlinear scale space , but comparable to sift , our $results$ reveal a step forward in performance both in #detection# and description against previous state-of-the-art methods .	2	evaluate-for	evaluate-for	1
even though our features are somewhat more expensive to compute than surf due to the construction of the nonlinear scale space , but comparable to sift , our results reveal a step forward in performance both in #detection# and $description$ against previous state-of-the-art methods .	1	conjunction	conjunction	1
even though our features are somewhat more expensive to compute than surf due to the construction of the nonlinear scale space , but comparable to sift , our results reveal a step forward in performance both in #detection# and description against previous $state-of-the-art methods$ .	2	evaluate-for	evaluate-for	1
even though our features are somewhat more expensive to compute than surf due to the construction of the nonlinear scale space , but comparable to sift , our $results$ reveal a step forward in performance both in detection and #description# against previous state-of-the-art methods .	2	evaluate-for	evaluate-for	1
even though our features are somewhat more expensive to compute than surf due to the construction of the nonlinear scale space , but comparable to sift , our results reveal a step forward in performance both in detection and #description# against previous $state-of-the-art methods$ .	2	evaluate-for	evaluate-for	1
#creating summaries# on lengthy semantic web documents for quick $identification of the corresponding entity$ has been of great contemporary interest .	0	used-for	used-for	1
$creating summaries$ on #lengthy semantic web documents# for quick identification of the corresponding entity has been of great contemporary interest .	0	used-for	used-for	1
specifically , we highlight the importance of $diversified -lrb- faceted -rrb- summaries$ by combining three dimensions : #diversity# , uniqueness , and popularity .	5	feature-of	feature-of	1
specifically , we highlight the importance of diversified -lrb- faceted -rrb- summaries by combining three dimensions : #diversity# , $uniqueness$ , and popularity .	1	conjunction	conjunction	1
specifically , we highlight the importance of $diversified -lrb- faceted -rrb- summaries$ by combining three dimensions : diversity , #uniqueness# , and popularity .	5	feature-of	feature-of	1
specifically , we highlight the importance of diversified -lrb- faceted -rrb- summaries by combining three dimensions : diversity , #uniqueness# , and $popularity$ .	1	conjunction	conjunction	1
specifically , we highlight the importance of $diversified -lrb- faceted -rrb- summaries$ by combining three dimensions : diversity , uniqueness , and #popularity# .	5	feature-of	feature-of	1
our novel $diversity-aware entity summarization approach$ mimics #human conceptual clustering techniques# to group facts , and picks representative facts from each group to form concise -lrb- i.e. , short -rrb- and comprehensive -lrb- i.e. , improved coverage through diversity -rrb- summaries .	0	used-for	used-for	1
we evaluate our #approach# against the state-of-the-art techniques and show that our work improves both the quality and the efficiency of $entity summarization$ .	0	used-for	used-for	1
we evaluate our $approach$ against the #state-of-the-art techniques# and show that our work improves both the quality and the efficiency of entity summarization .	6	compare	compare	1
we evaluate our approach against the #state-of-the-art techniques# and show that our work improves both the quality and the efficiency of $entity summarization$ .	0	used-for	used-for	1
we evaluate our approach against the state-of-the-art techniques and show that our work improves both the #quality# and the efficiency of $entity summarization$ .	2	evaluate-for	evaluate-for	1
we evaluate our approach against the state-of-the-art techniques and show that our work improves both the quality and the #efficiency# of $entity summarization$ .	2	evaluate-for	evaluate-for	1
we present a #framework# for the $fast computation of lexical affinity models$ .	0	used-for	used-for	1
the $framework$ is composed of a novel #algorithm# to efficiently compute the co-occurrence distribution between pairs of terms , an independence model , and a parametric affinity model .	4	part-of	part-of	1
the framework is composed of a novel #algorithm# to efficiently compute the $co-occurrence distribution$ between pairs of terms , an independence model , and a parametric affinity model .	0	used-for	used-for	1
the framework is composed of a novel #algorithm# to efficiently compute the co-occurrence distribution between pairs of terms , an $independence model$ , and a parametric affinity model .	1	conjunction	conjunction	1
the $framework$ is composed of a novel algorithm to efficiently compute the co-occurrence distribution between pairs of terms , an #independence model# , and a parametric affinity model .	4	part-of	part-of	1
the framework is composed of a novel algorithm to efficiently compute the co-occurrence distribution between pairs of terms , an #independence model# , and a $parametric affinity model$ .	1	conjunction	conjunction	1
the $framework$ is composed of a novel algorithm to efficiently compute the co-occurrence distribution between pairs of terms , an independence model , and a #parametric affinity model# .	4	part-of	part-of	1
in comparison with previous models , which either use arbitrary windows to compute similarity between words or use #lexical affinity# to create $sequential models$ , in this paper we focus on models intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus .	0	used-for	used-for	1
in comparison with previous $models$ , which either use arbitrary windows to compute similarity between words or use lexical affinity to create sequential models , in this paper we focus on #models# intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus .	6	compare	compare	1
in comparison with previous models , which either use arbitrary windows to compute similarity between words or use lexical affinity to create sequential models , in this paper we focus on #models# intended to capture the $co-occurrence patterns$ of any pair of words or phrases at any distance in the corpus .	0	used-for	used-for	1
we apply #it# in combination with a terabyte corpus to answer $natural language tests$ , achieving encouraging results .	0	used-for	used-for	1
we apply $it$ in combination with a #terabyte corpus# to answer natural language tests , achieving encouraging results .	2	evaluate-for	evaluate-for	1
this paper introduces a #system# for $categorizing unknown words$ .	0	used-for	used-for	1
the $system$ is based on a #multi-component architecture# where each component is responsible for identifying one class of unknown words .	0	used-for	used-for	1
the system is based on a $multi-component architecture$ where each #component# is responsible for identifying one class of unknown words .	4	part-of	part-of	1
the system is based on a multi-component architecture where each #component# is responsible for identifying one class of $unknown words$ .	0	used-for	used-for	1
the focus of this paper is the #components# that identify $names$ and spelling errors .	0	used-for	used-for	1
the focus of this paper is the #components# that identify names and $spelling errors$ .	0	used-for	used-for	1
the focus of this paper is the components that identify #names# and $spelling errors$ .	1	conjunction	conjunction	1
each $component$ uses a #decision tree architecture# to combine multiple types of evidence about the unknown word .	0	used-for	used-for	1
the $system$ is evaluated using data from #live closed captions# - a genre replete with a wide variety of unknown words .	2	evaluate-for	evaluate-for	1
at mit lincoln laboratory , we have been developing a $korean-to-english machine translation system$ #cclinc -lrb- common coalition language system at lincoln laboratory -rrb-# .	3	hyponym-of	hyponym-of	1
the $cclinc korean-to-english translation system$ consists of two #core modules# , language understanding and generation modules mediated by a language neutral meaning representation called a semantic frame .	4	part-of	part-of	1
the cclinc korean-to-english translation system consists of two core modules , $language understanding and generation modules$ mediated by a #language neutral meaning representation# called a semantic frame .	0	used-for	used-for	1
the cclinc korean-to-english translation system consists of two core modules , language understanding and generation modules mediated by a $language neutral meaning representation$ called a #semantic frame# .	3	hyponym-of	hyponym-of	1
the key features of the system include : -lrb- i -rrb- robust efficient parsing of #korean# -lrb- a $verb final language$ with overt case markers , relatively free word order , and frequent omissions of arguments -rrb- .	3	hyponym-of	hyponym-of	1
the key features of the system include : -lrb- i -rrb- robust efficient parsing of korean -lrb- a $verb final language$ with #overt case markers# , relatively free word order , and frequent omissions of arguments -rrb- .	5	feature-of	feature-of	1
-lrb- ii -rrb- high quality $translation$ via #word sense disambiguation# and accurate word order generation of the target language .	0	used-for	used-for	1
-lrb- ii -rrb- high quality translation via #word sense disambiguation# and accurate $word order generation$ of the target language .	1	conjunction	conjunction	1
-lrb- ii -rrb- high quality $translation$ via word sense disambiguation and accurate #word order generation# of the target language .	0	used-for	used-for	1
having been trained on #korean newspaper articles# on missiles and chemical biological warfare , the $system$ produces the translation output sufficient for content understanding of the original document .	0	used-for	used-for	1
having been trained on $korean newspaper articles$ on #missiles and chemical biological warfare# , the system produces the translation output sufficient for content understanding of the original document .	5	feature-of	feature-of	1
the #javelin system# integrates a flexible , planning-based architecture with a variety of language processing modules to provide an $open-domain question answering capability$ on free text .	0	used-for	used-for	1
the $javelin system$ integrates a flexible , #planning-based architecture# with a variety of language processing modules to provide an open-domain question answering capability on free text .	4	part-of	part-of	1
the $javelin system$ integrates a flexible , planning-based architecture with a variety of #language processing modules# to provide an open-domain question answering capability on free text .	4	part-of	part-of	1
the javelin system integrates a flexible , $planning-based architecture$ with a variety of #language processing modules# to provide an open-domain question answering capability on free text .	1	conjunction	conjunction	1
we present the first application of the #head-driven statistical parsing model# of collins -lrb- 1999 -rrb- as a $simultaneous language model$ and parser for large-vocabulary speech recognition .	0	used-for	used-for	1
we present the first application of the #head-driven statistical parsing model# of collins -lrb- 1999 -rrb- as a simultaneous language model and $parser$ for large-vocabulary speech recognition .	0	used-for	used-for	1
we present the first application of the head-driven statistical parsing model of collins -lrb- 1999 -rrb- as a #simultaneous language model# and $parser$ for large-vocabulary speech recognition .	1	conjunction	conjunction	1
we present the first application of the head-driven statistical parsing model of collins -lrb- 1999 -rrb- as a #simultaneous language model# and parser for $large-vocabulary speech recognition$ .	0	used-for	used-for	1
we present the first application of the head-driven statistical parsing model of collins -lrb- 1999 -rrb- as a simultaneous language model and #parser# for $large-vocabulary speech recognition$ .	0	used-for	used-for	1
the #model# is adapted to an $online left to right chart-parser$ for word lattices , integrating acoustic , n-gram , and parser probabilities .	0	used-for	used-for	1
the model is adapted to an #online left to right chart-parser# for $word lattices$ , integrating acoustic , n-gram , and parser probabilities .	0	used-for	used-for	1
the model is adapted to an $online left to right chart-parser$ for word lattices , integrating #acoustic , n-gram , and parser probabilities# .	4	part-of	part-of	1
the $parser$ uses #structural and lexical dependencies# not considered by n-gram models , conditioning recognition on more linguistically-grounded relationships .	0	used-for	used-for	1
experiments on the #wall street journal treebank# and $lattice corpora$ show word error rates competitive with the standard n-gram language model while extracting additional structural information useful for speech understanding .	1	conjunction	conjunction	1
experiments on the #wall street journal treebank# and lattice corpora show word error rates competitive with the standard $n-gram language model$ while extracting additional structural information useful for speech understanding .	2	evaluate-for	evaluate-for	1
experiments on the wall street journal treebank and #lattice corpora# show word error rates competitive with the standard $n-gram language model$ while extracting additional structural information useful for speech understanding .	2	evaluate-for	evaluate-for	1
experiments on the wall street journal treebank and lattice corpora show #word error rates# competitive with the standard $n-gram language model$ while extracting additional structural information useful for speech understanding .	2	evaluate-for	evaluate-for	1
experiments on the wall street journal treebank and lattice corpora show word error rates competitive with the standard n-gram language model while extracting additional #structural information# useful for $speech understanding$ .	0	used-for	used-for	1
#image composition -lrb- or mosaicing -rrb-# has attracted a growing attention in recent years as one of the main elements in $video analysis and representation$ .	4	part-of	part-of	1
in this paper we deal with the problem of #global alignment# and $super-resolution$ .	1	conjunction	conjunction	1
we also propose to evaluate the quality of the resulting $mosaic$ by measuring the #amount of blurring# .	2	evaluate-for	evaluate-for	1
$global registration$ is achieved by combining a #graph-based technique# -- that exploits the topological structure of the sequence induced by the spatial overlap -- with a bundle adjustment which uses only the homographies computed in the previous steps .	0	used-for	used-for	1
global registration is achieved by combining a #graph-based technique# -- that exploits the $topological structure$ of the sequence induced by the spatial overlap -- with a bundle adjustment which uses only the homographies computed in the previous steps .	0	used-for	used-for	1
global registration is achieved by combining a #graph-based technique# -- that exploits the topological structure of the sequence induced by the spatial overlap -- with a $bundle adjustment$ which uses only the homographies computed in the previous steps .	1	conjunction	conjunction	1
$global registration$ is achieved by combining a graph-based technique -- that exploits the topological structure of the sequence induced by the spatial overlap -- with a #bundle adjustment# which uses only the homographies computed in the previous steps .	0	used-for	used-for	1
global registration is achieved by combining a graph-based technique -- that exploits the topological structure of the sequence induced by the spatial overlap -- with a $bundle adjustment$ which uses only the #homographies# computed in the previous steps .	0	used-for	used-for	1
experimental comparison with other $techniques$ shows the effectiveness of our #approach# .	6	compare	compare	1
the main of this project is $computer-assisted acquisition and morpho-syntactic description of verb-noun collocations$ in #polish# .	0	used-for	used-for	1
we present methodology and resources obtained in three main project $phases$ which are : #dictionary-based acquisition of collocation lexicon# , feasibility study for corpus-based lexicon enlargement phase , corpus-based lexicon enlargement and collocation description .	3	hyponym-of	hyponym-of	1
we present methodology and resources obtained in three main project phases which are : #dictionary-based acquisition of collocation lexicon# , $feasibility study$ for corpus-based lexicon enlargement phase , corpus-based lexicon enlargement and collocation description .	1	conjunction	conjunction	1
we present methodology and resources obtained in three main project $phases$ which are : dictionary-based acquisition of collocation lexicon , #feasibility study# for corpus-based lexicon enlargement phase , corpus-based lexicon enlargement and collocation description .	3	hyponym-of	hyponym-of	1
we present methodology and resources obtained in three main project phases which are : dictionary-based acquisition of collocation lexicon , #feasibility study# for $corpus-based lexicon enlargement phase$ , corpus-based lexicon enlargement and collocation description .	0	used-for	used-for	1
we present methodology and resources obtained in three main project $phases$ which are : dictionary-based acquisition of collocation lexicon , feasibility study for corpus-based lexicon enlargement phase , #corpus-based lexicon enlargement and collocation description# .	3	hyponym-of	hyponym-of	1
we present methodology and resources obtained in three main project phases which are : dictionary-based acquisition of collocation lexicon , $feasibility study$ for corpus-based lexicon enlargement phase , #corpus-based lexicon enlargement and collocation description# .	1	conjunction	conjunction	1
the presented here #corpus-based approach# permitted us to triple the size the $verb-noun collocation dictionary$ for polish .	0	used-for	used-for	1
the presented here corpus-based approach permitted us to triple the size the $verb-noun collocation dictionary$ for #polish# .	5	feature-of	feature-of	1
along with the increasing requirements , the #hash-tag recommendation task# for $microblogs$ has been receiving considerable attention in recent years .	0	used-for	used-for	1
motivated by the successful use of #convolutional neural networks -lrb- cnns -rrb-# for many $natural language processing tasks$ , in this paper , we adopt cnns to perform the hashtag recommendation problem .	0	used-for	used-for	1
to incorporate the $trigger words$ whose effectiveness have been experimentally evaluated in several previous works , we propose a novel #architecture# with an attention mechanism .	0	used-for	used-for	1
to incorporate the trigger words whose effectiveness have been experimentally evaluated in several previous works , we propose a novel $architecture$ with an #attention mechanism# .	5	feature-of	feature-of	1
the results of experiments on the #data# collected from a real world microblogging service demonstrated that the proposed $model$ outperforms state-of-the-art methods .	2	evaluate-for	evaluate-for	1
the results of experiments on the data collected from a real world microblogging service demonstrated that the proposed #model# outperforms $state-of-the-art methods$ .	6	compare	compare	1
by incorporating trigger words into the consideration , the relative improvement of the proposed #method# over the $state-of-the-art method$ is around 9.4 % in the f1-score .	6	compare	compare	1
by incorporating trigger words into the consideration , the relative improvement of the proposed method over the $state-of-the-art method$ is around 9.4 % in the #f1-score# .	2	evaluate-for	evaluate-for	1
in this paper , we improve an $unsupervised learning method$ using the #expectation-maximization -lrb- em -rrb- algorithm# proposed by nigam et al. for text classification problems in order to apply it to word sense disambiguation -lrb- wsd -rrb- problems .	0	used-for	used-for	1
in this paper , we improve an unsupervised learning method using the #expectation-maximization -lrb- em -rrb- algorithm# proposed by nigam et al. for $text classification problems$ in order to apply it to word sense disambiguation -lrb- wsd -rrb- problems .	0	used-for	used-for	1
in this paper , we improve an unsupervised learning method using the expectation-maximization -lrb- em -rrb- algorithm proposed by nigam et al. for text classification problems in order to apply #it# to $word sense disambiguation -lrb- wsd -rrb- problems$ .	0	used-for	used-for	1
in experiments , we solved 50 noun wsd problems in the #japanese dictionary task# in $senseval2$ .	5	feature-of	feature-of	1
furthermore , our #methods# were confirmed to be effective also for $verb wsd problems$ .	0	used-for	used-for	1
#dividing sentences in chunks of words# is a useful preprocessing step for $parsing$ , information extraction and information retrieval .	0	used-for	used-for	1
#dividing sentences in chunks of words# is a useful preprocessing step for parsing , $information extraction$ and information retrieval .	0	used-for	used-for	1
#dividing sentences in chunks of words# is a useful preprocessing step for parsing , information extraction and $information retrieval$ .	0	used-for	used-for	1
dividing sentences in chunks of words is a useful preprocessing step for #parsing# , $information extraction$ and information retrieval .	1	conjunction	conjunction	1
dividing sentences in chunks of words is a useful preprocessing step for parsing , #information extraction# and $information retrieval$ .	1	conjunction	conjunction	1
-lrb- ramshaw and marcus , 1995 -rrb- have introduced a `` convenient '' #data representation# for $chunking$ by converting it to a tagging task .	0	used-for	used-for	1
in this paper we will examine seven different #data representations# for the problem of $recognizing noun phrase chunks$ .	0	used-for	used-for	1
however , equipped with the most suitable #data representation# , our $memory-based learning chunker$ was able to improve the best published chunking results for a standard data set .	0	used-for	used-for	1
however , equipped with the most suitable data representation , our $memory-based learning chunker$ was able to improve the best published chunking results for a standard #data set# .	2	evaluate-for	evaluate-for	1
we focus on $faq-like questions and answers$ , and build our #system# around a noisy-channel architecture which exploits both a language model for answers and a transformation model for answer/question terms , trained on a corpus of 1 million question/answer pairs collected from the web .	0	used-for	used-for	1
we focus on faq-like questions and answers , and build our $system$ around a #noisy-channel architecture# which exploits both a language model for answers and a transformation model for answer/question terms , trained on a corpus of 1 million question/answer pairs collected from the web .	0	used-for	used-for	1
we focus on faq-like questions and answers , and build our system around a #noisy-channel architecture# which exploits both a $language model$ for answers and a transformation model for answer/question terms , trained on a corpus of 1 million question/answer pairs collected from the web .	0	used-for	used-for	1
we focus on faq-like questions and answers , and build our system around a #noisy-channel architecture# which exploits both a language model for answers and a $transformation model$ for answer/question terms , trained on a corpus of 1 million question/answer pairs collected from the web .	0	used-for	used-for	1
in this paper we evaluate four objective #measures of speech# with regards to $intelligibility prediction$ of synthesized speech in diverse noisy situations .	2	evaluate-for	evaluate-for	1
in this paper we evaluate four objective measures of speech with regards to $intelligibility prediction$ of #synthesized speech# in diverse noisy situations .	0	used-for	used-for	1
in this paper we evaluate four objective measures of speech with regards to intelligibility prediction of $synthesized speech$ in #diverse noisy situations# .	5	feature-of	feature-of	1
we evaluated three #intel-ligibility measures# , the dau measure , the glimpse proportion and the speech intelligibility index -lrb- sii -rrb- and a $quality measure$ , the perceptual evaluation of speech quality -lrb- pesq -rrb- .	1	conjunction	conjunction	1
we evaluated three $intel-ligibility measures$ , the #dau measure# , the glimpse proportion and the speech intelligibility index -lrb- sii -rrb- and a quality measure , the perceptual evaluation of speech quality -lrb- pesq -rrb- .	3	hyponym-of	hyponym-of	1
we evaluated three intel-ligibility measures , the #dau measure# , the $glimpse proportion$ and the speech intelligibility index -lrb- sii -rrb- and a quality measure , the perceptual evaluation of speech quality -lrb- pesq -rrb- .	1	conjunction	conjunction	1
we evaluated three $intel-ligibility measures$ , the dau measure , the #glimpse proportion# and the speech intelligibility index -lrb- sii -rrb- and a quality measure , the perceptual evaluation of speech quality -lrb- pesq -rrb- .	3	hyponym-of	hyponym-of	1
we evaluated three intel-ligibility measures , the dau measure , the #glimpse proportion# and the $speech intelligibility index -lrb- sii -rrb-$ and a quality measure , the perceptual evaluation of speech quality -lrb- pesq -rrb- .	1	conjunction	conjunction	1
we evaluated three $intel-ligibility measures$ , the dau measure , the glimpse proportion and the #speech intelligibility index -lrb- sii -rrb-# and a quality measure , the perceptual evaluation of speech quality -lrb- pesq -rrb- .	3	hyponym-of	hyponym-of	1
we evaluated three intel-ligibility measures , the dau measure , the glimpse proportion and the speech intelligibility index -lrb- sii -rrb- and a $quality measure$ , the #perceptual evaluation of speech quality -lrb- pesq -rrb-# .	3	hyponym-of	hyponym-of	1
for the $generation of synthesized speech$ we used a state of the art #hmm-based speech synthesis system# .	0	used-for	used-for	1
the $noisy conditions$ comprised four #additive noises# .	4	part-of	part-of	1
the #measures# were compared with $subjective intelligibility scores$ obtained in listening tests .	6	compare	compare	1
the results show the #dau# and the $glimpse measures$ to be the best predictors of intelligibility , with correlations of around 0.83 to subjective scores .	1	conjunction	conjunction	1
the results show the #dau# and the glimpse measures to be the best $predictors of intelligibility$ , with correlations of around 0.83 to subjective scores .	3	hyponym-of	hyponym-of	1
the results show the #dau# and the glimpse measures to be the best predictors of intelligibility , with correlations of around 0.83 to $subjective scores$ .	6	compare	compare	1
the results show the dau and the #glimpse measures# to be the best $predictors of intelligibility$ , with correlations of around 0.83 to subjective scores .	3	hyponym-of	hyponym-of	1
the results show the dau and the #glimpse measures# to be the best predictors of intelligibility , with correlations of around 0.83 to $subjective scores$ .	6	compare	compare	1
the results show the $dau$ and the glimpse measures to be the best predictors of intelligibility , with #correlations# of around 0.83 to subjective scores .	2	evaluate-for	evaluate-for	1
the results show the dau and the $glimpse measures$ to be the best predictors of intelligibility , with #correlations# of around 0.83 to subjective scores .	2	evaluate-for	evaluate-for	1
all #measures# gave less accurate $predictions of intelligibility$ for synthetic speech than have previously been found for natural speech ; in particular the sii measure .	2	evaluate-for	evaluate-for	1
all measures gave less accurate $predictions of intelligibility$ for #synthetic speech# than have previously been found for natural speech ; in particular the sii measure .	0	used-for	used-for	1
all measures gave less accurate predictions of intelligibility for #synthetic speech# than have previously been found for $natural speech$ ; in particular the sii measure .	6	compare	compare	1
all $measures$ gave less accurate predictions of intelligibility for synthetic speech than have previously been found for natural speech ; in particular the #sii measure# .	3	hyponym-of	hyponym-of	1
in additional experiments , we processed the $synthesized speech$ by an #ideal binary mask# before adding noise .	0	used-for	used-for	1
the #glimpse measure# gave the most accurate $intelligibility predictions$ in this situation .	0	used-for	used-for	1
a #'' graphics for vision '' approach# is proposed to address the problem of $reconstruction$ from a large and imperfect data set : reconstruction on demand by tensor voting , or rod-tv .	0	used-for	used-for	1
a '' graphics for vision '' approach is proposed to address the problem of $reconstruction$ from a #large and imperfect data set# : reconstruction on demand by tensor voting , or rod-tv .	0	used-for	used-for	1
a '' graphics for vision '' approach is proposed to address the problem of reconstruction from a large and imperfect data set : $reconstruction$ on demand by #tensor voting# , or rod-tv .	0	used-for	used-for	1
a '' graphics for vision '' approach is proposed to address the problem of reconstruction from a large and imperfect data set : reconstruction on demand by #tensor voting# , or $rod-tv$ .	1	conjunction	conjunction	1
a '' graphics for vision '' approach is proposed to address the problem of reconstruction from a large and imperfect data set : $reconstruction$ on demand by tensor voting , or #rod-tv# .	0	used-for	used-for	1
$rod-tv$ simultaneously delivers good #efficiency# and robust-ness , by adapting to a continuum of primitive connectivity , view dependence , and levels of detail -lrb- lod -rrb- .	2	evaluate-for	evaluate-for	1
$rod-tv$ simultaneously delivers good efficiency and #robust-ness# , by adapting to a continuum of primitive connectivity , view dependence , and levels of detail -lrb- lod -rrb- .	2	evaluate-for	evaluate-for	1
rod-tv simultaneously delivers good $efficiency$ and #robust-ness# , by adapting to a continuum of primitive connectivity , view dependence , and levels of detail -lrb- lod -rrb- .	1	conjunction	conjunction	1
rod-tv simultaneously delivers good efficiency and robust-ness , by adapting to a continuum of $primitive connectivity$ , #view dependence# , and levels of detail -lrb- lod -rrb- .	1	conjunction	conjunction	1
rod-tv simultaneously delivers good efficiency and robust-ness , by adapting to a continuum of primitive connectivity , $view dependence$ , and #levels of detail -lrb- lod -rrb-# .	1	conjunction	conjunction	1
#locally inferred surface elements# are robust to noise and better capture $local shapes$ .	0	used-for	used-for	1
by inferring #per-vertex normals# at sub-voxel precision on the fly , we can achieve $interpolative shading$ .	0	used-for	used-for	1
by inferring $per-vertex normals$ at #sub-voxel precision# on the fly , we can achieve interpolative shading .	5	feature-of	feature-of	1
by relaxing the #mesh connectivity requirement# , we extend rod-tv and propose a simple but effective $multiscale feature extraction algorithm$ .	0	used-for	used-for	1
by relaxing the mesh connectivity requirement , we extend #rod-tv# and propose a simple but effective $multiscale feature extraction algorithm$ .	0	used-for	used-for	1
$rod-tv$ consists of a #hierarchical data structure# that encodes different levels of detail .	4	part-of	part-of	1
the $local reconstruction algorithm$ is #tensor voting# .	3	hyponym-of	hyponym-of	1
$it$ is applied on demand to the visible subset of data at a desired level of detail , by #traversing the data hierarchy# and collecting tensorial support in a neighborhood .	0	used-for	used-for	1
it is applied on demand to the visible subset of data at a desired level of detail , by #traversing the data hierarchy# and $collecting tensorial support$ in a neighborhood .	1	conjunction	conjunction	1
$it$ is applied on demand to the visible subset of data at a desired level of detail , by traversing the data hierarchy and #collecting tensorial support# in a neighborhood .	0	used-for	used-for	1
both #rhetorical structure# and $punctuation$ have been helpful in discourse processing .	1	conjunction	conjunction	1
both #rhetorical structure# and punctuation have been helpful in $discourse processing$ .	0	used-for	used-for	1
both rhetorical structure and #punctuation# have been helpful in $discourse processing$ .	0	used-for	used-for	1
based on a corpus annotation project , this paper reports the discursive usage of 6 #chinese punctuation marks# in $news commentary texts$ : colon , dash , ellipsis , exclamation mark , question mark , and semicolon .	4	part-of	part-of	1
based on a corpus annotation project , this paper reports the discursive usage of 6 $chinese punctuation marks$ in news commentary texts : #colon# , dash , ellipsis , exclamation mark , question mark , and semicolon .	3	hyponym-of	hyponym-of	1
based on a corpus annotation project , this paper reports the discursive usage of 6 chinese punctuation marks in news commentary texts : #colon# , $dash$ , ellipsis , exclamation mark , question mark , and semicolon .	1	conjunction	conjunction	1
based on a corpus annotation project , this paper reports the discursive usage of 6 $chinese punctuation marks$ in news commentary texts : colon , #dash# , ellipsis , exclamation mark , question mark , and semicolon .	3	hyponym-of	hyponym-of	1
based on a corpus annotation project , this paper reports the discursive usage of 6 chinese punctuation marks in news commentary texts : colon , #dash# , $ellipsis$ , exclamation mark , question mark , and semicolon .	1	conjunction	conjunction	1
based on a corpus annotation project , this paper reports the discursive usage of 6 $chinese punctuation marks$ in news commentary texts : colon , dash , #ellipsis# , exclamation mark , question mark , and semicolon .	3	hyponym-of	hyponym-of	1
based on a corpus annotation project , this paper reports the discursive usage of 6 chinese punctuation marks in news commentary texts : colon , dash , #ellipsis# , $exclamation mark$ , question mark , and semicolon .	1	conjunction	conjunction	1
based on a corpus annotation project , this paper reports the discursive usage of 6 $chinese punctuation marks$ in news commentary texts : colon , dash , ellipsis , #exclamation mark# , question mark , and semicolon .	3	hyponym-of	hyponym-of	1
based on a corpus annotation project , this paper reports the discursive usage of 6 chinese punctuation marks in news commentary texts : colon , dash , ellipsis , #exclamation mark# , $question mark$ , and semicolon .	1	conjunction	conjunction	1
based on a corpus annotation project , this paper reports the discursive usage of 6 $chinese punctuation marks$ in news commentary texts : colon , dash , ellipsis , exclamation mark , #question mark# , and semicolon .	3	hyponym-of	hyponym-of	1
based on a corpus annotation project , this paper reports the discursive usage of 6 chinese punctuation marks in news commentary texts : colon , dash , ellipsis , exclamation mark , #question mark# , and $semicolon$ .	1	conjunction	conjunction	1
based on a corpus annotation project , this paper reports the discursive usage of 6 $chinese punctuation marks$ in news commentary texts : colon , dash , ellipsis , exclamation mark , question mark , and #semicolon# .	3	hyponym-of	hyponym-of	1
the #rhetorical patterns# of these $marks$ are compared against patterns around cue phrases in general .	5	feature-of	feature-of	1
the #rhetorical patterns# of these marks are compared against $patterns around cue phrases$ in general .	6	compare	compare	1
results show that these #chinese punctuation marks# , though fewer in number than $cue phrases$ , are easy to identify , have strong correlation with certain relations , and can be used as distinctive indicators of nuclearity in chinese texts .	6	compare	compare	1
results show that these #chinese punctuation marks# , though fewer in number than cue phrases , are easy to identify , have strong correlation with certain relations , and can be used as distinctive $indicators of nuclearity$ in chinese texts .	0	used-for	used-for	1
results show that these chinese punctuation marks , though fewer in number than cue phrases , are easy to identify , have strong correlation with certain relations , and can be used as distinctive $indicators of nuclearity$ in #chinese texts# .	5	feature-of	feature-of	1
the $features$ based on #markov random field -lrb- mrf -rrb- models# are usually sensitive to the rotation of image textures .	0	used-for	used-for	1
this paper develops an #anisotropic circular gaussian mrf -lrb- acgmrf -rrb- model# for $modelling rotated image textures$ and retrieving rotation-invariant texture features .	0	used-for	used-for	1
this paper develops an #anisotropic circular gaussian mrf -lrb- acgmrf -rrb- model# for modelling rotated image textures and $retrieving rotation-invariant texture features$ .	0	used-for	used-for	1
this paper develops an anisotropic circular gaussian mrf -lrb- acgmrf -rrb- model for #modelling rotated image textures# and $retrieving rotation-invariant texture features$ .	1	conjunction	conjunction	1
to overcome the #singularity problem# of the $least squares estimate -lrb- lse -rrb- method$ , an approximate least squares estimate -lrb- alse -rrb- method is proposed to estimate the parameters of the acgmrf model .	5	feature-of	feature-of	1
to overcome the singularity problem of the least squares estimate -lrb- lse -rrb- method , an #approximate least squares estimate -lrb- alse -rrb- method# is proposed to estimate the $parameters of the acgmrf model$ .	0	used-for	used-for	1
the $rotation-invariant features$ can be obtained from the #parameters of the acgmrf model# by the one-dimensional -lrb- 1-d -rrb- discrete fourier transform -lrb- dft -rrb- .	0	used-for	used-for	1
the $rotation-invariant features$ can be obtained from the parameters of the acgmrf model by the #one-dimensional -lrb- 1-d -rrb- discrete fourier transform -lrb- dft -rrb-# .	0	used-for	used-for	1
significantly improved accuracy can be achieved by applying the #rotation-invariant features# to classify $sar -lrb- synthetic aperture radar$ -rrb- sea ice and brodatz imagery .	0	used-for	used-for	1
despite much recent progress on accurate $semantic role labeling$ , previous work has largely used #independent classifiers# , possibly combined with separate label sequence models via viterbi decoding .	0	used-for	used-for	1
despite much recent progress on accurate semantic role labeling , previous work has largely used #independent classifiers# , possibly combined with separate $label sequence models$ via viterbi decoding .	1	conjunction	conjunction	1
despite much recent progress on accurate semantic role labeling , previous work has largely used independent classifiers , possibly combined with separate $label sequence models$ via #viterbi decoding# .	0	used-for	used-for	1
we show how to build a joint model of argument frames , incorporating novel #features# that model these interactions into $discriminative log-linear models$ .	4	part-of	part-of	1
this $system$ achieves an #error reduction# of 22 % on all arguments and 32 % on core arguments over a state-of-the art independent classifier for gold-standard parse trees on propbank .	2	evaluate-for	evaluate-for	1
this system achieves an #error reduction# of 22 % on all arguments and 32 % on core arguments over a state-of-the art $independent classifier$ for gold-standard parse trees on propbank .	2	evaluate-for	evaluate-for	1
this $system$ achieves an error reduction of 22 % on all arguments and 32 % on core arguments over a state-of-the art #independent classifier# for gold-standard parse trees on propbank .	6	compare	compare	1
this $system$ achieves an error reduction of 22 % on all arguments and 32 % on core arguments over a state-of-the art independent classifier for #gold-standard parse trees# on propbank .	2	evaluate-for	evaluate-for	1
this system achieves an error reduction of 22 % on all arguments and 32 % on core arguments over a state-of-the art $independent classifier$ for #gold-standard parse trees# on propbank .	2	evaluate-for	evaluate-for	1
this system achieves an error reduction of 22 % on all arguments and 32 % on core arguments over a state-of-the art independent classifier for #gold-standard parse trees# on $propbank$ .	4	part-of	part-of	1
in order to deal with $ambiguity$ , the #morphological parser morpa# is provided with a probabilistic context-free grammar -lrb- pcfg -rrb- , i.e. it combines a `` conventional '' context-free morphological grammar to filter out ungrammatical segmentations with a probability-based scoring function which determines the likelihood of each successful parse .	0	used-for	used-for	1
in order to deal with ambiguity , the $morphological parser morpa$ is provided with a #probabilistic context-free grammar -lrb- pcfg -rrb-# , i.e. it combines a `` conventional '' context-free morphological grammar to filter out ungrammatical segmentations with a probability-based scoring function which determines the likelihood of each successful parse .	0	used-for	used-for	1
in order to deal with ambiguity , the morphological parser morpa is provided with a probabilistic context-free grammar -lrb- pcfg -rrb- , i.e. $it$ combines a #`` conventional '' context-free morphological grammar# to filter out ungrammatical segmentations with a probability-based scoring function which determines the likelihood of each successful parse .	0	used-for	used-for	1
in order to deal with ambiguity , the morphological parser morpa is provided with a probabilistic context-free grammar -lrb- pcfg -rrb- , i.e. it combines a #`` conventional '' context-free morphological grammar# to filter out $ungrammatical segmentations$ with a probability-based scoring function which determines the likelihood of each successful parse .	0	used-for	used-for	1
in order to deal with ambiguity , the morphological parser morpa is provided with a probabilistic context-free grammar -lrb- pcfg -rrb- , i.e. $it$ combines a `` conventional '' context-free morphological grammar to filter out ungrammatical segmentations with a #probability-based scoring function# which determines the likelihood of each successful parse .	0	used-for	used-for	1
in order to deal with ambiguity , the morphological parser morpa is provided with a probabilistic context-free grammar -lrb- pcfg -rrb- , i.e. it combines a $`` conventional '' context-free morphological grammar$ to filter out ungrammatical segmentations with a #probability-based scoring function# which determines the likelihood of each successful parse .	1	conjunction	conjunction	1
in order to deal with ambiguity , the morphological parser morpa is provided with a probabilistic context-free grammar -lrb- pcfg -rrb- , i.e. it combines a `` conventional '' context-free morphological grammar to filter out ungrammatical segmentations with a #probability-based scoring function# which determines the likelihood of each successful $parse$ .	0	used-for	used-for	1
test performance data will show that a #pcfg# yields good results in $morphological parsing$ .	0	used-for	used-for	1
#morpa# is a fully implemented $parser$ developed for use in a text-to-speech conversion system .	3	hyponym-of	hyponym-of	1
#morpa# is a fully implemented parser developed for use in a $text-to-speech conversion system$ .	0	used-for	used-for	1
morpa is a fully implemented #parser# developed for use in a $text-to-speech conversion system$ .	0	used-for	used-for	1
this paper describes the framework of a $korean phonological knowledge base system$ using the #unification-based grammar formalism# : korean phonology structure grammar -lrb- kpsg -rrb- .	0	used-for	used-for	1
this paper describes the framework of a korean phonological knowledge base system using the $unification-based grammar formalism$ : #korean phonology structure grammar -lrb- kpsg -rrb-# .	3	hyponym-of	hyponym-of	1
the #approach# of $kpsg$ provides an explicit development model for constructing a computational phonological system : speech recognition and synthesis system .	0	used-for	used-for	1
the approach of #kpsg# provides an explicit development model for constructing a computational $phonological system$ : speech recognition and synthesis system .	0	used-for	used-for	1
we show that the proposed #approach# is more describable than other $approaches$ such as those employing a traditional generative phonological approach .	6	compare	compare	1
we show that the proposed approach is more describable than other approaches such as $those$ employing a traditional #generative phonological approach# .	0	used-for	used-for	1
in this paper , we study the #design of core-selecting payment rules# for such $domains$ .	0	used-for	used-for	1
we design two #core-selecting rules# that always satisfy $ir$ in expectation .	0	used-for	used-for	1
to study the performance of our $rules$ we perform a #computational bayes-nash equilibrium analysis# .	0	used-for	used-for	1
we show that , in equilibrium , our new #rules# have better incentives , higher efficiency , and a lower rate of ex-post ir violations than standard $core-selecting rules$ .	6	compare	compare	1
we show that , in equilibrium , our new $rules$ have better incentives , higher efficiency , and a lower #rate of ex-post ir violations# than standard core-selecting rules .	2	evaluate-for	evaluate-for	1
we show that , in equilibrium , our new rules have better incentives , higher efficiency , and a lower #rate of ex-post ir violations# than standard $core-selecting rules$ .	2	evaluate-for	evaluate-for	1
in this paper , we will describe a #search tool# for a huge set of $ngrams$ .	0	used-for	used-for	1
this system can be a very useful #tool# for $linguistic knowledge discovery$ and other nlp tasks .	0	used-for	used-for	1
this system can be a very useful #tool# for linguistic knowledge discovery and other $nlp tasks$ .	0	used-for	used-for	1
this system can be a very useful tool for #linguistic knowledge discovery# and other $nlp tasks$ .	1	conjunction	conjunction	1
this paper explores the role of #user modeling# in such $systems$ .	4	part-of	part-of	1
since acquiring the knowledge for a #user model# is a fundamental problem in $user modeling$ , a section is devoted to this topic .	0	used-for	used-for	1
next , the benefits and costs of implementing a #user modeling component# for a $system$ are weighed in light of several aspects of the interaction requirements that may be imposed by the system .	4	part-of	part-of	1
#information extraction techniques# automatically create $structured databases$ from unstructured data sources , such as the web or newswire documents .	0	used-for	used-for	1
$information extraction techniques$ automatically create structured databases from #unstructured data sources# , such as the web or newswire documents .	0	used-for	used-for	1
information extraction techniques automatically create structured databases from $unstructured data sources$ , such as the #web# or newswire documents .	3	hyponym-of	hyponym-of	1
information extraction techniques automatically create structured databases from unstructured data sources , such as the #web# or $newswire documents$ .	1	conjunction	conjunction	1
information extraction techniques automatically create structured databases from $unstructured data sources$ , such as the web or #newswire documents# .	3	hyponym-of	hyponym-of	1
despite the successes of these $systems$ , #accuracy# will always be imperfect .	2	evaluate-for	evaluate-for	1
the $information extraction system$ we evaluate is based on a #linear-chain conditional random field -lrb- crf -rrb-# , a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary , overlapping features of the input in a markov model .	0	used-for	used-for	1
the information extraction system we evaluate is based on a #linear-chain conditional random field -lrb- crf -rrb-# , a $probabilistic model$ which has performed well on information extraction tasks because of its ability to capture arbitrary , overlapping features of the input in a markov model .	3	hyponym-of	hyponym-of	1
the information extraction system we evaluate is based on a linear-chain conditional random field -lrb- crf -rrb- , a #probabilistic model# which has performed well on $information extraction tasks$ because of its ability to capture arbitrary , overlapping features of the input in a markov model .	0	used-for	used-for	1
the information extraction system we evaluate is based on a linear-chain conditional random field -lrb- crf -rrb- , a #probabilistic model# which has performed well on information extraction tasks because of its ability to capture $arbitrary , overlapping features$ of the input in a markov model .	0	used-for	used-for	1
the information extraction system we evaluate is based on a linear-chain conditional random field -lrb- crf -rrb- , a probabilistic model which has performed well on information extraction tasks because of its ability to capture #arbitrary , overlapping features# of the $input$ in a markov model .	5	feature-of	feature-of	1
the information extraction system we evaluate is based on a linear-chain conditional random field -lrb- crf -rrb- , a probabilistic model which has performed well on information extraction tasks because of its ability to capture #arbitrary , overlapping features# of the input in a $markov model$ .	4	part-of	part-of	1
we implement several techniques to estimate the confidence of both #extracted fields# and entire $multi-field records$ , obtaining an average precision of 98 % for retrieving correct fields and 87 % for multi-field records .	1	conjunction	conjunction	1
we implement several $techniques$ to estimate the confidence of both extracted fields and entire multi-field records , obtaining an #average precision# of 98 % for retrieving correct fields and 87 % for multi-field records .	2	evaluate-for	evaluate-for	1
in this paper , we use the #information redundancy in multilingual input# to correct errors in $machine translation$ and thus improve the quality of multilingual summaries .	0	used-for	used-for	1
in this paper , we use the #information redundancy in multilingual input# to correct errors in machine translation and thus improve the quality of $multilingual summaries$ .	0	used-for	used-for	1
we demonstrate how errors in the $machine translations$ of the input #arabic documents# can be corrected by identifying and generating from such redundancy , focusing on noun phrases .	0	used-for	used-for	1
in this paper , we propose a new #approach# to generate $oriented object proposals -lrb- oops -rrb-$ to reduce the detection error caused by various orientations of the object .	0	used-for	used-for	1
in this paper , we propose a new approach to generate $oriented object proposals -lrb- oops -rrb-$ to reduce the #detection error# caused by various orientations of the object .	2	evaluate-for	evaluate-for	1
to this end , we propose to efficiently locate $object regions$ according to #pixelwise object probability# , rather than measuring the objectness from a set of sampled windows .	0	used-for	used-for	1
to this end , we propose to efficiently locate object regions according to #pixelwise object probability# , rather than measuring the $objectness$ from a set of sampled windows .	6	compare	compare	1
we formulate the $proposal generation problem$ as a #generative proba-bilistic model# such that object proposals of different shapes -lrb- i.e. , sizes and orientations -rrb- can be produced by locating the local maximum likelihoods .	0	used-for	used-for	1
we formulate the proposal generation problem as a generative proba-bilistic model such that $object proposals$ of different #shapes# -lrb- i.e. , sizes and orientations -rrb- can be produced by locating the local maximum likelihoods .	5	feature-of	feature-of	1
we formulate the proposal generation problem as a generative proba-bilistic model such that object proposals of different $shapes$ -lrb- i.e. , #sizes# and orientations -rrb- can be produced by locating the local maximum likelihoods .	3	hyponym-of	hyponym-of	1
we formulate the proposal generation problem as a generative proba-bilistic model such that object proposals of different shapes -lrb- i.e. , #sizes# and $orientations$ -rrb- can be produced by locating the local maximum likelihoods .	1	conjunction	conjunction	1
we formulate the proposal generation problem as a generative proba-bilistic model such that object proposals of different $shapes$ -lrb- i.e. , sizes and #orientations# -rrb- can be produced by locating the local maximum likelihoods .	3	hyponym-of	hyponym-of	1
we formulate the proposal generation problem as a generative proba-bilistic model such that $object proposals$ of different shapes -lrb- i.e. , sizes and orientations -rrb- can be produced by locating the #local maximum likelihoods# .	0	used-for	used-for	1
first , it helps the #object detector# handle objects of different $orientations$ .	0	used-for	used-for	1
third , #it# avoids massive window sampling , and thereby reducing the $number of proposals$ while maintaining a high recall .	0	used-for	used-for	1
third , $it$ avoids massive window sampling , and thereby reducing the number of proposals while maintaining a high #recall# .	2	evaluate-for	evaluate-for	1
experiments on the #pascal voc 2007 dataset# show that the proposed $oop$ outperforms the state-of-the-art fast methods .	2	evaluate-for	evaluate-for	1
experiments on the pascal voc 2007 dataset show that the proposed #oop# outperforms the $state-of-the-art fast methods$ .	6	compare	compare	1
further experiments show that the #rotation invariant property# helps a $class-specific object detector$ achieve better performance than the state-of-the-art proposal generation methods in either object rotation scenarios or general scenarios .	0	used-for	used-for	1
further experiments show that the rotation invariant property helps a #class-specific object detector# achieve better performance than the state-of-the-art $proposal generation methods$ in either object rotation scenarios or general scenarios .	6	compare	compare	1
further experiments show that the rotation invariant property helps a $class-specific object detector$ achieve better performance than the state-of-the-art proposal generation methods in either #object rotation scenarios# or general scenarios .	2	evaluate-for	evaluate-for	1
further experiments show that the rotation invariant property helps a class-specific object detector achieve better performance than the state-of-the-art $proposal generation methods$ in either #object rotation scenarios# or general scenarios .	2	evaluate-for	evaluate-for	1
further experiments show that the rotation invariant property helps a class-specific object detector achieve better performance than the state-of-the-art proposal generation methods in either #object rotation scenarios# or $general scenarios$ .	1	conjunction	conjunction	1
further experiments show that the rotation invariant property helps a $class-specific object detector$ achieve better performance than the state-of-the-art proposal generation methods in either object rotation scenarios or #general scenarios# .	2	evaluate-for	evaluate-for	1
further experiments show that the rotation invariant property helps a class-specific object detector achieve better performance than the state-of-the-art $proposal generation methods$ in either object rotation scenarios or #general scenarios# .	2	evaluate-for	evaluate-for	1
this paper describes three relatively #domain-independent capabilities# recently added to the $paramax spoken language understanding system$ : non-monotonic reasoning , implicit reference resolution , and database query paraphrase .	4	part-of	part-of	1
this paper describes three relatively $domain-independent capabilities$ recently added to the paramax spoken language understanding system : #non-monotonic reasoning# , implicit reference resolution , and database query paraphrase .	3	hyponym-of	hyponym-of	1
this paper describes three relatively $domain-independent capabilities$ recently added to the paramax spoken language understanding system : non-monotonic reasoning , #implicit reference resolution# , and database query paraphrase .	3	hyponym-of	hyponym-of	1
this paper describes three relatively $domain-independent capabilities$ recently added to the paramax spoken language understanding system : non-monotonic reasoning , implicit reference resolution , and #database query paraphrase# .	3	hyponym-of	hyponym-of	1
finally , we briefly describe an experiment which we have done in extending the $n-best speech/language integration architecture$ to improving #ocr accuracy# .	2	evaluate-for	evaluate-for	1
we investigate the problem of fine-grained sketch-based image retrieval -lrb- sbir -rrb- , where #free-hand human sketches# are used as queries to perform $instance-level retrieval of images$ .	0	used-for	used-for	1
this is an extremely challenging task because -lrb- i -rrb- visual comparisons not only need to be fine-grained but also executed cross-domain , -lrb- ii -rrb- free-hand -lrb- finger -rrb- sketches are highly abstract , making fine-grained matching harder , and most importantly -lrb- iii -rrb- #annotated cross-domain sketch-photo datasets# required for training are scarce , challenging many state-of-the-art $machine learning techniques$ .	0	used-for	used-for	1
we then develop a #deep triplet-ranking model# for $instance-level sbir$ with a novel data augmentation and staged pre-training strategy to alleviate the issue of insufficient fine-grained training data .	0	used-for	used-for	1
we then develop a #deep triplet-ranking model# for instance-level sbir with a novel data augmentation and staged pre-training strategy to alleviate the issue of $insufficient fine-grained training data$ .	0	used-for	used-for	1
we then develop a $deep triplet-ranking model$ for instance-level sbir with a novel #data augmentation# and staged pre-training strategy to alleviate the issue of insufficient fine-grained training data .	0	used-for	used-for	1
we then develop a deep triplet-ranking model for instance-level sbir with a novel #data augmentation# and $staged pre-training strategy$ to alleviate the issue of insufficient fine-grained training data .	1	conjunction	conjunction	1
we then develop a $deep triplet-ranking model$ for instance-level sbir with a novel data augmentation and #staged pre-training strategy# to alleviate the issue of insufficient fine-grained training data .	0	used-for	used-for	1
extensive experiments are carried out to contribute a variety of insights into the challenges of #data sufficiency# and $over-fitting avoidance$ when training deep networks for fine-grained cross-domain ranking tasks .	1	conjunction	conjunction	1
extensive experiments are carried out to contribute a variety of insights into the challenges of data sufficiency and over-fitting avoidance when training #deep networks# for $fine-grained cross-domain ranking tasks$ .	0	used-for	used-for	1
in this paper we target at generating $generic action proposals$ in #unconstrained videos# .	0	used-for	used-for	1
each action proposal corresponds to a $temporal series of spatial bounding boxes$ , i.e. , a #spatio-temporal video tube# , which has a good potential to locate one human action .	3	hyponym-of	hyponym-of	1
each action proposal corresponds to a temporal series of spatial bounding boxes , i.e. , a #spatio-temporal video tube# , which has a good potential to locate one $human action$ .	0	used-for	used-for	1
assuming each action is performed by a human with meaningful motion , both #appearance and motion cues# are utilized to measure the $ac-tionness$ of the video tubes .	0	used-for	used-for	1
assuming each action is performed by a human with meaningful motion , both appearance and motion cues are utilized to measure the #ac-tionness# of the $video tubes$ .	2	evaluate-for	evaluate-for	1
after picking those spatiotem-poral paths of high actionness scores , our $action proposal generation$ is formulated as a #maximum set coverage problem# , where greedy search is performed to select a set of action proposals that can maximize the overall actionness score .	0	used-for	used-for	1
after picking those spatiotem-poral paths of high actionness scores , our action proposal generation is formulated as a maximum set coverage problem , where #greedy search# is performed to select a set of $action proposals$ that can maximize the overall actionness score .	0	used-for	used-for	1
after picking those spatiotem-poral paths of high actionness scores , our action proposal generation is formulated as a maximum set coverage problem , where greedy search is performed to select a set of $action proposals$ that can maximize the overall #actionness score# .	2	evaluate-for	evaluate-for	1
compared with existing #action proposal approaches# , our $action proposals$ do not rely on video segmentation and can be generated in nearly real-time .	6	compare	compare	1
experimental results on two challenging #datasets# , msrii and ucf 101 , validate the superior performance of our $action proposals$ as well as competitive results on action detection and search .	2	evaluate-for	evaluate-for	1
experimental results on two challenging $datasets$ , #msrii# and ucf 101 , validate the superior performance of our action proposals as well as competitive results on action detection and search .	3	hyponym-of	hyponym-of	1
experimental results on two challenging datasets , #msrii# and $ucf 101$ , validate the superior performance of our action proposals as well as competitive results on action detection and search .	1	conjunction	conjunction	1
experimental results on two challenging $datasets$ , msrii and #ucf 101# , validate the superior performance of our action proposals as well as competitive results on action detection and search .	3	hyponym-of	hyponym-of	1
experimental results on two challenging datasets , msrii and ucf 101 , validate the superior performance of our $action proposals$ as well as competitive results on #action detection and search# .	2	evaluate-for	evaluate-for	1
this paper reports recent research into #methods# for $creating natural language text$ .	0	used-for	used-for	1
$kds -lrb- knowledge delivery system -rrb-$ , which embodies this #paradigm# , has distinct parts devoted to creation of the propositional units , to organization of the text , to prevention of excess redundancy , to creation of combinations of units , to evaluation of these combinations as potential sentences , to selection of the best among competing combinations , and to creation of the final text .	4	part-of	part-of	1
the fragment-and-compose paradigm and the #computational methods# of $kds$ are described .	0	used-for	used-for	1
this paper explores the issue of using different #co-occurrence similarities# between terms for separating $query terms$ that are useful for retrieval from those that are harmful .	0	used-for	used-for	1
this paper explores the issue of using different co-occurrence similarities between terms for separating #query terms# that are useful for $retrieval$ from those that are harmful .	0	used-for	used-for	1
this paper explores the issue of using different co-occurrence similarities between terms for separating $query terms$ that are useful for retrieval from #those# that are harmful .	6	compare	compare	1
the hypothesis under examination is that #useful terms# tend to be more similar to each other than to other $query terms$ .	6	compare	compare	1
preliminary experiments with $similarities$ computed using #first-order and second-order co-occurrence# seem to confirm the hypothesis .	0	used-for	used-for	1
we propose a new #phrase-based translation model# and $decoding algorithm$ that enables us to evaluate and compare several , previously proposed phrase-based translation models .	1	conjunction	conjunction	1
within our framework , we carry out a large number of experiments to understand better and explain why #phrase-based models# outperform $word-based models$ .	6	compare	compare	1
our empirical results , which hold for all examined language pairs , suggest that the highest levels of performance can be obtained through relatively simple $means$ : #heuristic learning of phrase translations# from word-based alignments and lexical weighting of phrase translations .	3	hyponym-of	hyponym-of	1
our empirical results , which hold for all examined language pairs , suggest that the highest levels of performance can be obtained through relatively simple means : $heuristic learning of phrase translations$ from #word-based alignments# and lexical weighting of phrase translations .	0	used-for	used-for	1
our empirical results , which hold for all examined language pairs , suggest that the highest levels of performance can be obtained through relatively simple $means$ : heuristic learning of phrase translations from word-based alignments and #lexical weighting of phrase translations# .	3	hyponym-of	hyponym-of	1
traditional #methods# for $color constancy$ can improve surface re-flectance estimates from such uncalibrated images , but their output depends significantly on the background scene .	0	used-for	used-for	1
traditional #methods# for color constancy can improve $surface re-flectance estimates$ from such uncalibrated images , but their output depends significantly on the background scene .	0	used-for	used-for	1
traditional methods for color constancy can improve $surface re-flectance estimates$ from such #uncalibrated images# , but their output depends significantly on the background scene .	0	used-for	used-for	1
we introduce the multi-view color constancy problem , and present a #method# to recover $estimates of underlying surface re-flectance$ based on joint estimation of these surface properties and the illuminants present in multiple images .	0	used-for	used-for	1
the #method# can exploit $image correspondences$ obtained by various alignment techniques , and we show examples based on matching local region features .	0	used-for	used-for	1
the method can exploit $image correspondences$ obtained by various #alignment techniques# , and we show examples based on matching local region features .	0	used-for	used-for	1
our results show that #multi-view constraints# can significantly improve $estimates of both scene illuminants and object color -lrb- surface reflectance -rrb-$ when compared to a baseline single-view method .	0	used-for	used-for	1
our results show that $multi-view constraints$ can significantly improve estimates of both scene illuminants and object color -lrb- surface reflectance -rrb- when compared to a #baseline single-view method# .	6	compare	compare	1
our contributions include a #concise , modular architecture# with reversible processes of $understanding$ and generation , an information-state model of reference , and flexible links between semantics and collaborative problem solving .	0	used-for	used-for	1
our contributions include a #concise , modular architecture# with reversible processes of understanding and $generation$ , an information-state model of reference , and flexible links between semantics and collaborative problem solving .	0	used-for	used-for	1
our contributions include a concise , modular architecture with reversible processes of #understanding# and $generation$ , an information-state model of reference , and flexible links between semantics and collaborative problem solving .	1	conjunction	conjunction	1
